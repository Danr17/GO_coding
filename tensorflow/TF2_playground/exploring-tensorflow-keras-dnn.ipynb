{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is my playground to test out some concepts I found reading  [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**: Concepts, Tools, and Techniques to Build Intelligent Systems 2nd Edition](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).  \n",
    "I would highly recommend this book to everyone, regardless of your expertise level, especially if Tensorflow is your thing.\n",
    "\n",
    "In this notebook I'll show how to use Sequential and Functional Keras API, also to discover best parameters using GridSearchCV or RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "- **1. [Dataset Description](#description)**\n",
    "- **2. [Pre-processing](#pre-processing)**\n",
    "  - **2.1. [Feature engineering](#feat-eng)**\n",
    "  - **2.2. [Fill in missing data](#feat-fill)**\n",
    "  - **2.3. [Encode Categorical & Scale continuous variables](#feat-encode)**\n",
    "- **3. [Build, Train and Evaluate the Model](#network)**\n",
    "  - **3.1. [Tensorflow2.0 & Keras](#tensorflow)**\n",
    "  - **3.2. [Keras building blocks](#layers)**\n",
    "  - **3.3. [Build a simple model - 0.76555 score](#simple-model)**\n",
    "    - **3.3.1. [Model architecture](#architecture)**\n",
    "    - **3.3.2. [Compile the Model](#compile)**\n",
    "    - **3.3.3. [Train the Model](#train)**\n",
    "    - **3.3.4. [Predict on the Test data and submit to Kaggle](#predict)**  \n",
    "  - **3.4. [Keras Model using Functional API - 0.77511 score](#functional-model)**\n",
    "  - **3.5. [Hyperparameter Tuning](#hyperparameter)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"description\"></a>\n",
    "## Dataset description\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. The challenge is to find the likehood to survive, finding relevant patterns in the data. This is one of the most used dataset in data science, along with MNIST and Housing Prices.\n",
    "The dataset contain folllowing information about each person embarked on Titanic:\n",
    "\n",
    "Variable | Definition |Key\n",
    "--- | --- | ---\n",
    "survival | Survival | 0 = No, 1 = Yes\n",
    "pclass | Ticket class |1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "sex | Sex\t\n",
    "Age | Age in years\t\n",
    "sibsp | # of siblings / spouses aboard the Titanic\t\n",
    "parch | # of parents / children aboard the Titanic\t\n",
    "ticket |Ticket number\t\n",
    "fare | Passenger fare\t\n",
    "cabin | Cabin number\t\n",
    "embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "The data has been split into two groups:\n",
    "\n",
    "* training set (train.csv) -- should be used to build your machine learning models and it has the labels\n",
    "* test set (test.csv) -- should be used to see how well your model performs on unseen data, as I'm using Kaggle the ground truth for each passenger is not provided\n",
    "\n",
    "Although **EDA** is very important to visualize and understand the data, I'm going to skip that part and focus on the model itself. But if you are interested in exploring the data then there are a bunch of useful [notebooks published for Titanic dataset](https://www.kaggle.com/c/titanic/notebooks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train = pd.read_csv(\"../input/titanic/train.csv\")\n",
    "test = pd.read_csv(\"../input/titanic/test.csv\")\n",
    "\n",
    "# Adding a column in each dataset before merging\n",
    "train['Type'] = 'train'\n",
    "test['Type'] = 'test'\n",
    "\n",
    "# Merging train and test\n",
    "data = train.append(test, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "## 2. Pre-Processing the data\n",
    "Pre-processing refers to the transformations applied to our data before feeding it to the algorithm. Data Preprocessing is a technique that is used to convert the raw data into a clean data set.  \n",
    "Deep Learning cannot operate on label data directly. It requires all input variables and output variables to be numeric, threfore the categorical data will be transformed in numbers and numerical values will be scaled for better convergence.\n",
    "We'll introduce new features as well, that helps the deep learning algorithms to find better patterns, which will increase prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1       0.0       3   \n",
       "1            2       1.0       1   \n",
       "2            3       1.0       3   \n",
       "3            4       1.0       1   \n",
       "4            5       0.0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked   Type  \n",
       "0      0         A/5 21171   7.2500   NaN        S  train  \n",
       "1      0          PC 17599  71.2833   C85        C  train  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  train  \n",
       "3      0            113803  53.1000  C123        S  train  \n",
       "4      0            373450   8.0500   NaN        S  train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the initial data, first 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat-eng\"></a>\n",
    "### 2.1. Feature Engineering \n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms.  \n",
    "Based on initial exploration of the data, we observed that the Titles from the names (like Miss, Mrs etc) could influence the performance of the model. Therefore we extract those and populate within a new column. Also we consolidate **Parch** and **SibSp** within a single column, named **Family_Size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning name and extracting Title\n",
    "for name_string in data['Name']:\n",
    "    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n",
    "    \n",
    "# Replacing rare titles \n",
    "mapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n",
    "           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n",
    "           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n",
    "           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n",
    "           \n",
    "data.replace({'Title': mapping}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Family_Size'] = (data['Parch'] + data['SibSp']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat-fill\"></a>\n",
    "### 2.2. Fill in the missing data\n",
    "\n",
    "Although it is possible to train algorithms with missing values, it's highly recommended to clean the data.  \n",
    "**Some Strategies**:\n",
    "* Drop the missing values: Drop the row if a particular feature is missing\n",
    "* Replace the missing values with mean/median imputation:  Calculate the mean or median of the feature and replace it with the missing values. This is an approximation which can add variance to the data set. But the loss of the data can be negated by this method which yields better results compared to removal of rows and columns\n",
    "\n",
    "In our example **Cabin** will be dropped off as has no importance, **Age** will be filled with the mean for the corresponding category\n",
    "and for  **Embarked** the two missing fields will be added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 15 columns):\n",
      "PassengerId    1309 non-null int64\n",
      "Survived       891 non-null float64\n",
      "Pclass         1309 non-null int64\n",
      "Name           1309 non-null object\n",
      "Sex            1309 non-null object\n",
      "Age            1046 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Ticket         1309 non-null object\n",
      "Fare           1308 non-null float64\n",
      "Cabin          295 non-null object\n",
      "Embarked       1307 non-null object\n",
      "Type           1309 non-null object\n",
      "Title          1309 non-null object\n",
      "Family_Size    1309 non-null int64\n",
      "dtypes: float64(3), int64(5), object(7)\n",
      "memory usage: 163.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sex     Pclass\n",
       "female  1         37.037594\n",
       "        2         27.499223\n",
       "        3         22.185329\n",
       "male    1         41.029272\n",
       "        2         30.815380\n",
       "        3         25.962264\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby([\"Sex\", \"Pclass\"]).Age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].fillna(data.groupby([\"Sex\", \"Pclass\"])['Age'].transform(\"mean\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Type</th>\n",
       "      <th>Title</th>\n",
       "      <th>Family_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>Miss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                       Name  \\\n",
       "61            62       1.0       1                        Icard, Miss. Amelie   \n",
       "829          830       1.0       1  Stone, Mrs. George Nelson (Martha Evelyn)   \n",
       "\n",
       "        Sex   Age  SibSp  Parch  Ticket  Fare Cabin Embarked   Type Title  \\\n",
       "61   female  38.0      0      0  113572  80.0   B28      NaN  train  Miss   \n",
       "829  female  62.0      0      0  113572  80.0   B28      NaN  train   Mrs   \n",
       "\n",
       "     Family_Size  \n",
       "61             0  \n",
       "829            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[pd.isnull(data['Embarked'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[61,'Embarked'] = 'S'\n",
    "data.loc[829,'Embarked'] = 'S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare'].fillna(data['Fare'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feat-encode\"></a>\n",
    "### 2.3. Encode Categorical and Scale Continuous variables\n",
    "\n",
    "There are two types of data:\n",
    "* Categorical : Features whose values are taken from a defined set of values. For instance, days in a week : {Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday} is a category because its value is always taken from this set. Another example could be the Boolean set : {True, False}\n",
    "* Numerical : Features whose values are continuous or integer-valued. They are represented by numbers and possess most of the properties of numbers. For instance, number of steps you walk in a day, or the speed at which you are driving your car at.\n",
    "\n",
    "#### The transformation:\n",
    "\n",
    "**Encode Categorical Variables**  \n",
    "We need to convert all categorical variables into numeric format. The categorical variables we will be keeping are Embarked, Sex and Title.\n",
    "\n",
    "**Scale Continuous variables**  \n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.  \n",
    "We can standardize data using scikit-learn with the StandardScaler class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop the variables we won't be using in the model\n",
    "data.drop(['Cabin', 'Name', 'Ticket', 'PassengerId', 'SibSp', 'Parch'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to cateogry dtype\n",
    "data['Sex'] = data['Sex'].astype('category')\n",
    "# convert to category codes\n",
    "data['Sex'] = data['Sex'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset all categorical variables which need to be encoded\n",
    "categorical = ['Embarked', 'Title', 'Pclass']\n",
    "\n",
    "for cat in categorical:\n",
    "    data = pd.concat([data, \n",
    "                    pd.get_dummies(data[cat], prefix=cat)], axis=1)\n",
    "    del data[cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale numerical values\n",
    "continuous = ['Age', 'Fare', 'Family_Size']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for val in continuous:\n",
    "    data[val] = data[val].astype('float64')\n",
    "    data[val] = scaler.fit_transform(data[val].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Type</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "      <th>Title_Royal</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.560331</td>\n",
       "      <td>-0.503595</td>\n",
       "      <td>train</td>\n",
       "      <td>0.073352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.655107</td>\n",
       "      <td>0.734503</td>\n",
       "      <td>train</td>\n",
       "      <td>0.073352</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.256471</td>\n",
       "      <td>-0.490544</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.558346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427212</td>\n",
       "      <td>0.382925</td>\n",
       "      <td>train</td>\n",
       "      <td>0.073352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.427212</td>\n",
       "      <td>-0.488127</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.558346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex       Age      Fare   Type  Family_Size  Embarked_C  \\\n",
       "0       0.0    1 -0.560331 -0.503595  train     0.073352           0   \n",
       "1       1.0    0  0.655107  0.734503  train     0.073352           1   \n",
       "2       1.0    0 -0.256471 -0.490544  train    -0.558346           0   \n",
       "3       1.0    0  0.427212  0.382925  train     0.073352           0   \n",
       "4       0.0    1  0.427212 -0.488127  train    -0.558346           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
       "0           0           1             0           0         1          0   \n",
       "1           0           0             0           0         0          1   \n",
       "2           0           1             0           1         0          0   \n",
       "3           0           1             0           0         0          1   \n",
       "4           0           1             0           0         1          0   \n",
       "\n",
       "   Title_Other  Title_Royal  Pclass_1  Pclass_2  Pclass_3  \n",
       "0            0            0         0         0         1  \n",
       "1            0            0         1         0         0  \n",
       "2            0            0         0         0         1  \n",
       "3            0            0         1         0         0  \n",
       "4            0            0         0         0         1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout the data after all transoformations\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Other</th>\n",
       "      <th>Title_Royal</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1.309000e+03</td>\n",
       "      <td>1.309000e+03</td>\n",
       "      <td>1.309000e+03</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.644003</td>\n",
       "      <td>2.459623e-16</td>\n",
       "      <td>1.602996e-17</td>\n",
       "      <td>2.593630e-16</td>\n",
       "      <td>0.206264</td>\n",
       "      <td>0.093965</td>\n",
       "      <td>0.699771</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.201681</td>\n",
       "      <td>0.578304</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.211612</td>\n",
       "      <td>0.541635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.478997</td>\n",
       "      <td>1.000382e+00</td>\n",
       "      <td>1.000382e+00</td>\n",
       "      <td>1.000382e+00</td>\n",
       "      <td>0.404777</td>\n",
       "      <td>0.291891</td>\n",
       "      <td>0.458533</td>\n",
       "      <td>0.210862</td>\n",
       "      <td>0.401408</td>\n",
       "      <td>0.494019</td>\n",
       "      <td>0.358440</td>\n",
       "      <td>0.131435</td>\n",
       "      <td>0.067573</td>\n",
       "      <td>0.431287</td>\n",
       "      <td>0.408607</td>\n",
       "      <td>0.498454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.218643e+00</td>\n",
       "      <td>-6.437751e-01</td>\n",
       "      <td>-5.583461e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.603308e-01</td>\n",
       "      <td>-4.911082e-01</td>\n",
       "      <td>-5.583461e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.564715e-01</td>\n",
       "      <td>-3.643001e-01</td>\n",
       "      <td>-5.583461e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.791418e-01</td>\n",
       "      <td>-3.906640e-02</td>\n",
       "      <td>7.335229e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.845630e+00</td>\n",
       "      <td>9.262219e+00</td>\n",
       "      <td>5.758637e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived          Sex           Age          Fare   Family_Size  \\\n",
       "count  891.000000  1309.000000  1.309000e+03  1.309000e+03  1.309000e+03   \n",
       "mean     0.383838     0.644003  2.459623e-16  1.602996e-17  2.593630e-16   \n",
       "std      0.486592     0.478997  1.000382e+00  1.000382e+00  1.000382e+00   \n",
       "min      0.000000     0.000000 -2.218643e+00 -6.437751e-01 -5.583461e-01   \n",
       "25%      0.000000     0.000000 -5.603308e-01 -4.911082e-01 -5.583461e-01   \n",
       "50%      0.000000     1.000000 -2.564715e-01 -3.643001e-01 -5.583461e-01   \n",
       "75%      1.000000     1.000000  5.791418e-01 -3.906640e-02  7.335229e-02   \n",
       "max      1.000000     1.000000  3.845630e+00  9.262219e+00  5.758637e+00   \n",
       "\n",
       "        Embarked_C   Embarked_Q   Embarked_S  Title_Master   Title_Miss  \\\n",
       "count  1309.000000  1309.000000  1309.000000   1309.000000  1309.000000   \n",
       "mean      0.206264     0.093965     0.699771      0.046600     0.201681   \n",
       "std       0.404777     0.291891     0.458533      0.210862     0.401408   \n",
       "min       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "50%       0.000000     0.000000     1.000000      0.000000     0.000000   \n",
       "75%       0.000000     0.000000     1.000000      0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000      1.000000     1.000000   \n",
       "\n",
       "          Title_Mr    Title_Mrs  Title_Other  Title_Royal     Pclass_1  \\\n",
       "count  1309.000000  1309.000000  1309.000000  1309.000000  1309.000000   \n",
       "mean      0.578304     0.151261     0.017571     0.004584     0.246753   \n",
       "std       0.494019     0.358440     0.131435     0.067573     0.431287   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          Pclass_2     Pclass_3  \n",
       "count  1309.000000  1309.000000  \n",
       "mean      0.211612     0.541635  \n",
       "std       0.408607     0.498454  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.000000     1.000000  \n",
       "75%       0.000000     1.000000  \n",
       "max       1.000000     1.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, \n",
    "#dispersion and shape of a dataset’s distribution\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting train and test\n",
    "train = data[data['Type'] == 'train'].drop(columns = ['Type', 'Survived'])\n",
    "train_ = data[data['Type'] == 'train']['Survived']\n",
    "\n",
    "test = data[data['Type'] == 'test'].drop(columns = ['Type', 'Survived'])\n",
    "\n",
    "X_train = train.values\n",
    "y_train = train_.values\n",
    "\n",
    "X_test = test.values\n",
    "X_test = X_test.astype(np.float64, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"network\"></a>\n",
    "## 3. Create the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensorflow\"></a>\n",
    "### 3.1. TensorFlow 2.0 & Keras\n",
    "\n",
    "TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks and it is used for machine learning applications such as neural networks. Tensorflow has grown to become one of the most loved and widely adopted ML platforms in the world. TensorFlow offers multiple levels of abstraction so you can choose the right one for your needs. Build and train models by using the high-level Keras API, which makes getting started with TensorFlow and machine learning easy.  \n",
    "\n",
    "Most notable additions in TensorFlow 2.0:\n",
    "* tf.keras  - default TensorFlow's high-level API for building and training deep learning models.\n",
    "* Eager execution -  is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later.\n",
    "* Distribution Strategy API -  makes it easy to distribute and train models on different hardware configurations without changing the model definition.\n",
    "* Standardized SavedModel file format - allows you to run your models with TensorFlow, deploy them with TensorFlow Serving, use them on mobile and embedded systems with TensorFlow Lite, and train and run in the browser or Node.js with TensorFlow.js.\n",
    "\n",
    "Keras is a user-friendly API standard for machine learning and it is the central high-level API used to build and train models in TensorFlow 2.0. Importantly, Keras provides several model-building APIs (Sequential, Functional, and Subclassing), so you can choose the right level of abstraction for your project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new architecture of TensorFlow 2.0 using a simplified, conceptual diagram:\n",
    "\n",
    "![Tensorflow Architecture](https://1.bp.blogspot.com/-4C_bx62kOI4/XfE3XQT422I/AAAAAAAABmY/AbMfOO8yzjctmg30IcgOBaU5UmcZNpAtwCLcBGAsYHQ/s1600/model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"layers\"></a>\n",
    "### 3.2. Keras building blocks\n",
    "Keras models are made by connecting configurable building blocks together.  \n",
    "In Keras, you assemble layers to build models. A model is a graph of layers. The most common type of model is a stack of layers: the tf.keras.Sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"simple-model\"></a>\n",
    "### 3.3. Build a simple model - 0.76555 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "#### 3.3.1. Model architecture\n",
    "\n",
    "Sequential model is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially.\n",
    "\n",
    "Keras Dense Layer implements the operation: **output = activation(dot(input, kernel) + bias)** where **activation** is the element-wise activation function passed as the activation argument, **kernel** is a weights matrix created by the layer, and **bias** is a bias vector created by the layer.  \n",
    "A neural network without an activation function is essentially just a linear regression model. The **activation function** does the non-linear transformation to the input making it capable to learn and perform more complex tasks. Most known activation functions are: **Sigmoid** , **Tanh** and **Relu**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model\n",
    "model1 = keras.models.Sequential()\n",
    "model1.add(keras.layers.Dense(18, input_dim = X_train.shape[1], activation = keras.activations.relu))\n",
    "model1.add(keras.layers.Dense(8, activation = keras.activations.relu))\n",
    "model1.add(keras.layers.Dense(1, activation = keras.activations.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 18)                306       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 467\n",
      "Trainable params: 467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# visualize the model\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# further explore the model\n",
    "weights, biases = model1.layers[1].get_weights()\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compile\"></a>\n",
    "#### 3.3.2. Compile the model\n",
    "\n",
    "tf.keras.Model.compile takes three important arguments:\n",
    "\n",
    "* optimizer: This object specifies the training procedure. Pass it optimizer instances from the tf.keras.optimizers module, such as tf.keras.optimizers.Adam or tf.keras.optimizers.SGD. If you just want to use the default parameters, you can also specify optimizers via strings, such as 'adam' or 'sgd'.\n",
    "* loss: The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, and binary_crossentropy. Loss functions are specified by name or by passing a callable object from the tf.keras.losses module.\n",
    "* metrics: Used to monitor training. These are string names or callables from the tf.keras.metrics module.\n",
    "* Additionally, to make sure the model trains and evaluates eagerly, you can make sure to pass run_eagerly=True as a parameter to compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling our model\n",
    "model1.compile(optimizer = keras.optimizers.SGD(), \n",
    "               loss = keras.losses.binary_crossentropy, \n",
    "               metrics = [tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is equivalent to:\n",
    "\n",
    "model1.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "#### 3.3.3. Train the model\n",
    "\n",
    "tf.keras.Model.fit takes three important arguments:\n",
    "\n",
    "* epochs: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n",
    "* batch_size: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n",
    "* validation_data: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.\n",
    "* validation_split: Float between 0 and 1. Same as validation_data, but the  but it use a fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/100\n",
      "712/712 [==============================] - 1s 1ms/sample - loss: 0.6796 - binary_accuracy: 0.6096 - val_loss: 0.6701 - val_binary_accuracy: 0.6425\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.6668 - binary_accuracy: 0.6096 - val_loss: 0.6525 - val_binary_accuracy: 0.6425\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.6534 - binary_accuracy: 0.6096 - val_loss: 0.6368 - val_binary_accuracy: 0.6425\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.6421 - binary_accuracy: 0.6096 - val_loss: 0.6228 - val_binary_accuracy: 0.6425\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.6323 - binary_accuracy: 0.6096 - val_loss: 0.6112 - val_binary_accuracy: 0.6425\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.6247 - binary_accuracy: 0.6096 - val_loss: 0.6001 - val_binary_accuracy: 0.6425\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.6174 - binary_accuracy: 0.6096 - val_loss: 0.5902 - val_binary_accuracy: 0.6425\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.6108 - binary_accuracy: 0.6096 - val_loss: 0.5817 - val_binary_accuracy: 0.6425\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.6049 - binary_accuracy: 0.6096 - val_loss: 0.5726 - val_binary_accuracy: 0.6425\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5988 - binary_accuracy: 0.6096 - val_loss: 0.5640 - val_binary_accuracy: 0.6425\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5931 - binary_accuracy: 0.6096 - val_loss: 0.5561 - val_binary_accuracy: 0.6425\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5878 - binary_accuracy: 0.6096 - val_loss: 0.5494 - val_binary_accuracy: 0.6425\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5829 - binary_accuracy: 0.6124 - val_loss: 0.5424 - val_binary_accuracy: 0.6425\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5784 - binary_accuracy: 0.6152 - val_loss: 0.5359 - val_binary_accuracy: 0.6536\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5739 - binary_accuracy: 0.6222 - val_loss: 0.5299 - val_binary_accuracy: 0.6704\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5695 - binary_accuracy: 0.6278 - val_loss: 0.5245 - val_binary_accuracy: 0.6704\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5655 - binary_accuracy: 0.6475 - val_loss: 0.5189 - val_binary_accuracy: 0.6872\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.5615 - binary_accuracy: 0.6503 - val_loss: 0.5144 - val_binary_accuracy: 0.7207\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 94us/sample - loss: 0.5578 - binary_accuracy: 0.6657 - val_loss: 0.5096 - val_binary_accuracy: 0.7207\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 94us/sample - loss: 0.5542 - binary_accuracy: 0.6784 - val_loss: 0.5046 - val_binary_accuracy: 0.7263\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5507 - binary_accuracy: 0.6812 - val_loss: 0.5000 - val_binary_accuracy: 0.7374\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5474 - binary_accuracy: 0.6924 - val_loss: 0.4962 - val_binary_accuracy: 0.7654\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.5446 - binary_accuracy: 0.7022 - val_loss: 0.4930 - val_binary_accuracy: 0.7654\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5414 - binary_accuracy: 0.7191 - val_loss: 0.4895 - val_binary_accuracy: 0.7709\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5386 - binary_accuracy: 0.7219 - val_loss: 0.4858 - val_binary_accuracy: 0.7933\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5358 - binary_accuracy: 0.7303 - val_loss: 0.4818 - val_binary_accuracy: 0.7933\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5331 - binary_accuracy: 0.7360 - val_loss: 0.4793 - val_binary_accuracy: 0.8212\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5307 - binary_accuracy: 0.7430 - val_loss: 0.4764 - val_binary_accuracy: 0.8212\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5281 - binary_accuracy: 0.7542 - val_loss: 0.4728 - val_binary_accuracy: 0.8268\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5257 - binary_accuracy: 0.7598 - val_loss: 0.4700 - val_binary_accuracy: 0.8380\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5233 - binary_accuracy: 0.7626 - val_loss: 0.4672 - val_binary_accuracy: 0.8324\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5212 - binary_accuracy: 0.7711 - val_loss: 0.4641 - val_binary_accuracy: 0.8268\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5189 - binary_accuracy: 0.7683 - val_loss: 0.4615 - val_binary_accuracy: 0.8380\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.5167 - binary_accuracy: 0.7683 - val_loss: 0.4596 - val_binary_accuracy: 0.8324\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5147 - binary_accuracy: 0.7781 - val_loss: 0.4568 - val_binary_accuracy: 0.8380\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5128 - binary_accuracy: 0.7795 - val_loss: 0.4543 - val_binary_accuracy: 0.8492\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5109 - binary_accuracy: 0.7823 - val_loss: 0.4516 - val_binary_accuracy: 0.8492\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5091 - binary_accuracy: 0.7823 - val_loss: 0.4498 - val_binary_accuracy: 0.8492\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5074 - binary_accuracy: 0.7879 - val_loss: 0.4486 - val_binary_accuracy: 0.8603\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5056 - binary_accuracy: 0.7992 - val_loss: 0.4477 - val_binary_accuracy: 0.8603\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5041 - binary_accuracy: 0.8020 - val_loss: 0.4450 - val_binary_accuracy: 0.8603\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5023 - binary_accuracy: 0.8034 - val_loss: 0.4426 - val_binary_accuracy: 0.8603\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5009 - binary_accuracy: 0.8020 - val_loss: 0.4408 - val_binary_accuracy: 0.8603\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4993 - binary_accuracy: 0.8034 - val_loss: 0.4390 - val_binary_accuracy: 0.8603\n",
      "Epoch 45/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4978 - binary_accuracy: 0.8034 - val_loss: 0.4372 - val_binary_accuracy: 0.8603\n",
      "Epoch 46/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4963 - binary_accuracy: 0.8034 - val_loss: 0.4355 - val_binary_accuracy: 0.8547\n",
      "Epoch 47/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4949 - binary_accuracy: 0.8034 - val_loss: 0.4335 - val_binary_accuracy: 0.8547\n",
      "Epoch 48/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.4937 - binary_accuracy: 0.8020 - val_loss: 0.4316 - val_binary_accuracy: 0.8603\n",
      "Epoch 49/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4925 - binary_accuracy: 0.8020 - val_loss: 0.4300 - val_binary_accuracy: 0.8603\n",
      "Epoch 50/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4913 - binary_accuracy: 0.8020 - val_loss: 0.4288 - val_binary_accuracy: 0.8547\n",
      "Epoch 51/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4901 - binary_accuracy: 0.8020 - val_loss: 0.4277 - val_binary_accuracy: 0.8547\n",
      "Epoch 52/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4888 - binary_accuracy: 0.8020 - val_loss: 0.4279 - val_binary_accuracy: 0.8547\n",
      "Epoch 53/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4878 - binary_accuracy: 0.7992 - val_loss: 0.4257 - val_binary_accuracy: 0.8547\n",
      "Epoch 54/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4869 - binary_accuracy: 0.7992 - val_loss: 0.4251 - val_binary_accuracy: 0.8547\n",
      "Epoch 55/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4857 - binary_accuracy: 0.8048 - val_loss: 0.4228 - val_binary_accuracy: 0.8547\n",
      "Epoch 56/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4848 - binary_accuracy: 0.8034 - val_loss: 0.4223 - val_binary_accuracy: 0.8547\n",
      "Epoch 57/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4840 - binary_accuracy: 0.8020 - val_loss: 0.4202 - val_binary_accuracy: 0.8547\n",
      "Epoch 58/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4830 - binary_accuracy: 0.8020 - val_loss: 0.4194 - val_binary_accuracy: 0.8547\n",
      "Epoch 59/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4822 - binary_accuracy: 0.8034 - val_loss: 0.4180 - val_binary_accuracy: 0.8547\n",
      "Epoch 60/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4812 - binary_accuracy: 0.8020 - val_loss: 0.4172 - val_binary_accuracy: 0.8547\n",
      "Epoch 61/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4804 - binary_accuracy: 0.8048 - val_loss: 0.4160 - val_binary_accuracy: 0.8492\n",
      "Epoch 62/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4795 - binary_accuracy: 0.8090 - val_loss: 0.4150 - val_binary_accuracy: 0.8492\n",
      "Epoch 63/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4787 - binary_accuracy: 0.8090 - val_loss: 0.4152 - val_binary_accuracy: 0.8380\n",
      "Epoch 64/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4779 - binary_accuracy: 0.8104 - val_loss: 0.4143 - val_binary_accuracy: 0.8380\n",
      "Epoch 65/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4775 - binary_accuracy: 0.8104 - val_loss: 0.4134 - val_binary_accuracy: 0.8380\n",
      "Epoch 66/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4767 - binary_accuracy: 0.8090 - val_loss: 0.4124 - val_binary_accuracy: 0.8380\n",
      "Epoch 67/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4759 - binary_accuracy: 0.8104 - val_loss: 0.4112 - val_binary_accuracy: 0.8380\n",
      "Epoch 68/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4752 - binary_accuracy: 0.8104 - val_loss: 0.4101 - val_binary_accuracy: 0.8380\n",
      "Epoch 69/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4746 - binary_accuracy: 0.8104 - val_loss: 0.4092 - val_binary_accuracy: 0.8380\n",
      "Epoch 70/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4737 - binary_accuracy: 0.8076 - val_loss: 0.4084 - val_binary_accuracy: 0.8436\n",
      "Epoch 71/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4732 - binary_accuracy: 0.8076 - val_loss: 0.4080 - val_binary_accuracy: 0.8436\n",
      "Epoch 72/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4726 - binary_accuracy: 0.8076 - val_loss: 0.4067 - val_binary_accuracy: 0.8436\n",
      "Epoch 73/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4719 - binary_accuracy: 0.8076 - val_loss: 0.4056 - val_binary_accuracy: 0.8436\n",
      "Epoch 74/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4715 - binary_accuracy: 0.8076 - val_loss: 0.4050 - val_binary_accuracy: 0.8492\n",
      "Epoch 75/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4708 - binary_accuracy: 0.8076 - val_loss: 0.4048 - val_binary_accuracy: 0.8436\n",
      "Epoch 76/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4702 - binary_accuracy: 0.8076 - val_loss: 0.4036 - val_binary_accuracy: 0.8436\n",
      "Epoch 77/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4699 - binary_accuracy: 0.8076 - val_loss: 0.4022 - val_binary_accuracy: 0.8436\n",
      "Epoch 78/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4692 - binary_accuracy: 0.8076 - val_loss: 0.4013 - val_binary_accuracy: 0.8436\n",
      "Epoch 79/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4689 - binary_accuracy: 0.8076 - val_loss: 0.4009 - val_binary_accuracy: 0.8436\n",
      "Epoch 80/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4682 - binary_accuracy: 0.8076 - val_loss: 0.3993 - val_binary_accuracy: 0.8492\n",
      "Epoch 81/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4679 - binary_accuracy: 0.8076 - val_loss: 0.3996 - val_binary_accuracy: 0.8436\n",
      "Epoch 82/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4672 - binary_accuracy: 0.8076 - val_loss: 0.3994 - val_binary_accuracy: 0.8436\n",
      "Epoch 83/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4668 - binary_accuracy: 0.8090 - val_loss: 0.3979 - val_binary_accuracy: 0.8436\n",
      "Epoch 84/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4665 - binary_accuracy: 0.8076 - val_loss: 0.3989 - val_binary_accuracy: 0.8436\n",
      "Epoch 85/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4660 - binary_accuracy: 0.8076 - val_loss: 0.3985 - val_binary_accuracy: 0.8436\n",
      "Epoch 86/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4656 - binary_accuracy: 0.8076 - val_loss: 0.3979 - val_binary_accuracy: 0.8436\n",
      "Epoch 87/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4650 - binary_accuracy: 0.8076 - val_loss: 0.3974 - val_binary_accuracy: 0.8436\n",
      "Epoch 88/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4650 - binary_accuracy: 0.8104 - val_loss: 0.3955 - val_binary_accuracy: 0.8436\n",
      "Epoch 89/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4643 - binary_accuracy: 0.8076 - val_loss: 0.3972 - val_binary_accuracy: 0.8492\n",
      "Epoch 90/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4640 - binary_accuracy: 0.8062 - val_loss: 0.3983 - val_binary_accuracy: 0.8436\n",
      "Epoch 91/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4637 - binary_accuracy: 0.8090 - val_loss: 0.3967 - val_binary_accuracy: 0.8436\n",
      "Epoch 92/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4633 - binary_accuracy: 0.8090 - val_loss: 0.3975 - val_binary_accuracy: 0.8436\n",
      "Epoch 93/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4631 - binary_accuracy: 0.8090 - val_loss: 0.3979 - val_binary_accuracy: 0.8436\n",
      "Epoch 94/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4629 - binary_accuracy: 0.8104 - val_loss: 0.3952 - val_binary_accuracy: 0.8492\n",
      "Epoch 95/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4623 - binary_accuracy: 0.8104 - val_loss: 0.3939 - val_binary_accuracy: 0.8492\n",
      "Epoch 96/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4619 - binary_accuracy: 0.8090 - val_loss: 0.3934 - val_binary_accuracy: 0.8492\n",
      "Epoch 97/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4616 - binary_accuracy: 0.8090 - val_loss: 0.3937 - val_binary_accuracy: 0.8436\n",
      "Epoch 98/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4613 - binary_accuracy: 0.8118 - val_loss: 0.3920 - val_binary_accuracy: 0.8492\n",
      "Epoch 99/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4611 - binary_accuracy: 0.8090 - val_loss: 0.3908 - val_binary_accuracy: 0.8492\n",
      "Epoch 100/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4606 - binary_accuracy: 0.8062 - val_loss: 0.3902 - val_binary_accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val_acc: 80.47%\n"
     ]
    }
   ],
   "source": [
    "val_acc = np.mean(history.history['val_binary_accuracy'])\n",
    "print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'epochs': 100,\n",
       " 'steps': 23,\n",
       " 'samples': 712,\n",
       " 'verbose': 0,\n",
       " 'do_validation': True,\n",
       " 'metrics': ['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAE3CAYAAABlzQLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XecXGXd///XNb3vbC/Z3SSbHhLSQFoSgkiJiIAIKEVEAREUf/gVlaK3BcUbbhsSlSZFKSpFQEhCDSkECIQkkJ5s6mY328v0dv3+OLuTTd9NtsxmP08e5zGzM9ecc+3ZMO9zXec611Faa4QQQgiRmUz9XQEhhBBCHJwEtRBCCJHBJKiFEEKIDCZBLYQQQmQwCWohhBAig0lQCyGEEBlMgloIIYTIYF0KaqXUd5RSHyqlokqpxw5T9halVI1SqkUp9TellL1HaiqEEEIMQl1tUe8C7gL+dqhCSqlzgB8DZwLDgArg50dRPyGEEGJQ61JQa62f11r/B2g4TNGrgUe01qu11k3AL4GvH10VhRBCiMGrp89RHwes7PTzSqBQKZXbw9sRQgghBgVLD6/PA7R0+rnjuZd9WuNKqeuB6wEcDse08vLyHq6K2FcqlcJkkvGDfUH2dd+Q/dw3ZD/3jg0bNtRrrfMPV66ngzoA+Dr93PG8bd+CWusHgQcBxowZo9evX9/DVRH7WrBgAbNmzervagwKsq/7huznviH7uXcopbZ1pVxPHyKtBiZ1+nkSsFtrfbhz20IIIYQ4gK5enmVRSjkAM2BWSjmUUgdqjT8BfFMpNV4plQ3cCTzWY7UVQgghBpmutqjvBMIYl15d2f78TqVUuVIqoJQqB9BazwPuAd4GtrUv/9PjtRZCCCEGiS6do9Za/wz42UHe9uxT9nfA746qVkIIIYQAZApRIYQQIqNJUAshhBAZTIJaCCGEyGAS1EIIIUQGk6AWQgghMpgEtRBCCJHBJKiFEEKIDCZBLYQQQmQwCWohhBAig0lQCyGEEBlMgloIIYTIYBLUQgghRAaToBZCCCEymAS1EEIIkcEkqIUQQogMJkEthBBCZDAJaiGEECKDSVALIYQQGUyCWgghhMhgEtRCCCFEBpOgFkIIITKYBLUQQgiRwSSohRBCiAwmQS2EEEJkMAlqIYQQIoNJUAshhBAZTIJaCCGEyGAS1EIIIUQGk6AWQgghMpgEtRBCCJHBJKiFEEKIDGbp7woIMVAE3/+A+j//mVQodNiyOa2tbJnz56PanjKZcH3mRHyzZ2MfNw6l1EHL6liM4Hvv0Tp3HtFNm/Zbj3PaNHyzZ+OYcNwh1yOEyDwS1EIcRioUovZ3v6fpH//AWlKCbeSIw38mlcKc7T/q7Tb87VEaHnoY69ByfOfOxnXCNDCZ9ioTeOcd2l5/g1RLCyavF+fxx4N5TxkdCtP4xBM0/u1vWEtL8c0+F9eJJ4LZfFT162vmLD+O8eNQJukIFIOLBLUQhxD66CN23X478W3byb7qKgpu+f8wuVyH/VzlggVMnjXrqLefaGqi7Y03aJs7l4aHHqLhgQf2K2Nyu/Gc+Vl8587GPf00TDbbfmWSzc20vfkWrXPnpsN/ILIUF+M75xx8s8/Fcfzx/V0dIfqEBLUYdFLhMPV//jPhj1ccspxOJgmvWIG1pITyxx/HfdJn+qiGe1iys8m+5BKyL7mERGMjsa1b93pfmUzYx43DZLcfcj1mvx//xV/Cf/GXSDQ1EduypRdr3TviO3bQOm8+TU8+SeNjj2EpKSbb7WHbw4+ky1gKCsj//vexlQ7px5r2nMiGDbTNm0dk/Qbcp52K7+yzseTl9cq2opVbaJ03l8inq3GffBLec87BWljYK9s6EjoWI/Duu7TNfw0A7zln4zn1VNQBDky7K15dTeu8+QQXL0ZHo4cubLHgPukz+GbPxjZs2N7r2b2btvnzCSxchI5EjrpeHZTWusdWdqTGjBmj169f39/VOOYtWLCAWT3QyhvIQss/pvq224ht24ZzyhSU1XrI8o5x48i/+buY3O5ubUf2de9JtrbS9tZbBN58k4Zt2/H795xiiHz6KQAFP/4R/ksu6dfz8alQiMi69aBT3fqcTiQJLVtG67y5xDZtBpMJS2EhiepqMJlwndg+bmHUyKOuo04mCS//mNa5c4muXw9KYS0uJr5rFyiFc9pUfOfOZm04xJSpU496e0ci2dJC2+tv0Pbmm6RaWzF5vQCk2tow+Xx4P/c5vJ87E3NWVvdWrDWRNWtofXUu4RXGQbt91CjM2dmH/FgqECCyZo1Rftw4fLNnY3K5aJ03l/BHy0FrbCNHYMnJPWwVhv39iY+01iccrpwE9SAymMMjFY1S98f7aHz0UawlJRT/6le4Tz6p17Y3mPd1X9p3P8erqth1x52E3nsP9/TpFN/1S6xFRX1Wn1Q4TGDhIlrnziWwYMGRt6qUwnXCCXhnn4vvrLMw5+UR3biRtnnzaH117n49K0fLOWUKvtnnplvR0cpKWufNo23uPKIbN/boto6EyePBe+aZeGefi+fUUwGM1vXceUaABwJHvG772LH4zj0X3+xzsQ0d2qXPxGtq0vsnvHKlsZ5Ro4y/17mzsVcM79J6lFIS1GJvmRweqXCYhoceJtnaiu+cs3FOm9alQUPhFStoevoZHMdPNLoF8/P3ej9RV0fra6/R9ORTxCor8V96KQU//CFmT/dayN2Vyfv6WHKg/axTKZqeeYbae/8PZbHgnDy5T+qiE3HCK1ehQyHMubn4zjkb9/QZmByHPi1xILYRI7EWFhx4O1oT3biRZH390VbZ2Nbw4ViLiw/6frSyko/mz2fSpEk9sr3uUjYbjuOPP+DYC4BULEZk1Sp0LNbtdVuKi7EP71qoHkx81y5S0egRraerQS3nqEW/69wdrex2mv7xDyz5+Xjbj3KdkyfvF9qpaJT6+++n4ZG/oWw2Wl58kd13/aq9W/BcAFrnziO0bBlojX3UKMoefhjP9NP641cUfUiZTORcfjme6dOpvff/iO/e3WfbzvrCF/B9fjauE05AWXrn61UphWP0aBg9ulfWvy97RQWxceNwt7dkM43JZsN1wmGzrtdYS0p6fRsS1KLfpKJR6u67j8ZHH8NaVET5Y4/inDiRtgULaJs3j+Z//pOmv/8dS1HRnpG+kyYR+XQ1u277MbFNm/Ff8mUKfvQjEtXVtM6dR+vcudT8/BcA2CoqyLvxRnznnoN91Kh+/m1FX7OVl1P6p/v6uxpCHDUJatHnEo2NtL3+Bo1PPEFs8+b9uqOzzjuPrPPOIxkIEHj7bVrnzqPpqadofPxxY1BNfT2WvDzKHnoQz4wZAJhHjSJ/1CjyvvsdYu0TfthGjpTJPYQQA54EtegV8erqvWfwSqUIrVhB29x5BN9/H5JJbBUVlD30EJ4Z0w+4DrPHQ9b555N1/vkk29oIvPUWra+9jqUgn4JbbsHs8+33GaWUtJ6FEMcUCWrR45r/8x+qf3zbAd+zDi0n99pr8X1+NvbRo7vc4jV7vWRdcAFZF1zQk1UVQoiMJ0EtelSypYXae+7FMel4cq++eq/3bMOGHXbOaiGEEHuToBY9qu5P95Nsbqb8kYdxjBvX39URwpBKQaQZgvUQbd3/fbsX3Png8O81lzqJqPGZUD0k44ffjrcIfENADkZ7RrgZWnaAvxwc3ZzQpKu0hlgQgnUQbjR+7sxiN/5tuHLBfJAJkrQ2Pl+7FurWQeMW8BVDwXjIHwtZpUf1b6JLQa2UygEeAc4G6oHbtNZPHaCcHfgjcBFgBZYAN2itq464hmLAiKxfT9NTT5H9la9ISIu+oTXUrYctC2HrIqjbAOi934+0GEGbShx+fcpsfCHbXBBqPHCoH47dB/ljoGAcZA8H08BvD5Vt3wxLVu15IZWAUINxEBOsM/avMrUHWh6488CVY+zPbtHQthvq1kLtOmjbtect3xAj9ArGQfYwY1vuPOPR5oHGyvagXGs8hpsOv7lYyKh/Ity16jmzjX8f+/5Ng3XG/uhgdUG80xgdmxcKxrbXf7zxvGB817ZJ11vUc4AYUAhMBl5RSq3UWq/ep9z3gFOA44EW4CHgT8CXulwjMSBpran55S8xZ2WRf/N3+7s6/U6lksYXTrDOWHSy/Yul/YvMYoNkwjiC7ygTO/ztM/ffkDJage72L0eHv39ac7EQbJwPG+YDak99On9xd3yxWp0HX0fHl36o0SjX8XlntjEVZ9MW40u4di3UruaUTQvhnfYvZF8plEwG0z7h4Mhqr0P7/ndk7b2PtDYCOVi3J3jiIeML2Z23p/4Wx6H3gU4Zrb/a9pBZ+1/j73sMGAFQuc+LFueev6mn0Pj9A7Wwe42xD5OHmTP7YCwO40Bn+Ewj0Pzl0Lx9z9992RJIHGLGN0cW5I+DvNGH/3/B4uz0bzPfOLjYN4RjQePfZLBhTyDr5N5lyj6zp/VcMM7YH+Emo3XdUe+6dbD+Vfj4793fJYcroJRyAxcDE7TWAWCxUuol4Crgx/sUHw7M11rvbv/sM8Dvul0rMeC0/vcVwh9+RNEvf4HZf3S3dxxQYkGo+sj4Yu5oBdRv4PRQPSw8xOdsHuOz9PDMgCaLse6eCGubp/2Lp/3oP3+M8Vpn9Rth9fOwfh7Eg+DMMVoTh/qitrqNA5XOEtG9WyD7UmajxZbq1P3sH0qzfyKFJ30Zhs8wWq+Z1OWstfE7ZcDsj0dr0aJFzGi/FBIwDoYOdsAF7b97uNvznAPGevc92OosldxzOqLj4CraarSy88cZpx8y4d+BKweGnmosnQXq9nxX/PxbXVpVV1rUo4Gk1npDp9dWAqcfoOwjwB+VUiVAM3AFMPdAK1VKXQ9cD5Cfn8+CBQu6VGFx5AKBQK/sZxUOk3vXXaSGDmVlbi4Mkr9lduPHjFn/JxxRo8srbnETcpUTzJpKm9+DcucTt2YRs/kBhTXegjXegi3WiiXRStLsJmbLai+TRdLsALr3BaN0CksiiDXejC3WijXegjnZxW68w7DG23BXb8K1eQEmffDzs3GLl7r86dQWTKfZf5wRqlpjTobbf9+WTo9GHdU+LRKtLMStPmI2P3FrFnGrF1Mqhi3WnP6s0kmC7jJCrjKC7jJSZgeBQABPmwdWbQe298jvLfYXiCRZsPTD/q7GQeQZSwDYsR4YKNNRd31mua4EtQejG7uzFsB7gLIbMP5vqQKSwCfAdw60Uq31g8CDYMz1LfMi976emH860dBA4+NPkArvCYPY5k0EW1oY8dCDOAfDPYKjbfDaT2DVo5A3Bi66H0omY/UWk6UUWRxjc30nE9C0FerX79/l6MrDOvRUSsxWen8ixf0dU/s5g8l+7l9dCeoAsO/MEj6g7QBl/wI4gFwgCPwQo0Xde7cpEn1GJ5NU3fJ9Qh9+iMmzdxdo7rdvGBwhvXUx/OdG45zZqd+FM+4E62HOXQ50ZgvkjTQWIUSf60pQbwAsSqlRWuuO+51NAvYdSNbx+h1a60YApdSfgF8opfK01j1zqxfRb+r/8ldCH3xA8a9/jf9LF/V3dfpO0zZY/YJxLrZ6pXEu9Jq5MPSU/q6ZEGIQOGxQa62DSqnnMQL3WoxR3xcAB7qVyjLga0qpBUAIuBHYJSE98AXfe5/6OXPIuuCLZF10YX9Xp+cE66F2jTGwo7Fyn0t4NOxaAVXt5+aGTIOzfwUnXAO23r1NphBCdOjq5Vk3An8DaoEG4Nta69VKqRnAXK11Rz/oD4D7gI2ADfgU45pqMYAlGhrYdeut2IYNo+inPx14M4tFWmDVv6C1qv3ym/bRok1bjZGjHaxuY3KDzvzl8LmfwXEXGaNKhRCij3UpqNu7svdrRmmtF2EMNuv4uQFjpLc4RuhUil0/+jHJlhbKHn4Ik3sAtSRTKVj1T3j9pxCsNS5dSl9PmwtjzjUu5yhoX7zFmXFZhxBCdDLwp8wRvSbR2EjDgw8RXLyYop/9DMeYMf1boWTCOFccqGkP2LEHn65x1wp49VbY+QGUngiXPwMlUyWIhRADjgS12EuiqYm2N97Y63aUWRd8Ef9ll/ZfpVJJ+ORZeOc3xnnkzuw+yB25d5d1KgE7PzRmHLrgzzDpq3vP3yyEEAOIBPUgpZNJdnzrBiOMO4sbE1sc6e0oe1SgDirfhnfugYaNUDgRvvIUlJ/SPjVfxyCwzXsPAjNb4dTvwIwfgHMQzZImhDgmSVAPUk3//CfBxYvJ+vLFWHJy06+bnA48p5/e97ejjLTChnmwc9meuXE7Bnrlj4NLn4Cx5+9pGR9oaj4hhDgGSVAPQonGRur+8Edcp5xM8S9/2X+juKMBI5xXvwAbXzfmhu64y8yY2cb80kUTYOh06boWQgxaEtSDUN3v/0AqFKLojjv6J6RTSVh6Pyz4jXHTAk+RcW3ycV8yBn5JKAshRJoE9SAT/uQTmp99lpyvfx37yH6YErJhM/zn27DjfRjzeTjlO1B+8qHvliOEEIOYBPVgkkpR88u7MOflknfTjX2+bT54EN74mXGLw4sehOMvlculhBDiMCSoBxHH0veIrFpFyT3/i3mfm2r0mkQM1r4ES+fAruUw8iz44n3g6497LQkhxMAjQT1IJJqa8P7nBZzTpuE7//ze32BrNXz0GHz0KAR2Q04FXDAHJl8hrWghhOgGCepBILB4CdV33okKhSm6sxcHkEVaYN0r8OnzxvXPqSSMOgs+cz2MOFMGiQkhxBGQoD6GJQNBau+5h+Z//QvbiBE0fv1qxo8b17MbSaVg3X9hxVOw+U1IxiCrHE65CaZeDbkjenZ7QggxyEhQH4N0IkFg4SJ233UX8epqcr75DfJvvpkdS5f24Ea0EdBv3w21q8FbAideBxO+ZNwOUrq3hRCiR0hQHyN0Mklo2Ye0zptL22uvk2xsxDZ0KEOffBLX1Ck9t6F4GDa/ZVwDXbPKmGf7Sw8bAS2XWAkhRI+ToB5gtNZU33Y7kdWf7vV6or6BZFMTyunEe8YsvLNn45k5E5PdfpA1ddGOD2DD/D1zazduATRkD4cL/woTLwGz/DMSQojeIt+wA0zw3Xdp+c9/cJ1wAuacnPTrjuMm4Jk1C8/pMzE5nT2zsTUvwb+/bjzPHQFFE2Hipcbj6HOMm18IIYToVRLUA4jWmvo5f8ZSVETZ3x7BZLP13sbWvQLPXmOcb77yWXBk9d62hBBCHJRcLzOAhN57j/Dy5eRef13vhvT6efCvq6F4koS0EEL0MwnqAUJrTd2cOVgKCvBffHHvbWjjG/Cvq6DwOLjyeQlpIYToZxLUA0Tog2WEP/yI3OuuO/oBYgeSSsIHD8Ezl0P+GLjqBXD6e347QgghukXOUQ8Q9XPmYMnPx3/pJT2/8u3vwas/gJpPYPhMuORxcOUc/nNCCCF6nQT1ABBatozQBx9QeNuPe7Y13VYDr/8PrHoGfEPgksdg/IUyWYkQQmQQCeoBoG7OnzHn5eG/7LKeWaHWsPIZmPsjSIRhxg9gxvfB5u6Z9QshhOgxEtQZLrB4CaH33qPgRz/C5HAc/QrbdsN/b4H1r0D5KcYdrWQ+biGEyFgS1BkstHw5VTffjK2iguzLLj26laVS5Ncuhj9fA7EgnP0rOPnbMu2nEEJkuIwI6vqwJhRL4LJlRHUyQujjj9lx7XVY8vMpf/RRTC5X91ZQvRIq32mf+nMt1K3nuHgQSqbCRX81RnYLIYTIeBmRjIG45qI57/KXK6dSke/p7+r0u/CKFey49jrM+XmUP/E41sKCrn84lYJFv4UFvwadAk8h5I+FqVextsXBuEt+InNzCyHEAJIR39h5riS1bREuuH8J914yiXMnFPV3lfqE1pr4tm3oRCL9Wry6mqpbvo85N5ehjz+OtbCw6ysMNsAL18OmN2DCl+Hc34AnP/327gULGCchLYQQA0pGfGu3pGp57ppy7n6pgRv+8RHfmlnBreeMwWI+dudj0ckku267jdaXXt7vPWtpKUMffwxrUTcOWHYsM26gEayF834HJ3xDLrMSQohjQEYENcCd732Hh694lEcWZPHAwko+qWrh/sunkuPuxTmt+4lOJqm+/Q5aX3qZnG98A+fECXveVArXSSdhyc7u+go/fhJevtm4Fvqbr0FJD95/WgghRL/KiKAusBYQSoS46a0beHz240wu83PbC59w/p8W8+DXpnFcybEz37ROpai+8ye0vPgieTd/l/wbbzy6FS6dA/Nvh4pZxoQlzm4EvBBCiIyXEX3LVmXlL5/7C3XhOq577To+N8HLszecQkprLv7Lu7y0cld/V7FH6FSK6p/8hJYXXiDvppuOLqS1hrd+ZYT0uC/C5f+SkBZCiGNQRgQ1wKT8Sdz32fvY3rqdG16/gYoCCy99ZzoTh2Rx89Mf86tX1hBLpPq7mkcsFYlQfcedtDz3PLnfvoG879x0FCtLwdwfwsJ7YMqV8OVHwdILN+oQQgjR7zImqAFOLj6Z3876LWsb1/Kt17+FzRbhyWtP5qqTh/LQoi188f7FrNnV2t/V7LbwihVsufAioyV947fJv/lmVHcGeqWS0LAZ1v4XFt4Lf78QPngQTvkOfPF+udxKCCGOYRkV1ACzymalw/ob879Ba7yRX144gYe+dgL1gRgXzFnMnLc3kUhmfus6FY1S+9vfsvXyK0jFopT/7ZHuh/TKf8LdZfCnqfDPK+Ctu6BxC5x9l7HIyG4hhDimZURTzNzURM1dv0r/fBzwSPBUFu5cyAsvfIEzyz/LRKubZ+NJ3tvewNZ7X+Dxv9g4bWQeflfmjgoPvreU2KbN+C/5MgU/+hFmTzcnc9m6BF68CYZMhSlXQcF4yB8Ndm/vVFgIIUTGyYigVoEgLS/vfT2xCzgrZSMYb6VxxYtErS5MyswUYEIiRXhnkta1mqjFjN1q6l4rtY9YcnIoe+hBPDNmdP/DDZuNFnT2MLj8nzJQTAghBqmMCOpEWSlj3n/vgO+taVjDDa/fgEmZ+NNn/8TE/IkANAZj3Dt/Pc8s206u28YPzx3Ll6eWYjJlXmB3W7gZnv6K8VxCWgghBrWMO0e9r/G543ls9mM4LA6umX8N87bOAyDHbePuL03kpZumU57j4ofPruKCOUuY92kNqZTu51ofhWTcmGGscQtc9g+5BaUQQgxyGR/UABVZFTx13lOMzx3Pre/cygMrH0BrI4wnlmbx7A2n8ttLJtEcjnHDPz7ic797h2c+2E40keznmneD1lDzKTx/PVS+Def/AYZN7+9aCSGE6GcDIqgBchw5PHz2w3yh4gvcv+J+bl98O9FkFACTSXHxtFLe/n+zuO+rU3DazPz4+U+Y/r9vM+ftTTQFY/1c+0OoXQdv/xrmfAb+ehqseRFm3WZcHy2EEGLQy4hz1F1lM9v49fRfM8w3jPtX3M/Gpo3cPeNuRmWPAsBiNvHFSSWcf3wxizfV88A7ldw7fz1/emsjF00p5ZvThzGyIANGTCeiRiC//wBUfQgoo/V80g0w/gJw5/V3DYUQQmSIARXUAEopvjXpW4zNGctP3/0pl/33Mr439XtcNf4qTMqULjNjVD4zRuWzvqaNR5ds4bnlO3n6g+3MGJXHFScN5cxxBVj78u5cWkPTVljxJHz0GATrIGcEnHM3TLgYvN24naUQQohBY8AFdYfTy07n+S8+z8+W/oz/+/D/WLRzEXdNv4si9963hhxT5OU3Fx/PreeM4an3t/Pk+9u54R8fke+1c8m0Ur5yYjnlua6eq5jWENgNtWuNpW6t0b1dtw6irYCC0efAZ66HijPANGDOPgghhOgHXQpqpVQO8AhwNlAP3Ka1fuogZacCfwCmAkHg11rrPx5q/Y5ILbzw7e7UG4Bc4D6ted4xkv+tWcaXnj2XWx0VXGgt2O+66lzgu8BNYzU1rRG21AepXhLhgyVQ6bFRnuOiNNuJzWzudj0A0Clo3m4Ec7hpz+vOHGOikuMvhfyxMPJzkDP8yLYhhBBi0Olqi3oOEAMKgcnAK0qplVrr1Z0LKaXygHnALcCzgA0oPWwlkmHYurg79d6zTeBi4DMmuNMNP2Ujr7Vu5H+CUHSAq7RMQAlQYoZEdopQNEkokiSxM0XtTnBYzbhsZpxWM4puXpPtKzHOMeePg4L2xZ0v03wKIYQ4YocNaqWUGyMLJ2itA8BipdRLwFXAj/cp/n1gvtb6yfafo8Daw20j4B4Kt3zSrYrvqwx4VKd4et3T/HH5H7nIbeaHJ/6QC0deeNBZyyyAD/Bqzepdrby4ooqXVu5id2MUr8PC+ZNKuHhqKVPL/Rk585kQQohjX1dOkI4GklrrDZ1eW4kxJfe+TgYalVLvKqVqlVIvK6XKe6KiXWFSJq4YdwXPnf8cY3LG8NN3f8p1r11HZXPlIT+nlGLCkCzuOG887/74TJ689iTOGlfIC8uruPgv73Lmb9/hj29sZHNdoI9+EyGEEMKgOiYOOWgBpWYA/9ZaF3V67TrgCq31rH3KbgAKgLOAT4B7gGla69MOsN7rgesB8vPzp/3rX/86ut9kHymdYklgCS83v0w0FeWzvs9ybta52E1dv29zOKH5sCbB4qoEG5pSaKDMa+KkYjOfKbJQ4BpYA8ECgQCe7t4YRBwR2dd9Q/Zz35D93DvOOOOMj7TWJxyuXFeCegqwRGvt6vTa/wNmaa3P36fsSmC51vqa9p9zMQaf+bXWLQfbxpgxY/T69esPV9cj0hBu4I/L/8gLm16gwFXArSfcytnDzk5fytVVu1sjvLKqmpdX7eLj7c0AjC/2ce6EIs6dUMSoAk/Gd48vWLCAWbNm9Xc1BgXZ131D9nPfkP3cO5RSXQrqrqTVBsCilBrV6bVJwOoDlF0FdE7+juf9lmC5zlx+cdov+Pvsv5PjyOHWhbdy2X8v4+3tb3O4g5TOCn0OvjF9OC/ceBqLfngGt39+LE6bmd+9voGzf7+QM3/7Dr9+dS1LNzcQHwD3yhZCCDEwHHYwmdY6qJR6HviFUupajFHfFwCnHqD4o8BzSqn7MIL8J8BirXVzD9b5iEwumMwz5z3Dq1te5a8r/8rNb9/M+Nzx3DT5JmYMmdGt1nBZjovrZ47g+pkjqG2N8Nqa3cxfXcOjS7bw4MJKvA4LM0flc8bYAmaOyqPA5+jK091QAAAgAElEQVTF30wIIcSxrKuXZ90I/A2oBRqAb2utV7efv56rtfYAaK3fUkrdDryCcUvpxcDlPV/tI2M2mTl/xPnMHj6b/1b+l7+u/Cs3vXkTUwqm8IMTfsDx+cd3e50FPgdXnjyUK08eSiCaYPHGet5eV8tb62t55ZNqAMYWeZkxKo/po/I5aXgODusRXqsthBBi0OlSUGutG4ELD/D6IsCzz2t/Af7SI7XrJRaThQtHXsh5FefxwsYX+POKP3PFq1dwzrBz+N7U71HmLTui9XrslvQ561RKs6a6lcWb6lm0sY7H393GQ4u2YLOY+MywHGaMymPGqHzGFXsz/ty2EEKI/jNgpxDtCVaTlUvHXMoXKr7Ao6sf5fHVj/Pm9jf5ypivcO3Ea8l15h7xuk0m45KvCUOyuOH0EYRjSd7f0sDijfUs2ljP3XPXcffcdeR57Jw6IpfTRuZy2sg8SrN7cDpTIYQQA96gDuoOLquLmybfxCWjL2HOijk8te4pntv4HFeOu5Krj7uaLHvWUW/DaTMza0wBs8YUAFDTEkm3tpdsauCllbsAGJrr4pSKXE4clsNnhudQmu2UFrcQQgxiEtSdFLgK+PmpP+fq467mryv+ykOfPMQz657ha8d9jSvHXYnH1nPXERZlOfjytFK+PK0UrTUbawMs2VTPkk31vPpJNc8s2wFAcZYjHdonDc9h5AC4DEwIIUTPkaA+gIqsCu45/R6+OfGbzFkxhzkr5vDEmie4fOzlXDHuCrId2T26PaUUowu9jC70cs1pw0mlNBtq2/hgSyMfbGnkvco9Le4ct40Th2Vz4rAcppRnM2GID7tFBqcJIcSxSoL6EMbkjOG+z97H6obVPLzqYR5Y9QBPrHmCS0ZfwtfGf41Cd+/cQ9pkUowt8jG2yMfXThmG1prtjSHebw/u97c0MH/1bgBsZhMThviYWp7N1KHZTCn3U5zl7JV6CSGE6HsS1F1wXO5x/P6M37O5eTOPfPIIT659kqfWPsUZ5WdwyehLOKn4pG7PdNYdSimG5roZmuvm0hOMEem1bRGWb2tm+fYmlm9r4on3tvHw4i2A0V0+pdzPpFI/x5f6mViahccuf2ohhBiI5Nu7G0b4R/DrGb/mxsk38vS6p3lx84u8vu11yrxlXDzqYi4adRE5jpw+qUuB15G+FAwgmkiytrqNj7c3sWKHEeCvflIDGHfZHJHvocASZat1CxNL/Ywv9uG0SZe5EEJkOgnqI1DqLeXWE2/l5qk388a2N/j3hn/zh+V/YM6KOcwePpuvjv0qE/Im9Gmd7BYzk8v8TC7zp19rCET5pKqFVTtbWLWzmWWVQd59eQ0AZpNiVIGH8cU+RhV6GV3oYXShlyF+JyaTDFYTQohMIUF9FOxmO+dVnMd5FedR2VzJ0+ue5qXNL/HS5peYmDeRr479KmcNPQuHpX+mEM312Pe6JGzBggWMm3oyq3a28MnOZlZVtbC0soHnP65Kf8ZtMzO22MdxJR1LFqMKPTJgTQgh+okEdQ+p8Fdwx8l38L2p3+OlzS/x9LqnuX3x7dz9wd2cX3E+F4++mNHZo/u7mhT6HJw13sFZ4/cMhGsJx9lU28b6mgDra1pZU93Kcx/t5ImlScBofQ/NdTGm0JtufY8p9DIsz43VPLBu9SmEEAONBHUP89g8XD7ucr4y9issq1nGcxuf498b/s1T655iYt5ELhx5IecMO6dHJlHpKVlOK9OG5jBt6J7z66mUZltjiNW7Wlhf08aG3W2sq2lj/uoaUu03HbOaFcPz3EZ4F3gZWeBhZIGHYXkuaYELIUQPkaDuJSZl4qTikzip+CSaI828XPkyz298nl++90t+88FvmFk6ky9UfIGZpTOxmW39Xd39mExGCA/Pc/OFTvcqicSTbK4LsHF3gPW729i4u41VO5t59ZNqOu4aajYpynNc6c93LBX5bop8DpmwRQghukGCug/4HX6uGn8VV467knWN6/hv5X95dcurvLn9TbxWL7PKZnHW0LM4dcip2M32/q7uITmsZo4ryeK4kr17BMIxI8A31wXYVGssW+qDvLu5nkh8z/253TYzFfkeRuS7qcj3MCzPzfBcN0PzXPgc1r7+dYQQIuNJUPchpRTjcscxLncct0y7hQ+qP+DVLa/y9o63ebnyZVwWF6eXns7Zw85m+pDp/TYI7Ug4beb0TUg6S6U0Na0RttQHqawPsrnWCPNlW5v4z4pde5XNdduM4O5ogee5GZbnpizHJdeBCyEGLfn26ycWk4VTh5zKqUNOJZ6Ks6x6Ga9te423tr/F3K1zcVlcnFF+BucMPYfThpyWkd3jXWEyKUr8Tkr8Tk4bmbfXe+FYkm2NQbbWh9jWEGRrQ5At9UEWbazj2Y927lU222WlNNtFWY6TsmwXZTkuhua6KM9xUeJ3yqA2IcQxS4I6A1hN1nRo33nynSyrWcb8rfN5Y/sbvFL5Cm6rmxOLTuSU4lM4peQUhvmGHRPneZ02c3qq1H0Fogm21hvBvbMpzM6mEDuawqyrbuONtbXEEnu6080mRYnfQXmOi/IcN0NzXZRluyjxOxiS7STPbZdrw4UQA5YEdYaxmCycUmIE8h0n38EH1R/w5vY3WbprKQt2LACgyF3EaSWnMWPIDE4qPqlH7+qVKTx2ywG70sHoTt/dFmF7Q4jtjcayrf35/NU1NAZje5W3mU0U+x2UZBkt+yF+B8V+J8VZDkraH71yflwIkaEkqDOY1WTltCGncdqQ0wDY0baDpbuWsnTXUuZvnc9zG5/DoixMKZzCjCEzOL3sdIb7hh8Tre1DMZkUxVlOirOcnFSRu9/7rZE4VU1hdjUby87mMFVNYapbIizdXE9NayR9iVkHr91Csd/Rvl4HRVkOirMcFPqM54VeB36X9Zjft0KIzCNBPYCUecsoG1PGpWMuJZ6Ks6J2BYurFrO4ajG/++h3/O6j31HuLef0stOZVTqLKYVTsJoGX0vR57DiK7Yyrnj/LnWARDLF7rYo1c1hdrVEjMf257tbI6ypbqWuLbrf52xmEwU+O0U+B4VZDorbQ7zA5yDPYyPfYyffayfLOfj2uRCi90hQD1BWk5UTi07kxKITuWXaLdQEa3hnxzu8vfNtnln3DH9f83c8Vg8nFZ9ktMpLTuvvKmcMi9nEEL+TIf6D3w40lkixuzXSvkSNx7YIu1si1LRGWF3Vwptrd+916VkHq1nhs8Lwte+mA73AZyfPYyfXYyfPYzOeu21YZBCcEOIwJKiPEUXuIi4bexmXjb2MUDzE0l1LWVS1iCW7lvDm9jcBKLAU8Pa7bzMpfxKT8yczLGtYr96ecyCzWUyU5Rijyw9Ga01LOE5tW5T6tih1gSh17Y+rNm5DmxVrdrXy1tpawvHkfp9XCnJcNvK9Rks832Mnz7snyNOL10aOS0JdiMFKgvoY5LK6OHPomZw59Ey01lS2VLKkagmvrH6FN7a9wfMbnwfAa/MyIXcC43LHMT53PONzxlPqLZXzsF2klMLvsuF32Rhd6N3rvQXO3cyadQpgBHogmqAhEKM+EKU+/Riltq093NuiVNYFqQ9EiSb2b6UrBdkuG3keG7luI9Bz3UbI+11Wsl02/E5re32s+F1WnFaz/C2FOAZIUB/jlFKM8I9ghH8E5XXlzDx9Jltbt7KydiUr61aypmENT6x5gkQqAYDX6uW4vOOYmDfRWPInkufMO8xWxKEopfA6rHgdVobluQ9ZtiPU02HeZgR6XaefG4IxPtnZTH0gRiCaOOi6bGYTPqcR2h0hnu2yku22pcM9u9NrHc+l5S5EZpGgHmRMykRFVgUVWRVcNOoiAGLJGJuaN7G2YS2rG1bzaf2nPPrpoyS0EQLF7mIm5U9iUv4kjs8/nnE547CaZcBUb+gc6sMPE+pgzL3eHIrTHI7RFIzTEo7RFIrTEo7TnH6M0RyKs7MpxKdVcZpCsQO22jv4HBZy3DayXDZ8DgseuwWvw4LHbsXr6HhuweOw4HVYyXZZ8Ttt+N1WvHaLtOKF6GES1AKb2WZ0feeO52IuBiCSiLCucR2r6laxqn4VK+pWMG/rPMC4D/fEvIlMK5zGtMJpTMqfhMt68HO5ovc4rGaKsswUZXVvutlwLElTKGYswTiNoRjN7c+bQjEag8Z7gWiCmpYIgWiCtkjikC14MCaf2RPm1nSou+0W3DYzLpsFt92M12ExRuc7O8Lfisfe8b5RVlr2QhgkqMUBOSwOJhdMZnLB5PRru4O7WVW/io9rP2b57uU89MlDPLDqAczKzPCs4QzzDWNY1jCG+oYyzDeMEf4ReG3eQ2xF9BenzYzTZkwA0x2plCYQSxCIGMHdGjFa7h2t9o5wD0QStEYSBKJxatsihOqTBGMJQlHjcd/r2A/EYTXhc+wJcq/Dgqs97I1HM/XVMTZbtuBtb+EbBwXm9rA3Qt9lM2O3mKSlLwYsCWrRZYXuQs5yn8VZQ88CIBgPsrJ2JR/u/pCNzRvZ1LyJBTsWpLvMwRiNPtI/klH+UYzLHcfx+cdT4i6RL80BymRSRkv4KGZy01oTjCVpi8RpDRth3xaJE4wmCUYTBGPGYyCaoDUcTx8QtEUS1LZGCcX3BH4knuKFTWsOu02zSeGymfHYLTitZmwWE3aLCVv74rR2Dvg9LX+nrePnPQcITpsZd/tzu9WMw2rCZpYDAdF7JKjFEXNb3ek5yjskUgl2BXZR2VLJpuZNxtK0ifer3yeeigOQ68hlUv4kJuZPZHT2aCqyKijxlMilYoOEUso4x223ULz/DLHd8uZbb3PCydNpi8bT3fOhWJJQe9AHowlC8fYDgGiSUMw4EIglUsSTKWKJFJF4iqZgmGAskS53oMvpDsWkjNMQLtue1rzHbsFlN+OwGGHusJpxWM3YrSbsFqOVb7cYr7vt5vTBgstmwW4xYTErLCbjIMBqUTitxkGCHBQMPhLUokdZTBbKfeWU+8qZVTYr/Xo8FWdT0yZW1q1kVd0qVtat5K0db6Xfd1qcDM8azkj/yPQyKnsUha5C+VISB2U2KbJcVrJcPTu4MZnShONGsIeiSUKxJOG4cRBgBLnxPBJPEYknicSNMsaSSPcINAZjROJJoomOcimiieQBJ8rpKpMCl82Cw2rGaTMZAW41Wvc2856At5oVdosJp804QHC2HyhYzcZ7xqNxsOCymXHYzOl1Wdrft5iMx+ZIitZIHIfFjNWs5P/JPiZBLfqE1WRN34v7K2O/AkBLtIXKlko2N29mc/NmNjVvYumupby0+aX057xWL6OyRzE6ezRjcsYwJnsMI7NH4rR079yqEN1hNu1p9dMLwyy01sSSKSPAY/uEfKcWfzyZIpE0yoZjyT0HD50OEjpe73gvkdLEk5pE+/rD7QcSkXiSeLILgwMOZsFrgLFvHJ1OG9gs7a3+fQ4SLCZTe+/Bnh4Em8UoZzyq9gMFc7pnoaMnQSmFAkxKpXsrHO09Ch2nLqydDiYs7QcfZpOxXZPimDqYkKAW/SbLnsWUgilMKZiy1+st0ZZ0l/nG5o2sb1zPS5tfIrQ+lC4zxDOE4VnDqciqYHjWcEo8JRS5iyhyFckIdJHxlFLtAWU+qvP93ZVIptqDPEU8aTxG4ynjvH8sSaQ99ONJTSKVItke+p+uWUvZ8JHpA4NQLJk+dRBrf4x3Wney/TEUTOzVixBPGuUT7dtOdGVU4RHqCPL0wYHZhMkEZqUwmRQmpbCYOsLdeM3afnCRPjBoP01hMat0b0XHwYHZ1OngwGzC2t770FHGpIz3TIr0885jI+wWc5d/FwlqkXGy7FnpS786pHSKqkAVGxo3sLF5I5UtlVQ2V7KsZhnRZHS/z5d5yhjhH8FI/8j0Y6G7UM6Di0HNYjZhMRst1O7Ia9vErOnDe7w+qZQmmjCCvOP0gNHq12gNKW2chogmkumegVB7j0MiqYmn9oR+MqVJpHT6MZ5MEe8Yi5DUxBIpUtp4P6k1WmsSSU1K7/lcPJkiEE1Q12bMEBiOJYkmkultxZNGub4mQS0GBJMyGXcP85Zx5tAz068nU0mqg9XUBGuoDlazO7Sb6kA129q2sWTXEl7c/GK6rNPiZKhvaPrysWFZw6jIqmCYb5i0woXoByaTar9UsHsHDv0ptdcBgXGA0NFL0HGaouN142BDk9JGb0YskWo/MDEOTi78365tU4JaDGhmk5lSbyml3tIDvt8caWZT8yYqWyrZ0rKFra1bWV2/mte3vU5K7xnQU+wuZphvGCWeEgpcBRS6Cil0F5LryMVv95Nlz8JpcR5T572EEN1nMilspo7vgb45wJCgFsc0v8PPCUUncELRCXu9HkvG2N66nS2tW6hsrmRL6xa2tmxl486NNIQb0OzfvWUz2ciyZ+G1efHYPHitxmO+M5/hWcMZ4R9BRVYF2Y7svvr1hBCDgAS1GJRsZhsjs0cyMnskDN37vXgqTn2ont2h3TSEG2iONtMSazEeoy20xdoIxAK0xlqpClSxMLSQcCKc/ny2PRs/fl5f/Dpl3jLKveUM8Q6hyFVEnjMPs2ngdPMJIfqfBLUQ+7CarBR7iin2FHepfEqn2B3czeaWzVQ2V1LZUsmq7at4v/r9vS41AzArM3nOPIrcRXu62F2FxnN3YfpnuemJEKKDBLUQR8mkTOlgnz5kOgALYguYNWsWkUSEHW070gPedod2szu4m5pQDZuaN7GkagmhRGi/deY6cilyF5HvyiffaSx5rjwKXYWUuEso8ZTIADghBgkJaiF6kcPiYFT2KEZljzpomUAskA7w3aHd1ARrqAnVsDu4m12BXaysXUlTtGm/z2XbsynxlFDsLjauIW9fCl2F5DnzyHXmysQwQhwDJKiF6GcemwePzcMI/4iDlokn4zREGqgJ1rArsItdwV1UBarS86ov2bVkr/PkHdxWtzFy3eE3Rq/bssiy71l8Nl96gJxCkdIpkjpJSqewm+2UekvJtmfLaHch+pEEtRADgNVsTbeYO996tIPWmtZYa7p7vSHcQEOkgYZwA/XhepqjzdSF6tjUtInmaPMBu9sPxm11U+opZYhnCD67D7vZjtPixG6247P5GOIdQqmnlDJvmXTHC9ELJKiFOAYopdKt5DE5Yw5bPp6M0xprpSXWQmu0lbZYGxqNWZkxKRNmZSaUCLGzbSc7AzvZ0baDba3bCCaCRBIRosnoAVvwOY4cPFYPNrMNu9mO3WzHbXUzxDOEUq8R9kM8Q/Db/bisLlxWF1aTDJwT4lAkqIUYhKxmK7nOXHKduUe8Dq01LdEWqgJV7AjsYGfbTqoCVQTjQWLJWHqpD9ezonYFbfG2A67HZrLhsXnw2/1kO7LJtmeT7cjGY/UYYW5x4ba6cdvc6a77jklotO776RyF6GsS1EKII6KUMs59O/wcl3fcYct3hHpVoIq2WBvBeJBQPEQoEaIt1kZztJmmSBNbWrawvHY5wXhwv3nc92XChPcZLx6rB6/Ni9PiRGtNSqeMhRQOswOXtT3s2xev1Ws82ozHzhPYeG3G+hwWR0/tKiGOSpeCWimVAzwCnA3UA7dprZ86RHkbsArwaK0PPLejEGJQ6eiaH587vsufiafihBNhQnEjzFtjrTRHm2mNGo+rN64muzibtrgxCU04EUYphVmZUUphwkQ0GaUp0sTOtp0E40EC8cABu+335TA78Nl9+O3GQDyfzYfX5k0/diwdBwlemxeXxYXT6sRlceGwOOQmMKJHdLVFPQeIAYXAZOAVpdRKrfXqg5S/FagFPEdfRSHEYGU1WbHarPhsPorcRfu9v6B+AbNOntXt9SZTSYKJIIFYIN26D8QD6Vnn2uJttESN2eg6ZqTb2rqV1phxPr8rQQ9G2FvNVuP3aF+8Nq8R/g6/MYud3Y/Pboy+7+jad1vdOCyO9MA9m9lmHHygZAT+IHTYoFZKuYGLgQla6wCwWCn1EnAV8OMDlB8OXAl8H3ioZ6srhBBHz2wy47P58Nl8R/T5jsF4gXggHeyBWIBQIkQ4HjYeE2EiiQjxVJxYMkY8FSeajBKIB2iONlNVX0VTtIm22IHP3R+MSZkwYcJpdaYvr+sI+BxHjrE4c8h15OKz+fbq8ndZXdjNdmnpDzBdaVGPBpJa6w2dXlsJnH6Q8n8Cbge6dsgphBADTE8MxuuQSCWMEfjRFlqiLbTGWgnFjaCPJqPpUfYpUmit09e5hxPh9GdaYi3sCu6iMdx40EF7nVlMFmwmY2S+1+ZNT5CT58zDb/eT0ikSqQTxVJxEKkFVYxXLli3DbDJjURasZis59pz053IduZhMJtpibelTFKF4CIvJsldvgtVsxWF2YDPb0j0GHYvNbMNi6vqwqWQquWdbiRB+u59cZ+5BryLQWqd/p47FZXENiEsKu7JXPEDLPq+1AN59CyqlLgIsWusXlFKzDrVSpdT1wPUA+fn5LFiwoCv1FUchEAjIfu4jsq/7xrG6nx3t/3WJpX1xAdkQ13ECyQBtyTbCqTARHSGSihDVUSKpCAmdSC9xHSeUDNHW0kZVYxWtqVbCqXD7ai3pS/W01ry39j1StE+IQ+pQNTpiJky4TC78Fj/Z5mz8Fj8+k49gKkhLsoXWZCutyVYCyQBhfeC2oMfkwWf2YVEWIqkIER0hmooS1QcemJhjzqHIVkSxtZgiaxFesxe3yY3b5MZlcuE0Ofu9B6IrQR0A9u0f8gF7Hba1d5HfA3y+KxvWWj8IPAgwZswYPWvWrK58TByFBQuM+adF75N93TdkP/e8lE7tdy583/2cSCVoijRRH66nIWJMqqO13mugncvqIqmTxJNGqzyWMi7X69xLEElG9rqUL5qM0hRtMqbRDdawNbiVtngbLouLfFc+ed48RjlHke3ITg/w67hXfHO0mbpwHXWhOurCdSRSCWNEf6fL/Oxme7plbzVZaY4a96vf3LyZhS0LiafiB9wnFmXBbtnT8k/pFMlUkqROkkwlsZqt6fn4O+bm7xiH0HEpodfqTa/DbrbjMHf9qoKuBPUGwKKUGqW13tj+2iRg34Fko4BhwKL2P7ANyFJK1QAna623drlWQggh+kVXWo8Wk8W4YYwrv9frE0/G++RucolUgupANY3RxvQpheZoM4F4IH0QEU1EiSajmE1mzMpYLCYL4USYhnADteFaNjZupCHSQFIne6xuhw1qrXVQKfU88Aul1LUYo74vAE7dp+inQFmnn08F7gemAnU9U10hhBCDSV/d8tVislDmK6Nsrxg7Mimdoi2295UDgVjACPtOy7f4Vtfq1sXt3gj8DeOSqwbg21rr1UqpGcBcrbVHa50Aajo+oJRqBFJa65oDrlEIIYQ4BpmUKT1vQDnlBy3Xo0GttW4ELjzA64s4yLXSWusFgEx2IoQQQhwFuZhOCCGEyGAS1EIIIUQGk6AWQgghMpgEtRBCCJHBJKiFEEKIDCZBLYQQQmQwCWohhBAig0lQCyGEEBlMgloIIYTIYBLUQgghRAaToBZCCCEymAS1EEIIkcEkqIUQQogMJkEthBBCZDAJaiGEECKDSVALIYQQGUyCWgghhMhgEtRCCCFEBpOgFkIIITKYBLUQQgiRwSSohRBCiAwmQS2EEEJkMAlqIYQQIoNJUAshhBAZTIJaCCGEyGAS1EIIIUQGk6AWQgghMpgEtRBCCJHBJKiFEEKIDCZBLYQQQmQwCWohhBAig0lQCyGEEBnM0t8VOJh4PM7OnTuJRCL9XZVjRlZWFmvXru3vavQYh8NBaWkpVqu1v6sihBC9JmODeufOnXi9XoYNG4ZSqr+rc0xoa2vD6/X2dzV6hNaahoYGdu7cyfDhw/u7OkII0Wsytus7EomQm5srIS0OSClFbm6u9LgIIY55GRvUgIS0OCT59yGEGAwyOqj7m8fj6e8qCCGEGOQkqIUQQogMJkHdBVprbr31ViZMmMDEiRP55z//CUB1dTUzZ85k8uTJTJgwgUWLFpFMJvn617+eLvv73/++n2svhBBiIMvYUd+Z5Pnnn2fFihWsXLmS+vp6TjzxRGbOnMlTTz3FOeecwx133EEymSQUCrFixQqqqqr49NNPAWhubu7n2gshhBjIBkRQ//zl1azZ1dqj6xxf4uN/zj+uS2UXL17MV7/6VcxmM4WFhZx++uksW7aME088kW984xvE43EuvPBCJk+eTEVFBZWVlXz3u9/lvPPO4+yzz+7RegshhBhcpOu7C7TWB3x95syZLFy4kCFDhnDVVVfxxBNPkJ2dzcqVK5k1axZz5szh2muv7ePaCiGEOJYMiBZ1V1u+vWXmzJk88MADXH311TQ2NrJw4ULuvfdetm3bxpAhQ7juuusIBoMsX76cz3/+89hsNi6++GJGjBjB17/+9X6tuxBCiIFtQAR1f7voootYunQpkyZNQinFPffcQ1FREY8//jj33nsvVqsVj8fDE088QVVVFddccw2pVAqAu+++u59rL4QQYiCToD6EQCAAGBNr3Hvvvdx77717vX/11Vdz9dVX7/e55cuX90n9hBBCHPu6dI5aKZWjlHpBKRVUSm1TSl1+kHK3KqU+VUq1KaW2KKVu7dnqCiGEEINLV1vUc4AYUAhMBl5RSq3UWq/ep5wCvgasAkYArymldmitn+mpCgshhBCDyWFb1Eop9//f3r1HVVXtCxz//hBUlIFJmpma2inFBLZkhIqYdkSvDh8pkpxUQgd60xOW5UnLxyGNjqdOr9NDD6VpRglHS9Med1xSNIea4AszlZOPjMxHYQgaIjDvH3u7ryKPrW7YG/h9xtgj91xzr/1bkxU/5lpzzQlEAnOMMQXGmM3Ap8C4snWNMS8aY3YaY4qNMQeBNUCYs4NWSiml6gtHetSdgBJjTPZlZXuA+yv7kFhXTAgH/lXB9knAJICWLVuSnp5+xfZmzZqRn5/vQHjKUSUlJXWuTQsLC686d9xBQUGBW8ZV12g71wxtZ9dyJFH7ANmRKu4AABWCSURBVHllyvKAqhY2TsDaY3+vvI3GmCQgCaBz586mb9++V2zfv39/nVk72V3UpfWoL2ncuDHBwcGuDuMq6enplD2nlfNpO9cMbWfXciRRFwC+Zcp8gQq7ZiLyGNZ71eHGmAvXH55SSilVvzky6jsb8BSRuy4rswBlB5IBICITgJnAH40xOTceolJKKVV/VZmojTHngI+BeSLSVETCgOHA8rJ1RWQM8AIQYYw57Oxga9rRo0cJCAi4qjwuLo7vvvvOBREppZSqbxyd63sK4A2cAj4CJhtj9olIuIgUXFbveeBmIENECmyvRc4N2fXeffdd7r777hveT3FxsROiqR4lJSWuDkEppRQOJmpjTK4x5kFjTFNjzO3GmA9t5V8bY3wuq9fRGONljPG57PVodQVfE4qLi3nkkUcICgpi1KhRnD9/nr59+5KZmQmAj48Ps2bNwmKx0KNHD06ePAnA2rVrCQ0NJTg4mP79+9vLExISmDRpEgMGDCAmJobw8HB2795t/76wsDCysrLKjWX79u306tWL4OBgevXqxcGDBwFrUp0+fTqBgYEEBQXxxhtvAJCRkUGvXr2wWCzcd9995Ofns3TpUh577DH7PocMGWIfzenj48PcuXMJDQ1l69atzJs3j5CQEAICApg0aZJ9cZLvv/+e/v37Y7FYuOeeezh06BDjxo1jzZo19v2OGTOGTz/91Bk/AqWUqtdqxxSiX8yEE3udu89bA2HQgiqrHTx4kMWLFxMWFsaECRN4++23r9h+7tw5evToQWJiIk8//TTvvPMOs2fPpnfv3mzbtg0R4d133+XFF1/k5ZdfBmDHjh1s3rwZb29vli1bxtKlS3nttdfIzs7mwoULBAUFlRuLv78/mzZtwtPTk7S0NJ599llWrVpFUlISR44cYdeuXXh6epKbm0tRURGjR48mJSWFkJAQzp49W2Uv+dy5cwQEBDBv3jwA7r77bubOnQvAuHHjWLduHUOHDmXMmDHMnDmTESNGUFhYSGlpKXFxcbz66qsMHz6cvLw8tmzZwrJly6psX6WUUpXTZS6r0K5dO8LCrHO2jB07ls2bN1+xvWHDhgwZMgSA7t27c/ToUQBycnIYOHAggYGBvPTSS+zb9/9j74YNG4a3tzcAUVFRrFu3josXL7JkyZJKV9vKy8sjKiqKgIAApk2bZt9nWloajz76KJ6e1r+7/Pz8OHjwIK1btyYkJAQAX19f+/aKNGjQgMjISPv7DRs2EBoaSmBgIOvXr2ffvn3k5+fz008/MWLECMD6eFSTJk24//77+f777zl16hQfffQRkZGRVX6fUkqpqtWO36QO9Hyri3Xelorfe3l52csaNGhgv+8cHx/Pk08+ybBhw0hPTychIcH+maZNm9r/3aRJEyIiIlizZg2pqan2S+rlmTNnDv369eOTTz7h6NGj9ucajTFXxVVeGYCnp6d9ZS+wThhySePGjWnQoIG9fMqUKWRmZtKuXTsSEhIoLCyscG1usPa6k5OTWbFiBUuWLKmwnlJKKcdpj7oKx44dY+vWrQB89NFH9O7d26HP5eXl0aZNG4AqLwHHxcUxdepUQkJC8PPzc2ifS5cutZcPGDCARYsW2f9IyM3Nxd/fn+PHj5ORkQFYJzspLi6mQ4cO7N69m9LSUn788Ue2b99e7nddSuAtWrSgoKCAlStXAtaeedu2bVm9ejUAFy5c4Pz58wDExsby2muvAdC1q2vXEFdKqbpCE3UVunTpwrJlywgKCiI3N5fJkyc79LmEhASioqIIDw+nRYsWldbt3r07vr6+jB8/vtJ6Tz/9NM888wxhYWFX3G+Oi4vj9ttvJygoCIvFwocffkjDhg1JSUkhPj4ei8VCREQEhYWFhIWF0bFjRwIDA5k+fTr33HNPud910003MXHiRAIDA3nwwQftl9ABli9fzj//+U+CgoLo1asXJ06cAKBVq1Z06dKlyuNQSinlOKnsUmZN6dy5s7k0gvmS/fv306VLFxdFVLOOHz9O3759OXDgAB4e1fe3U3VPIXr+/HkCAwPZuXMnzZo1q7bvuZy7nic65WLN0HauGdrO1UNEdhhj7q2qnvaoXez9998nNDSUxMTEak3S1S0tLQ1/f3/i4+NrLEkrpVR9UDsGk9VhMTExxMTEXFH23nvv8frrr19RFhYWxltvvVWToV2T/v37c+zYMVeHoZRSdY4majc0fvx4vc+rlFIK0EvfSimllFvTRK2UUkq5MU3USimllBvTRK2UUkq5MU3UTuLj41PhtorWtVZKKaWqoolaKaWUcmO14vGsv2//OwdyDzh1n/5+/sy4b0aF22fMmEH79u2ZMmUKYJ0SVETYtGkTZ86c4eLFizz//PMMHz78mr63sLCQyZMnk5mZiaenJ6+88gr9+vVj3759jB8/nqKiIkpLS1m1ahW33XYbDz30EDk5OZSUlDBnzhxGjx59Q8etlFKqdqkVidoVoqOjeeKJJ+yJOjU1lS+//JJp06bh6+vLL7/8Qo8ePRg2bFi5q1RV5NKkJXv37uXAgQMMGDCA7OxsFi1axOOPP86YMWMoKiqipKSEzz//nNtuu43PPvsMsC7KoZRSqn6pFYm6sp5vdQkODubUqVMcP36c06dP07x5c1q3bs20adPYtGkTHh4e/PTTT5w8eZJbb73V4f1u3ryZ+Ph4APz9/Wnfvj3Z2dn07NmTxMREcnJyGDlyJHfddZd94YwZM2YwZMgQwsPDq+twlVJKuSm9R12JUaNGsXLlSlJSUoiOjiY5OZnTp0+zY8cOdu/eTatWra5Yz9kRFS2C8vDDD/Ppp5/i7e3NwIEDWb9+PZ06dWLHjh0EBgbyzDPPMG/ePGccllJKqVqkVvSoXSU6OpqJEyfyyy+/sHHjRlJTU7nlllvw8vJiw4YN/PDDD9e8zz59+pCcnMwDDzxAdnY2x44do3Pnzhw+fJg77riDqVOncvjwYbKysvD398fPz4+xY8fi4+NzxRrUSiml6gdN1JXo2rUr+fn5tGnThtatWzNmzBiGDh3KvffeS7du3fD397/mfU6ZMoVHH32UwMBAPD09Wbp0KY0aNSIlJYUPPvgALy8vbr31VubOnUtGRgZ/+ctf8PDwwMvLi4ULF1bDUSqllHJnuh51PVLd61G7grueJ7p+b83Qdq4Z2s7VQ9ejVkoppeoAvfTtRHv37mXcuHFXlDVq1IhvvvnGRREppZSq7TRRO1FgYCC7d+92dRhKKaXqEL30rZRSSrkxTdRKKaWUG9NErZRSSrkxTdRKKaWUG9NE7SSVrUednp7OkCFDyt02ePBgfvvtt+oKSymlVC2no75d7PPPP3fKfoqLi/H0dL8fpzEGYwweHvo3oVJKXQ/3+81ejhMvvMCF/c5dj7pRF39uffbZCrc7ez3qs2fPMmLECA4ePEifPn14++238fDwoEOHDmRmZlJQUMCgQYPo3bs3W7ZsoU2bNqxZswZvb2/eeecdkpKSKCoq4s4772T58uU0adKE2NhY/Pz82LVrF926dWPdunVs2bKFli1bUlpaSqdOndi2bRstWrS4Kp61a9fy/PPPU1RUxM0330xycjKtWrWioKCA+Ph4MjMzERH++te/EhkZyZdffsmzzz5LSUkJLVq04KuvviIhIQEfHx+mT58OQEBAAOvWrQNg0KBB9OvXj61bt7J69WoWLFhARkYGv//+O6NGjeK5554DICMjg8cff5xz587RqFEjvvrqKwYPHswbb7xBt27dAAgLC2PhwoUEBQU5/gNWSqk6Qrs5FYiOjiYlJcX+PjU1lfHjx/PJJ5+wc+dONmzYwFNPPVXhalhlbd++nZdffpm9e/dy6NAhPv7446vq/Oc//+HPf/4z+/bt46abbmLVqlUAjBw5koyMDPbs2UOXLl1YvHix/TPZ2dmkpaXx6quvMnbsWJKTkwFIS0vDYrGUm6QBevfuzbZt29i1axfR0dG8+OKLAMyfP59mzZqxd+9esrKyeOCBBzh9+jQTJ05k1apV7Nmzh3//+99VHu/BgweJiYlh165dtG/fnsTERDIzM8nKymLjxo1kZWVRVFTE6NGjef3119mzZw9paWl4e3sTFxdnX4AkOzubCxcuaJJWStVbtaJHXVnPt7o4ez3q++67jzvuuAOAP/3pT2zevJlRo0ZdUadjx472XmT37t05evQoAN9++y2zZ8/mt99+o6CggIEDB9o/ExUVRYMGDQCYMGECw4cP54knnmDJkiWMHz++wnhycnIYPXo0P//8M0VFRXTs2BGwJvgVK1bY6zVv3py1a9fSp08fex0/P78qj7d9+/b06NHD/j41NZWkpCSKi4v5+eef+e677xARWrduTUhICAC+vr72Y5o/fz4vvfQSS5YsITY2tsrvU0qpuqpWJGpXubQe9YkTJ65aj9rLy4sOHTo4vB61iFT6HqzTjV7SoEEDfv/9dwBiY2NZvXo1FouFpUuXkp6ebq/XtGlT+7/btWtHq1atWL9+Pd988429d12e+Ph4nnzySYYNG0Z6ejoJCQmA9Z5y2djKKwPw9PSktLTU/v7ytrg8riNHjvCPf/yDjIwMmjdvTmxsLIWFhRXut0mTJkRERLBmzRpSU1PJzMys8DiUUqqu00vflYiOjmbFihWsXLmSUaNGkZeXd93rUW/fvp0jR45QWlpKSkoKvXv3dviz+fn5tG7dmosXL1aafAHi4uIYO3YsDz30kL2nXZ68vDzatGkDwLJly+zlAwYM4M0337S/P3PmDD179mTjxo0cOXIEgNzcXAA6dOjAzp07Adi5c6d9e1lnz56ladOmNGvWjJMnT/LFF18A4O/vz/Hjx8nIyLAfZ3Fxsf04pk6dSkhIiEM9eKWUqqs0UVeivPWoMzMzuffee0lOTr6m9ah79uzJzJkzCQgIoGPHjowYMcLhz86fP5/Q0FAiIiKq/M5hw4ZRUFBQ6WVvsA6Oi4qKIjw8/Ir72LNnz+bMmTMEBARgsVjYsGEDLVu2JCkpiZEjR2KxWBg9ejQAkZGR5Obm0q1bNxYuXEinTp3K/S6LxUJwcDBdu3ZlwoQJhIWFAdCwYUNSUlKIj4/HYrEQERFh75V3794dX1/fKo9DKaXqOl2Puo7JzMxk2rRpfP3111dtq03rUR8/fpy+ffty4MCBSh/tctfzRNfvrRnazjVD27l66HrU9dCCBQuIjIzkb3/7m6tDuSHvv/8+oaGhJCYm6vPXSql6TweTOZGr16OeOXMmM2fOvKIsMTHR/jhVaWkpHh4eREVFMWvWrBqJ6XrExMQQExPj6jCUUsotaKJ2Indcj3rWrFn2pFybLn0rpZSycuvriu5w/1y5Lz0/lFL1gdsm6saNG/Prr7/qL2NVLmMMv/76K40bN3Z1KEopVa3c9tJ327ZtycnJ4fTp064Opc4oLCysU4mtcePGtG3b1tVhKKVUtXIoUYuIH7AYGAD8AjxjjPmwnHoCLADibEWLgRnmOrrFXl5e9ikrlXOkp6cTHBzs6jCUUkpdA0d71G8BRUAroBvwmYjsMcbsK1NvEvAgYAEM8L/AYWCRc8JVSiml6pcq71GLSFMgEphjjCkwxmwGPgXGlVP9EeBlY0yOMeYn4GUg1onxKqWUUvWKI4PJOgElxpjsy8r2AF3LqdvVtq2qekoppZRygCOXvn2AvDJleUB5D+SWrZsH+IiIlL1PLSKTsF4qB7ggIt86FrK6AS2wjjFQ1U/bumZoO9cMbefq0d6RSo4k6gLAt0yZL5DvQF1foKC8wWTGmCQgCUBEMh2Z71TdGG3nmqNtXTO0nWuGtrNrOXLpOxvwFJG7LiuzAGUHkmErszhQTymllFIOqDJRG2POAR8D80SkqYiEAcOB5eVUfx94UkTaiMhtwFPAUifGq5RSStUrjs5MNgXwBk4BHwGTjTH7RCRcRAouq/cvYC2wF/gW+MxWVpUkx0NWN0DbueZoW9cMbeeaoe3sQm6xHrVSSimlyue2c30rpZRSShO1Ukop5dZcmqhFxE9EPhGRcyLyg4g87Mp46goRaSQii21tmi8iu0Rk0GXb/ygiB0TkvIhsEBGHnuVTFRORu0SkUEQ+uKzsYdvP4JyIrLbNma+uk4hEi8h+W3seEpFwW7mez04iIh1E5HMROSMiJ0TkTRHxtG3rJiI7bO28Q0S6uTre+sLVPerL5xAfAywUEZ3J7MZ5Aj8C9wPNgDlAqu1/whZYR/HPAfyATCDFVYHWIW8BGZfe2M7jf2GdarcVcB542zWh1X4iEgH8HRiPdbKlPsBhPZ+d7m2sg4ZbY13X4X5giog0BNYAHwDNgWXAGlu5qmYuG0xmm0P8DBBwaXpSEVkO/GSMmemSoOowEckCngNuBmKNMb1s5U2xzjgUbIw54MIQay0RiQZGAt8BdxpjxorIC0AHY8zDtjp/APYDNxtjypssSFVCRLYAi40xi8uUT0LPZ6cRkf3AU8aYz23vX8I6cdUq4D2g7aUJrETkGDDJGPOlq+KtL1zZo76WOcTVDRCRVljbex9l5mO3PSd/CG336yIivsA8rHMGXK5sOx/CevWoU81FVzeISAPgXqCliHwvIjm2S7Le6PnsbK8D0SLSRETaAIOAL7G2Z1aZWSaz0HauEa5M1Ncyh7i6TiLiBSQDy2w9DG1355qPtaf3Y5lybWfnaQV4AaOAcKyXZIOB2Wg7O9tGrMn3LJCD9VbCarSdXcqVifpa5hBX10FEPLDOIFcEPGYr1nZ3Ettgmv7Aq+Vs1nZ2nt9t/33DGPOzMeYX4BVgMNrOTmP7ffE/WO/5N8W6EEdzrGMDtJ1dyJWJ+lrmEFfXSEQEWIy1NxJpjLlo23TFfOy2e3p/QNv9evQFOgDHROQEMB2IFJGdXN3OdwCNsJ736hoYY85g7d2VN6BGz2fn8QPaAW8aYy4YY37Fel96MNb2DLL9XrkkCG3nGuGyRH2Nc4ira7cQ6AIMNcb8fln5J0CAiESKSGNgLtZ7Tzrw5tolYU0K3WyvRVinzR2I9XbDUNs0u02x3sf+WAeSXbf3gHgRuUVEmgNPAOvQ89lpbFcqjgCTRcRTRG4CHsE6BiAdKAGm2h7/vHSFbr1Lgq1nXP14VrlziLs2pNrP9hzpf2NNHidEpMD2GmOMOQ1EAolYR92HAtGui7b2MsacN8acuPTCenmw0Bhz2nYeP4o1YZ/Cei9vigvDre3mY338LRvr6PldQKKez043Evgv4DTwPVAMTDPGFAEPAjHAb8AE4EFbuapmOte3Ukop5cZc3aNWSimlVCU0USullFJuTBO1Ukop5cY0USullFJuTBO1Ukop5cY0USullFJuTBO1Ukop5cY0USullFJuTBO1Ukop5cb+D+dq3n+n0O1mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "#### 3.3.4 Predict on the Test data and submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions, this model scores 0.76555 on Kaggle\n",
    "\n",
    "submission = pd.read_csv(\"../input/titanic/gender_submission.csv\", index_col='PassengerId')\n",
    "submission['Survived'] = model1.predict(X_test)\n",
    "submission['Survived'] = submission['Survived'].apply(lambda x: round(x,0)).astype('int')\n",
    "submission.to_csv('Titanic_model1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functional-model\"></a>\n",
    "### 3.4. Keras Model using Functional API - 0.77511 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is possible to use Keras Sequential API, for next model as the layers are added one after another, for the sake of this exercise I'll create the model using Functional API.  \n",
    "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.  \n",
    "Comparing with previuos model:\n",
    "  * I'm using Elu as activation function - ELU function looks a lot like the ReLU function but it has a nonzero gradient for z < 0\n",
    "  * I'm using Dropout regularization, which is the most popular technique, however L1, L2 and Max-Norm can be used as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(36, activation=\"elu\")(input_)\n",
    "drop1 = keras.layers.Dropout(rate=0.2)(hidden1)\n",
    "hidden2 = keras.layers.Dense(18, activation=\"elu\")(drop1)\n",
    "drop2 = keras.layers.Dropout(rate=0.2)(hidden2)\n",
    "hidden3 = keras.layers.Dense(8, activation=\"elu\")(drop2)\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(hidden3)\n",
    "model2 = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 36)                612       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 152       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,439\n",
      "Trainable params: 1,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(optimizer = 'nadam', \n",
    "               loss = 'binary_crossentropy', \n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch.  \n",
    "tf.keras.callbacks.EarlyStopping is usefull when you want to stop training when a monitored quantity has stopped improving. The number of epochs can be set to a large value since training will stop automatically when there is no more progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure EarlyStopping and ask to restore the best weights\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/100\n",
      "712/712 [==============================] - 1s 2ms/sample - loss: 0.7387 - accuracy: 0.5449 - val_loss: 0.5882 - val_accuracy: 0.7207\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.5875 - accuracy: 0.7149 - val_loss: 0.4963 - val_accuracy: 0.7877\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.5222 - accuracy: 0.7823 - val_loss: 0.4407 - val_accuracy: 0.8101\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 95us/sample - loss: 0.4985 - accuracy: 0.7809 - val_loss: 0.4055 - val_accuracy: 0.8268\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4736 - accuracy: 0.7921 - val_loss: 0.3846 - val_accuracy: 0.8547\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4745 - accuracy: 0.7893 - val_loss: 0.3743 - val_accuracy: 0.8547\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4678 - accuracy: 0.8020 - val_loss: 0.3663 - val_accuracy: 0.8603\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4541 - accuracy: 0.8020 - val_loss: 0.3589 - val_accuracy: 0.8659\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4552 - accuracy: 0.8174 - val_loss: 0.3562 - val_accuracy: 0.8715\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4421 - accuracy: 0.8048 - val_loss: 0.3505 - val_accuracy: 0.8715\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4397 - accuracy: 0.8216 - val_loss: 0.3492 - val_accuracy: 0.8659\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4521 - accuracy: 0.8006 - val_loss: 0.3480 - val_accuracy: 0.8659\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.4484 - accuracy: 0.8104 - val_loss: 0.3487 - val_accuracy: 0.8715\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4433 - accuracy: 0.8202 - val_loss: 0.3460 - val_accuracy: 0.8715\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4446 - accuracy: 0.8090 - val_loss: 0.3433 - val_accuracy: 0.8659\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4359 - accuracy: 0.8174 - val_loss: 0.3414 - val_accuracy: 0.8715\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4422 - accuracy: 0.8104 - val_loss: 0.3431 - val_accuracy: 0.8659\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4297 - accuracy: 0.8188 - val_loss: 0.3447 - val_accuracy: 0.8659\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 87us/sample - loss: 0.4343 - accuracy: 0.8216 - val_loss: 0.3422 - val_accuracy: 0.8659\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4375 - accuracy: 0.8076 - val_loss: 0.3409 - val_accuracy: 0.8659\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4338 - accuracy: 0.8160 - val_loss: 0.3412 - val_accuracy: 0.8715\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.4292 - accuracy: 0.8160 - val_loss: 0.3421 - val_accuracy: 0.8659\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.4302 - accuracy: 0.8104 - val_loss: 0.3426 - val_accuracy: 0.8659\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4287 - accuracy: 0.8272 - val_loss: 0.3484 - val_accuracy: 0.8715\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4267 - accuracy: 0.8090 - val_loss: 0.3436 - val_accuracy: 0.8659\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4376 - accuracy: 0.8118 - val_loss: 0.3378 - val_accuracy: 0.8715\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4352 - accuracy: 0.8146 - val_loss: 0.3365 - val_accuracy: 0.8715\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4141 - accuracy: 0.8244 - val_loss: 0.3366 - val_accuracy: 0.8771\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4166 - accuracy: 0.8216 - val_loss: 0.3336 - val_accuracy: 0.8771\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4248 - accuracy: 0.8188 - val_loss: 0.3315 - val_accuracy: 0.8771\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4191 - accuracy: 0.8188 - val_loss: 0.3303 - val_accuracy: 0.8771\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4220 - accuracy: 0.8132 - val_loss: 0.3293 - val_accuracy: 0.8827\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 95us/sample - loss: 0.4286 - accuracy: 0.8118 - val_loss: 0.3284 - val_accuracy: 0.8883\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 95us/sample - loss: 0.4207 - accuracy: 0.8244 - val_loss: 0.3274 - val_accuracy: 0.8827\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 92us/sample - loss: 0.4108 - accuracy: 0.8258 - val_loss: 0.3276 - val_accuracy: 0.8827\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4272 - accuracy: 0.8216 - val_loss: 0.3284 - val_accuracy: 0.8771\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4249 - accuracy: 0.8258 - val_loss: 0.3295 - val_accuracy: 0.8771\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4316 - accuracy: 0.8188 - val_loss: 0.3313 - val_accuracy: 0.8771\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4265 - accuracy: 0.8174 - val_loss: 0.3313 - val_accuracy: 0.8771\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4217 - accuracy: 0.8216 - val_loss: 0.3308 - val_accuracy: 0.8771\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 89us/sample - loss: 0.4239 - accuracy: 0.8174 - val_loss: 0.3314 - val_accuracy: 0.8715\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 90us/sample - loss: 0.4234 - accuracy: 0.8272 - val_loss: 0.3338 - val_accuracy: 0.8771\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 88us/sample - loss: 0.4252 - accuracy: 0.8230 - val_loss: 0.3324 - val_accuracy: 0.8827\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 91us/sample - loss: 0.4176 - accuracy: 0.8357 - val_loss: 0.3302 - val_accuracy: 0.8771\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val_acc: 86.41%\n"
     ]
    }
   ],
   "source": [
    "val_acc = np.mean(history2.history['val_accuracy'])\n",
    "print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "pd.DataFrame(history2.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission2 = pd.read_csv(\"../input/titanic/gender_submission.csv\", index_col='PassengerId')\n",
    "submission2['Survived'] = model2.predict(X_test)\n",
    "submission2['Survived'] = submission['Survived'].apply(lambda x: round(x,0)).astype('int')\n",
    "submission2.to_csv('Titanic_model2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hyperparameter\"></a>\n",
    "### 3.5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, in such way to be configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=[16], n_hidden=1, n_neurons=30, activation = 'relu', optimizer = 'SGD'):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    i = 1\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons/i, activation=activation))\n",
    "        if n_neurons > 20:\n",
    "            model.add(keras.layers.Dropout(rate=0.2))\n",
    "            i = i + 2\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras there is a wrapper for using the Scikit-Learn API with Keras models, which has 2 classes:\n",
    " * KerasClassifier: Implementation of the scikit-learn classifier API for Keras.\n",
    " * KerasRegressor: Implementation of the scikit-learn regressor API for Keras.\n",
    "\n",
    "**KerasClassifier** object is a thin wrapper around the Keras model built using build_model(). The object can be used like a regular Scikit-Learn regressor: we can train, evaluate and make predictions using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/100\n",
      "712/712 [==============================] - 1s 724us/sample - loss: 0.7399 - accuracy: 0.5112 - val_loss: 0.7218 - val_accuracy: 0.6648\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.6886 - accuracy: 0.6390 - val_loss: 0.6758 - val_accuracy: 0.6648\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 84us/sample - loss: 0.6611 - accuracy: 0.6517 - val_loss: 0.6414 - val_accuracy: 0.7039\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 93us/sample - loss: 0.6260 - accuracy: 0.6994 - val_loss: 0.6151 - val_accuracy: 0.7095\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.6211 - accuracy: 0.6938 - val_loss: 0.5929 - val_accuracy: 0.7095\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.6050 - accuracy: 0.6994 - val_loss: 0.5746 - val_accuracy: 0.7039\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.5835 - accuracy: 0.7093 - val_loss: 0.5585 - val_accuracy: 0.7263\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.5762 - accuracy: 0.7107 - val_loss: 0.5448 - val_accuracy: 0.7430\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5581 - accuracy: 0.7416 - val_loss: 0.5326 - val_accuracy: 0.7654\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.5641 - accuracy: 0.7374 - val_loss: 0.5216 - val_accuracy: 0.7765\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5531 - accuracy: 0.7331 - val_loss: 0.5116 - val_accuracy: 0.7709\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5498 - accuracy: 0.7346 - val_loss: 0.5025 - val_accuracy: 0.7709\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.5420 - accuracy: 0.7346 - val_loss: 0.4938 - val_accuracy: 0.7765\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5307 - accuracy: 0.7612 - val_loss: 0.4865 - val_accuracy: 0.7877\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.5229 - accuracy: 0.7725 - val_loss: 0.4793 - val_accuracy: 0.7933\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5275 - accuracy: 0.7556 - val_loss: 0.4728 - val_accuracy: 0.7821\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5164 - accuracy: 0.7809 - val_loss: 0.4675 - val_accuracy: 0.8045\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.5116 - accuracy: 0.7893 - val_loss: 0.4623 - val_accuracy: 0.8045\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.5127 - accuracy: 0.7542 - val_loss: 0.4571 - val_accuracy: 0.8212\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.5028 - accuracy: 0.7851 - val_loss: 0.4525 - val_accuracy: 0.8212\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 84us/sample - loss: 0.5056 - accuracy: 0.7921 - val_loss: 0.4483 - val_accuracy: 0.8212\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4961 - accuracy: 0.7949 - val_loss: 0.4443 - val_accuracy: 0.8324\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4935 - accuracy: 0.7795 - val_loss: 0.4399 - val_accuracy: 0.8268\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4990 - accuracy: 0.7978 - val_loss: 0.4363 - val_accuracy: 0.8436\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4882 - accuracy: 0.7978 - val_loss: 0.4335 - val_accuracy: 0.8380\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4933 - accuracy: 0.7865 - val_loss: 0.4299 - val_accuracy: 0.8436\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4923 - accuracy: 0.7935 - val_loss: 0.4274 - val_accuracy: 0.8380\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4857 - accuracy: 0.7921 - val_loss: 0.4241 - val_accuracy: 0.8436\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4835 - accuracy: 0.7949 - val_loss: 0.4212 - val_accuracy: 0.8436\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4841 - accuracy: 0.8006 - val_loss: 0.4189 - val_accuracy: 0.8436\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4683 - accuracy: 0.8104 - val_loss: 0.4166 - val_accuracy: 0.8436\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4833 - accuracy: 0.7921 - val_loss: 0.4152 - val_accuracy: 0.8436\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4824 - accuracy: 0.7879 - val_loss: 0.4132 - val_accuracy: 0.8436\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4835 - accuracy: 0.8006 - val_loss: 0.4110 - val_accuracy: 0.8436\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4846 - accuracy: 0.7978 - val_loss: 0.4092 - val_accuracy: 0.8436\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4725 - accuracy: 0.8118 - val_loss: 0.4076 - val_accuracy: 0.8492\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4760 - accuracy: 0.8132 - val_loss: 0.4056 - val_accuracy: 0.8492\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4633 - accuracy: 0.8118 - val_loss: 0.4041 - val_accuracy: 0.8436\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4678 - accuracy: 0.8034 - val_loss: 0.4019 - val_accuracy: 0.8436\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4683 - accuracy: 0.7978 - val_loss: 0.4003 - val_accuracy: 0.8436\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4574 - accuracy: 0.8104 - val_loss: 0.3986 - val_accuracy: 0.8436\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4653 - accuracy: 0.7963 - val_loss: 0.3970 - val_accuracy: 0.8436\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4647 - accuracy: 0.8048 - val_loss: 0.3962 - val_accuracy: 0.8436\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4725 - accuracy: 0.8020 - val_loss: 0.3951 - val_accuracy: 0.8436\n",
      "Epoch 45/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4541 - accuracy: 0.8118 - val_loss: 0.3936 - val_accuracy: 0.8436\n",
      "Epoch 46/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4603 - accuracy: 0.8146 - val_loss: 0.3930 - val_accuracy: 0.8436\n",
      "Epoch 47/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4653 - accuracy: 0.8020 - val_loss: 0.3922 - val_accuracy: 0.8436\n",
      "Epoch 48/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4706 - accuracy: 0.8076 - val_loss: 0.3916 - val_accuracy: 0.8436\n",
      "Epoch 49/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4737 - accuracy: 0.7978 - val_loss: 0.3910 - val_accuracy: 0.8436\n",
      "Epoch 50/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4674 - accuracy: 0.7949 - val_loss: 0.3902 - val_accuracy: 0.8436\n",
      "Epoch 51/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4655 - accuracy: 0.8048 - val_loss: 0.3895 - val_accuracy: 0.8492\n",
      "Epoch 52/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4670 - accuracy: 0.8048 - val_loss: 0.3884 - val_accuracy: 0.8492\n",
      "Epoch 53/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4599 - accuracy: 0.8104 - val_loss: 0.3875 - val_accuracy: 0.8492\n",
      "Epoch 54/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4601 - accuracy: 0.8090 - val_loss: 0.3867 - val_accuracy: 0.8492\n",
      "Epoch 55/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4556 - accuracy: 0.8174 - val_loss: 0.3865 - val_accuracy: 0.8492\n",
      "Epoch 56/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4582 - accuracy: 0.8020 - val_loss: 0.3857 - val_accuracy: 0.8492\n",
      "Epoch 57/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4534 - accuracy: 0.7978 - val_loss: 0.3848 - val_accuracy: 0.8492\n",
      "Epoch 58/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4534 - accuracy: 0.8076 - val_loss: 0.3843 - val_accuracy: 0.8492\n",
      "Epoch 59/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4576 - accuracy: 0.8160 - val_loss: 0.3839 - val_accuracy: 0.8492\n",
      "Epoch 60/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4671 - accuracy: 0.8034 - val_loss: 0.3836 - val_accuracy: 0.8492\n",
      "Epoch 61/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4464 - accuracy: 0.8090 - val_loss: 0.3831 - val_accuracy: 0.8492\n",
      "Epoch 62/100\n",
      "712/712 [==============================] - 0s 79us/sample - loss: 0.4560 - accuracy: 0.8146 - val_loss: 0.3823 - val_accuracy: 0.8492\n",
      "Epoch 63/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4675 - accuracy: 0.8160 - val_loss: 0.3820 - val_accuracy: 0.8492\n",
      "Epoch 64/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4628 - accuracy: 0.8020 - val_loss: 0.3815 - val_accuracy: 0.8492\n",
      "Epoch 65/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4471 - accuracy: 0.8160 - val_loss: 0.3812 - val_accuracy: 0.8492\n",
      "Epoch 66/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4550 - accuracy: 0.8104 - val_loss: 0.3808 - val_accuracy: 0.8492\n",
      "Epoch 67/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4457 - accuracy: 0.8104 - val_loss: 0.3801 - val_accuracy: 0.8492\n",
      "Epoch 68/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4571 - accuracy: 0.8062 - val_loss: 0.3795 - val_accuracy: 0.8492\n",
      "Epoch 69/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4576 - accuracy: 0.8076 - val_loss: 0.3795 - val_accuracy: 0.8492\n",
      "Epoch 70/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4547 - accuracy: 0.8090 - val_loss: 0.3791 - val_accuracy: 0.8492\n",
      "Epoch 71/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4501 - accuracy: 0.8174 - val_loss: 0.3788 - val_accuracy: 0.8492\n",
      "Epoch 72/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4493 - accuracy: 0.8118 - val_loss: 0.3780 - val_accuracy: 0.8492\n",
      "Epoch 73/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4542 - accuracy: 0.8048 - val_loss: 0.3776 - val_accuracy: 0.8492\n",
      "Epoch 74/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4546 - accuracy: 0.8146 - val_loss: 0.3773 - val_accuracy: 0.8492\n",
      "Epoch 75/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4483 - accuracy: 0.8160 - val_loss: 0.3768 - val_accuracy: 0.8492\n",
      "Epoch 76/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4598 - accuracy: 0.8076 - val_loss: 0.3764 - val_accuracy: 0.8492\n",
      "Epoch 77/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4537 - accuracy: 0.8090 - val_loss: 0.3756 - val_accuracy: 0.8492\n",
      "Epoch 78/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4518 - accuracy: 0.8104 - val_loss: 0.3749 - val_accuracy: 0.8492\n",
      "Epoch 79/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4438 - accuracy: 0.8160 - val_loss: 0.3746 - val_accuracy: 0.8492\n",
      "Epoch 80/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4541 - accuracy: 0.7992 - val_loss: 0.3742 - val_accuracy: 0.8492\n",
      "Epoch 81/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4498 - accuracy: 0.8118 - val_loss: 0.3737 - val_accuracy: 0.8492\n",
      "Epoch 82/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4591 - accuracy: 0.8076 - val_loss: 0.3735 - val_accuracy: 0.8492\n",
      "Epoch 83/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4673 - accuracy: 0.8034 - val_loss: 0.3730 - val_accuracy: 0.8492\n",
      "Epoch 84/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4459 - accuracy: 0.8132 - val_loss: 0.3727 - val_accuracy: 0.8492\n",
      "Epoch 85/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4441 - accuracy: 0.8146 - val_loss: 0.3722 - val_accuracy: 0.8492\n",
      "Epoch 86/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4581 - accuracy: 0.8062 - val_loss: 0.3723 - val_accuracy: 0.8492\n",
      "Epoch 87/100\n",
      "712/712 [==============================] - 0s 74us/sample - loss: 0.4401 - accuracy: 0.8230 - val_loss: 0.3714 - val_accuracy: 0.8492\n",
      "Epoch 88/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4445 - accuracy: 0.8132 - val_loss: 0.3711 - val_accuracy: 0.8492\n",
      "Epoch 89/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4429 - accuracy: 0.8174 - val_loss: 0.3706 - val_accuracy: 0.8492\n",
      "Epoch 90/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4464 - accuracy: 0.8230 - val_loss: 0.3703 - val_accuracy: 0.8492\n",
      "Epoch 91/100\n",
      "712/712 [==============================] - 0s 77us/sample - loss: 0.4519 - accuracy: 0.8174 - val_loss: 0.3700 - val_accuracy: 0.8492\n",
      "Epoch 92/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4428 - accuracy: 0.8062 - val_loss: 0.3698 - val_accuracy: 0.8492\n",
      "Epoch 93/100\n",
      "712/712 [==============================] - 0s 78us/sample - loss: 0.4415 - accuracy: 0.8202 - val_loss: 0.3693 - val_accuracy: 0.8492\n",
      "Epoch 94/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4414 - accuracy: 0.8216 - val_loss: 0.3692 - val_accuracy: 0.8492\n",
      "Epoch 95/100\n",
      "712/712 [==============================] - 0s 76us/sample - loss: 0.4496 - accuracy: 0.8034 - val_loss: 0.3684 - val_accuracy: 0.8492\n",
      "Epoch 96/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4484 - accuracy: 0.8104 - val_loss: 0.3683 - val_accuracy: 0.8492\n",
      "Epoch 97/100\n",
      "712/712 [==============================] - 0s 80us/sample - loss: 0.4549 - accuracy: 0.8090 - val_loss: 0.3683 - val_accuracy: 0.8492\n",
      "Epoch 98/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4469 - accuracy: 0.8090 - val_loss: 0.3682 - val_accuracy: 0.8492\n",
      "Epoch 99/100\n",
      "712/712 [==============================] - 0s 75us/sample - loss: 0.4454 - accuracy: 0.7992 - val_loss: 0.3674 - val_accuracy: 0.8492\n",
      "Epoch 100/100\n",
      "712/712 [==============================] - 0s 73us/sample - loss: 0.4379 - accuracy: 0.8146 - val_loss: 0.3666 - val_accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "history3 = model3.fit(X_train, y_train, epochs=100,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val_acc: 82.82%\n"
     ]
    }
   ],
   "source": [
    "val_acc = np.mean(history3.history['val_accuracy'])\n",
    "print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random search for hyper parameters.**\n",
    "\n",
    "In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.\n",
    "\n",
    "**Activation functions**:  \n",
    "* Relu - Applies the rectified linear unit activation function: max(x, 0)\n",
    "* Selu - Scaled Exponential Linear Unit (SELU):  scale * x if x > 0 and scale * alpha * (exp(x) - 1) if x < 0 \n",
    "* Elu - Exponential linear unit: x if x > 0 and alpha * (exp(x)-1) if x < 0\n",
    "\n",
    "![Activation](https://www.researchgate.net/publication/336539704/figure/fig3/AS:814073524535298@1571101661525/Functions-including-exponential-linear-unit-ELU-parametric-rectified-linear-unit.png)\n",
    "\n",
    "**Optimizers**:\n",
    "* SGD - stochastic gradient descent, is the \"classical\" optimization algorithm. In SGD we compute the gradient of the network loss function with respect to each individual weight in the network. Each forward pass through the network results in a certain parameterized loss function, and use the calculated gradients with the learning rate to move the weights in whatever direction its gradient is pointing.\n",
    "* Adagrad - scales down the gradient vector along the steepest dimensions\n",
    "* RMSprop - accumulate only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training)\n",
    "* Adam - stands for adaptive moment estimation, combines the ideas of momentum optimization and RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7175 - accuracy: 0.4400 - val_loss: 0.7114 - val_accuracy: 0.3613\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.7065 - accuracy: 0.4632 - val_loss: 0.6944 - val_accuracy: 0.5210\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6966 - accuracy: 0.5453 - val_loss: 0.6807 - val_accuracy: 0.5546\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6814 - accuracy: 0.5663 - val_loss: 0.6722 - val_accuracy: 0.5714\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.6803 - accuracy: 0.5874 - val_loss: 0.6642 - val_accuracy: 0.6387\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6722 - accuracy: 0.6063 - val_loss: 0.6563 - val_accuracy: 0.6303\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.6653 - accuracy: 0.6274 - val_loss: 0.6494 - val_accuracy: 0.6303\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.6653 - accuracy: 0.6105 - val_loss: 0.6435 - val_accuracy: 0.6303\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6602 - accuracy: 0.6105 - val_loss: 0.6373 - val_accuracy: 0.6303\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.6524 - accuracy: 0.6316 - val_loss: 0.6312 - val_accuracy: 0.6303\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.6563 - accuracy: 0.6042 - val_loss: 0.6256 - val_accuracy: 0.6303\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.6584 - accuracy: 0.6147 - val_loss: 0.6206 - val_accuracy: 0.6387\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6462 - accuracy: 0.6105 - val_loss: 0.6150 - val_accuracy: 0.6387\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6443 - accuracy: 0.6147 - val_loss: 0.6096 - val_accuracy: 0.6387\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.6454 - accuracy: 0.6063 - val_loss: 0.6051 - val_accuracy: 0.6387\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 113us/sample - loss: 0.6408 - accuracy: 0.6147 - val_loss: 0.5994 - val_accuracy: 0.6387\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6253 - accuracy: 0.6547 - val_loss: 0.5932 - val_accuracy: 0.6387\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.6296 - accuracy: 0.6337 - val_loss: 0.5874 - val_accuracy: 0.6471\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.6237 - accuracy: 0.6484 - val_loss: 0.5811 - val_accuracy: 0.6471\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6212 - accuracy: 0.6505 - val_loss: 0.5752 - val_accuracy: 0.6471\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.6176 - accuracy: 0.6358 - val_loss: 0.5698 - val_accuracy: 0.6471\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6098 - accuracy: 0.6463 - val_loss: 0.5635 - val_accuracy: 0.6471\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6044 - accuracy: 0.6674 - val_loss: 0.5572 - val_accuracy: 0.6639\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.6073 - accuracy: 0.6674 - val_loss: 0.5510 - val_accuracy: 0.6639\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6103 - accuracy: 0.6505 - val_loss: 0.5460 - val_accuracy: 0.6891\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5893 - accuracy: 0.6821 - val_loss: 0.5394 - val_accuracy: 0.7059\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.6025 - accuracy: 0.6884 - val_loss: 0.5342 - val_accuracy: 0.7563\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5944 - accuracy: 0.6779 - val_loss: 0.5287 - val_accuracy: 0.7647\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5824 - accuracy: 0.7116 - val_loss: 0.5235 - val_accuracy: 0.8067\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5907 - accuracy: 0.7305 - val_loss: 0.5176 - val_accuracy: 0.8151\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5855 - accuracy: 0.6989 - val_loss: 0.5124 - val_accuracy: 0.8067\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5600 - accuracy: 0.7284 - val_loss: 0.5048 - val_accuracy: 0.8151\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5785 - accuracy: 0.7074 - val_loss: 0.5003 - val_accuracy: 0.8151\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5621 - accuracy: 0.7305 - val_loss: 0.4942 - val_accuracy: 0.8319\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5654 - accuracy: 0.7158 - val_loss: 0.4890 - val_accuracy: 0.8235\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5612 - accuracy: 0.7516 - val_loss: 0.4840 - val_accuracy: 0.8319\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5557 - accuracy: 0.7368 - val_loss: 0.4794 - val_accuracy: 0.8235\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5584 - accuracy: 0.7474 - val_loss: 0.4753 - val_accuracy: 0.8319\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5448 - accuracy: 0.7474 - val_loss: 0.4708 - val_accuracy: 0.8487\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5642 - accuracy: 0.7516 - val_loss: 0.4681 - val_accuracy: 0.8487\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5479 - accuracy: 0.7453 - val_loss: 0.4639 - val_accuracy: 0.8487\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5550 - accuracy: 0.7326 - val_loss: 0.4606 - val_accuracy: 0.8487\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5467 - accuracy: 0.7684 - val_loss: 0.4568 - val_accuracy: 0.8487\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5465 - accuracy: 0.7453 - val_loss: 0.4525 - val_accuracy: 0.8487\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5456 - accuracy: 0.7663 - val_loss: 0.4489 - val_accuracy: 0.8487\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5209 - accuracy: 0.7663 - val_loss: 0.4443 - val_accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5438 - accuracy: 0.7474 - val_loss: 0.4416 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5291 - accuracy: 0.7663 - val_loss: 0.4375 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5402 - accuracy: 0.7642 - val_loss: 0.4345 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5071 - accuracy: 0.7811 - val_loss: 0.4300 - val_accuracy: 0.8655\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5289 - accuracy: 0.7768 - val_loss: 0.4280 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5207 - accuracy: 0.7705 - val_loss: 0.4248 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5096 - accuracy: 0.7747 - val_loss: 0.4211 - val_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5454 - accuracy: 0.7453 - val_loss: 0.4196 - val_accuracy: 0.8655\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5113 - accuracy: 0.7789 - val_loss: 0.4170 - val_accuracy: 0.8655\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5241 - accuracy: 0.7642 - val_loss: 0.4151 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5163 - accuracy: 0.7621 - val_loss: 0.4132 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5148 - accuracy: 0.7853 - val_loss: 0.4124 - val_accuracy: 0.8655\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5137 - accuracy: 0.7600 - val_loss: 0.4100 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5304 - accuracy: 0.7663 - val_loss: 0.4101 - val_accuracy: 0.8739\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5269 - accuracy: 0.7726 - val_loss: 0.4089 - val_accuracy: 0.8655\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5137 - accuracy: 0.7726 - val_loss: 0.4067 - val_accuracy: 0.8655\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4986 - accuracy: 0.7916 - val_loss: 0.4030 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4988 - accuracy: 0.7768 - val_loss: 0.4003 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4700 - accuracy: 0.8105 - val_loss: 0.3970 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4923 - accuracy: 0.7958 - val_loss: 0.3941 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5076 - accuracy: 0.7895 - val_loss: 0.3940 - val_accuracy: 0.8655\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4901 - accuracy: 0.7895 - val_loss: 0.3930 - val_accuracy: 0.8655\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5081 - accuracy: 0.7916 - val_loss: 0.3949 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5181 - accuracy: 0.7705 - val_loss: 0.3941 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4851 - accuracy: 0.7979 - val_loss: 0.3908 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5064 - accuracy: 0.7979 - val_loss: 0.3911 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 119us/sample - loss: 0.4963 - accuracy: 0.7747 - val_loss: 0.3901 - val_accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4918 - accuracy: 0.7979 - val_loss: 0.3882 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4993 - accuracy: 0.7747 - val_loss: 0.3882 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4868 - accuracy: 0.7916 - val_loss: 0.3863 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4604 - accuracy: 0.8126 - val_loss: 0.3830 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4707 - accuracy: 0.8105 - val_loss: 0.3795 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4809 - accuracy: 0.7811 - val_loss: 0.3786 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5028 - accuracy: 0.7874 - val_loss: 0.3794 - val_accuracy: 0.8655\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4695 - accuracy: 0.7874 - val_loss: 0.3780 - val_accuracy: 0.8655\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4756 - accuracy: 0.8126 - val_loss: 0.3771 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4832 - accuracy: 0.7789 - val_loss: 0.3766 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4705 - accuracy: 0.8021 - val_loss: 0.3754 - val_accuracy: 0.8655\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4576 - accuracy: 0.8211 - val_loss: 0.3745 - val_accuracy: 0.8655\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4917 - accuracy: 0.8042 - val_loss: 0.3770 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4585 - accuracy: 0.8211 - val_loss: 0.3733 - val_accuracy: 0.8655\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4775 - accuracy: 0.7937 - val_loss: 0.3712 - val_accuracy: 0.8655\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4792 - accuracy: 0.7958 - val_loss: 0.3702 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.4857 - accuracy: 0.7916 - val_loss: 0.3697 - val_accuracy: 0.8655\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4586 - accuracy: 0.8295 - val_loss: 0.3679 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 118us/sample - loss: 0.4365 - accuracy: 0.8232 - val_loss: 0.3661 - val_accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 146us/sample - loss: 0.4740 - accuracy: 0.8000 - val_loss: 0.3666 - val_accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 122us/sample - loss: 0.4651 - accuracy: 0.7895 - val_loss: 0.3664 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4680 - accuracy: 0.8084 - val_loss: 0.3661 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4584 - accuracy: 0.8168 - val_loss: 0.3642 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4541 - accuracy: 0.8021 - val_loss: 0.3632 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4687 - accuracy: 0.7979 - val_loss: 0.3633 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4487 - accuracy: 0.8063 - val_loss: 0.3621 - val_accuracy: 0.8655\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4686 - accuracy: 0.7768 - val_loss: 0.3616 - val_accuracy: 0.8655\n",
      "297/297 [==============================] - 0s 59us/sample - loss: 0.4754 - accuracy: 0.8081\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu, total=   5.9s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.6784 - accuracy: 0.6000 - val_loss: 0.6726 - val_accuracy: 0.6555\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6793 - accuracy: 0.6295 - val_loss: 0.6688 - val_accuracy: 0.6555\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6743 - accuracy: 0.6463 - val_loss: 0.6647 - val_accuracy: 0.6387\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6725 - accuracy: 0.6358 - val_loss: 0.6613 - val_accuracy: 0.6387\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.6719 - accuracy: 0.6379 - val_loss: 0.6581 - val_accuracy: 0.6387\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6643 - accuracy: 0.6442 - val_loss: 0.6548 - val_accuracy: 0.6387\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6610 - accuracy: 0.6379 - val_loss: 0.6514 - val_accuracy: 0.6387\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6635 - accuracy: 0.6379 - val_loss: 0.6489 - val_accuracy: 0.6387\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6551 - accuracy: 0.6400 - val_loss: 0.6462 - val_accuracy: 0.6387\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6435 - val_accuracy: 0.6387\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6530 - accuracy: 0.6358 - val_loss: 0.6408 - val_accuracy: 0.6387\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.6549 - accuracy: 0.6358 - val_loss: 0.6387 - val_accuracy: 0.6387\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6537 - accuracy: 0.6400 - val_loss: 0.6364 - val_accuracy: 0.6387\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.6476 - accuracy: 0.6400 - val_loss: 0.6341 - val_accuracy: 0.6387\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6462 - accuracy: 0.6400 - val_loss: 0.6314 - val_accuracy: 0.6387\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6449 - accuracy: 0.6400 - val_loss: 0.6285 - val_accuracy: 0.6387\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6437 - accuracy: 0.6379 - val_loss: 0.6260 - val_accuracy: 0.6387\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6413 - accuracy: 0.6379 - val_loss: 0.6239 - val_accuracy: 0.6387\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6355 - accuracy: 0.6400 - val_loss: 0.6213 - val_accuracy: 0.6387\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6395 - accuracy: 0.6421 - val_loss: 0.6190 - val_accuracy: 0.6387\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6365 - accuracy: 0.6400 - val_loss: 0.6167 - val_accuracy: 0.6387\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6297 - accuracy: 0.6400 - val_loss: 0.6134 - val_accuracy: 0.6387\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6395 - accuracy: 0.6337 - val_loss: 0.6114 - val_accuracy: 0.6387\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6256 - accuracy: 0.6400 - val_loss: 0.6082 - val_accuracy: 0.6387\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.6240 - accuracy: 0.6400 - val_loss: 0.6050 - val_accuracy: 0.6387\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6196 - accuracy: 0.6358 - val_loss: 0.6014 - val_accuracy: 0.6387\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6194 - accuracy: 0.6442 - val_loss: 0.5985 - val_accuracy: 0.6387\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6381 - accuracy: 0.6379 - val_loss: 0.5965 - val_accuracy: 0.6387\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6149 - accuracy: 0.6421 - val_loss: 0.5925 - val_accuracy: 0.6387\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6190 - accuracy: 0.6400 - val_loss: 0.5893 - val_accuracy: 0.6387\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6130 - accuracy: 0.6442 - val_loss: 0.5857 - val_accuracy: 0.6387\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6082 - accuracy: 0.6421 - val_loss: 0.5818 - val_accuracy: 0.6387\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6180 - accuracy: 0.6442 - val_loss: 0.5792 - val_accuracy: 0.6387\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6162 - accuracy: 0.6442 - val_loss: 0.5762 - val_accuracy: 0.6387\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6148 - accuracy: 0.6400 - val_loss: 0.5730 - val_accuracy: 0.6387\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6028 - accuracy: 0.6400 - val_loss: 0.5686 - val_accuracy: 0.6387\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5996 - accuracy: 0.6442 - val_loss: 0.5646 - val_accuracy: 0.6387\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6043 - accuracy: 0.6463 - val_loss: 0.5609 - val_accuracy: 0.6387\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5984 - accuracy: 0.6442 - val_loss: 0.5568 - val_accuracy: 0.6387\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6146 - accuracy: 0.6337 - val_loss: 0.5539 - val_accuracy: 0.6387\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5897 - accuracy: 0.6358 - val_loss: 0.5489 - val_accuracy: 0.6387\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.5998 - accuracy: 0.6316 - val_loss: 0.5450 - val_accuracy: 0.6387\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5882 - accuracy: 0.6421 - val_loss: 0.5403 - val_accuracy: 0.6555\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 113us/sample - loss: 0.6052 - accuracy: 0.6484 - val_loss: 0.5377 - val_accuracy: 0.6555\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5852 - accuracy: 0.6568 - val_loss: 0.5337 - val_accuracy: 0.6639\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5923 - accuracy: 0.6526 - val_loss: 0.5306 - val_accuracy: 0.6723\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5741 - accuracy: 0.6674 - val_loss: 0.5258 - val_accuracy: 0.6975\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5887 - accuracy: 0.6842 - val_loss: 0.5226 - val_accuracy: 0.6975\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5768 - accuracy: 0.6632 - val_loss: 0.5185 - val_accuracy: 0.7059\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.5763 - accuracy: 0.6863 - val_loss: 0.5145 - val_accuracy: 0.7143\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.5638 - accuracy: 0.7053 - val_loss: 0.5098 - val_accuracy: 0.7311\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5697 - accuracy: 0.6758 - val_loss: 0.5067 - val_accuracy: 0.7647\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5692 - accuracy: 0.6737 - val_loss: 0.5046 - val_accuracy: 0.7647\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5656 - accuracy: 0.6905 - val_loss: 0.5010 - val_accuracy: 0.7563\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5627 - accuracy: 0.6947 - val_loss: 0.4974 - val_accuracy: 0.7899\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5494 - accuracy: 0.7284 - val_loss: 0.4926 - val_accuracy: 0.8151\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5650 - accuracy: 0.6968 - val_loss: 0.4904 - val_accuracy: 0.8235\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5619 - accuracy: 0.7011 - val_loss: 0.4882 - val_accuracy: 0.8319\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5555 - accuracy: 0.7095 - val_loss: 0.4853 - val_accuracy: 0.8403\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5633 - accuracy: 0.7158 - val_loss: 0.4839 - val_accuracy: 0.8487\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5600 - accuracy: 0.7284 - val_loss: 0.4828 - val_accuracy: 0.8487\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5623 - accuracy: 0.7179 - val_loss: 0.4820 - val_accuracy: 0.8487\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.5478 - accuracy: 0.7368 - val_loss: 0.4782 - val_accuracy: 0.8487\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5496 - accuracy: 0.7474 - val_loss: 0.4751 - val_accuracy: 0.8487\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.5501 - accuracy: 0.7495 - val_loss: 0.4714 - val_accuracy: 0.8487\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5581 - accuracy: 0.7474 - val_loss: 0.4692 - val_accuracy: 0.8487\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5422 - accuracy: 0.7684 - val_loss: 0.4648 - val_accuracy: 0.8487\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5130 - accuracy: 0.7663 - val_loss: 0.4607 - val_accuracy: 0.8487\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5393 - accuracy: 0.7579 - val_loss: 0.4580 - val_accuracy: 0.8487\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5303 - accuracy: 0.7453 - val_loss: 0.4576 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5284 - accuracy: 0.7516 - val_loss: 0.4545 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5299 - accuracy: 0.7558 - val_loss: 0.4511 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5383 - accuracy: 0.7621 - val_loss: 0.4513 - val_accuracy: 0.8655\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5153 - accuracy: 0.7621 - val_loss: 0.4482 - val_accuracy: 0.8655\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5200 - accuracy: 0.7600 - val_loss: 0.4465 - val_accuracy: 0.8655\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5423 - accuracy: 0.7621 - val_loss: 0.4441 - val_accuracy: 0.8655\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5297 - accuracy: 0.7600 - val_loss: 0.4427 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5280 - accuracy: 0.7789 - val_loss: 0.4413 - val_accuracy: 0.8739\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5406 - accuracy: 0.7495 - val_loss: 0.4431 - val_accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5292 - accuracy: 0.7600 - val_loss: 0.4404 - val_accuracy: 0.8739\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4915 - accuracy: 0.7811 - val_loss: 0.4355 - val_accuracy: 0.8739\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5096 - accuracy: 0.7811 - val_loss: 0.4334 - val_accuracy: 0.8739\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5357 - accuracy: 0.7368 - val_loss: 0.4320 - val_accuracy: 0.8739\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5162 - accuracy: 0.7937 - val_loss: 0.4303 - val_accuracy: 0.8739\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5253 - accuracy: 0.7642 - val_loss: 0.4301 - val_accuracy: 0.8655\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5251 - accuracy: 0.7516 - val_loss: 0.4291 - val_accuracy: 0.8655\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5252 - accuracy: 0.7663 - val_loss: 0.4290 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5317 - accuracy: 0.7789 - val_loss: 0.4269 - val_accuracy: 0.8655\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4940 - accuracy: 0.7811 - val_loss: 0.4241 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5193 - accuracy: 0.7811 - val_loss: 0.4247 - val_accuracy: 0.8571\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5245 - accuracy: 0.7747 - val_loss: 0.4255 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5366 - accuracy: 0.7726 - val_loss: 0.4264 - val_accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5081 - accuracy: 0.7642 - val_loss: 0.4242 - val_accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5238 - accuracy: 0.7811 - val_loss: 0.4228 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5233 - accuracy: 0.7705 - val_loss: 0.4217 - val_accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5374 - accuracy: 0.7642 - val_loss: 0.4237 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5054 - accuracy: 0.7684 - val_loss: 0.4221 - val_accuracy: 0.8739\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5184 - accuracy: 0.7684 - val_loss: 0.4204 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5060 - accuracy: 0.8084 - val_loss: 0.4172 - val_accuracy: 0.8655\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5228 - accuracy: 0.7747 - val_loss: 0.4171 - val_accuracy: 0.8655\n",
      "297/297 [==============================] - 0s 64us/sample - loss: 0.4896 - accuracy: 0.8148\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu, total=   6.3s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu ........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.6529 - accuracy: 0.6147 - val_loss: 0.6558 - val_accuracy: 0.6050\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6463 - accuracy: 0.6063 - val_loss: 0.6504 - val_accuracy: 0.6050\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6518 - accuracy: 0.6063 - val_loss: 0.6459 - val_accuracy: 0.6050\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.6547 - accuracy: 0.6000 - val_loss: 0.6416 - val_accuracy: 0.6050\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6351 - accuracy: 0.6126 - val_loss: 0.6357 - val_accuracy: 0.6050\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6377 - accuracy: 0.6021 - val_loss: 0.6307 - val_accuracy: 0.6050\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6312 - accuracy: 0.6211 - val_loss: 0.6249 - val_accuracy: 0.6050\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6311 - accuracy: 0.6105 - val_loss: 0.6208 - val_accuracy: 0.6050\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6273 - accuracy: 0.6168 - val_loss: 0.6159 - val_accuracy: 0.6050\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6344 - accuracy: 0.6189 - val_loss: 0.6126 - val_accuracy: 0.6050\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6178 - accuracy: 0.6168 - val_loss: 0.6075 - val_accuracy: 0.6050\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6255 - accuracy: 0.6211 - val_loss: 0.6034 - val_accuracy: 0.6050\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6173 - accuracy: 0.6232 - val_loss: 0.5996 - val_accuracy: 0.6134\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6089 - accuracy: 0.6295 - val_loss: 0.5946 - val_accuracy: 0.6134\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6144 - accuracy: 0.6232 - val_loss: 0.5915 - val_accuracy: 0.6134\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6022 - accuracy: 0.6400 - val_loss: 0.5871 - val_accuracy: 0.6218\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.6071 - accuracy: 0.6400 - val_loss: 0.5840 - val_accuracy: 0.6303\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6063 - accuracy: 0.6379 - val_loss: 0.5811 - val_accuracy: 0.6303\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6125 - accuracy: 0.6400 - val_loss: 0.5777 - val_accuracy: 0.6723\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6005 - accuracy: 0.6421 - val_loss: 0.5748 - val_accuracy: 0.6975\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.5925 - accuracy: 0.6695 - val_loss: 0.5715 - val_accuracy: 0.7143\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5875 - accuracy: 0.6484 - val_loss: 0.5691 - val_accuracy: 0.7143\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6099 - accuracy: 0.6526 - val_loss: 0.5673 - val_accuracy: 0.7563\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5824 - accuracy: 0.6947 - val_loss: 0.5641 - val_accuracy: 0.7647\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5837 - accuracy: 0.6926 - val_loss: 0.5625 - val_accuracy: 0.7647\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5686 - accuracy: 0.7326 - val_loss: 0.5593 - val_accuracy: 0.7647\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5759 - accuracy: 0.7284 - val_loss: 0.5568 - val_accuracy: 0.7647\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.5650 - accuracy: 0.7158 - val_loss: 0.5543 - val_accuracy: 0.7647\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5894 - accuracy: 0.7032 - val_loss: 0.5528 - val_accuracy: 0.7647\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5780 - accuracy: 0.7305 - val_loss: 0.5509 - val_accuracy: 0.7647\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5874 - accuracy: 0.7305 - val_loss: 0.5505 - val_accuracy: 0.7731\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.5760 - accuracy: 0.7516 - val_loss: 0.5488 - val_accuracy: 0.7647\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5593 - accuracy: 0.7537 - val_loss: 0.5466 - val_accuracy: 0.7647\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5654 - accuracy: 0.7600 - val_loss: 0.5451 - val_accuracy: 0.7647\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5862 - accuracy: 0.7432 - val_loss: 0.5447 - val_accuracy: 0.7815\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5426 - accuracy: 0.7832 - val_loss: 0.5422 - val_accuracy: 0.7731\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5725 - accuracy: 0.7368 - val_loss: 0.5411 - val_accuracy: 0.7815\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5472 - accuracy: 0.7579 - val_loss: 0.5397 - val_accuracy: 0.7815\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5839 - accuracy: 0.7474 - val_loss: 0.5403 - val_accuracy: 0.7899\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5599 - accuracy: 0.7621 - val_loss: 0.5387 - val_accuracy: 0.7899\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5349 - accuracy: 0.7853 - val_loss: 0.5369 - val_accuracy: 0.7899\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5511 - accuracy: 0.7600 - val_loss: 0.5357 - val_accuracy: 0.7899\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5540 - accuracy: 0.7895 - val_loss: 0.5349 - val_accuracy: 0.7899\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5559 - accuracy: 0.7684 - val_loss: 0.5339 - val_accuracy: 0.7899\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5661 - accuracy: 0.7621 - val_loss: 0.5325 - val_accuracy: 0.7899\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5472 - accuracy: 0.7789 - val_loss: 0.5321 - val_accuracy: 0.7899\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5609 - accuracy: 0.7579 - val_loss: 0.5316 - val_accuracy: 0.7899\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5389 - accuracy: 0.7811 - val_loss: 0.5307 - val_accuracy: 0.7899\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5496 - accuracy: 0.7853 - val_loss: 0.5292 - val_accuracy: 0.7899\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5598 - accuracy: 0.7537 - val_loss: 0.5284 - val_accuracy: 0.7899\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5317 - accuracy: 0.7853 - val_loss: 0.5274 - val_accuracy: 0.7899\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5706 - accuracy: 0.7832 - val_loss: 0.5264 - val_accuracy: 0.7899\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5341 - accuracy: 0.7705 - val_loss: 0.5257 - val_accuracy: 0.7899\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5589 - accuracy: 0.7832 - val_loss: 0.5255 - val_accuracy: 0.7899\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5393 - accuracy: 0.7874 - val_loss: 0.5247 - val_accuracy: 0.7983\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5362 - accuracy: 0.7958 - val_loss: 0.5240 - val_accuracy: 0.7899\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5376 - accuracy: 0.7684 - val_loss: 0.5225 - val_accuracy: 0.7899\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5423 - accuracy: 0.7789 - val_loss: 0.5217 - val_accuracy: 0.7899\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5314 - accuracy: 0.7874 - val_loss: 0.5211 - val_accuracy: 0.7899\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5291 - accuracy: 0.7874 - val_loss: 0.5206 - val_accuracy: 0.7983\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5409 - accuracy: 0.7979 - val_loss: 0.5200 - val_accuracy: 0.7899\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5667 - accuracy: 0.7768 - val_loss: 0.5194 - val_accuracy: 0.7899\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5445 - accuracy: 0.8000 - val_loss: 0.5197 - val_accuracy: 0.7899\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5457 - accuracy: 0.7811 - val_loss: 0.5187 - val_accuracy: 0.7899\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5413 - accuracy: 0.7895 - val_loss: 0.5184 - val_accuracy: 0.7899\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5283 - accuracy: 0.7789 - val_loss: 0.5179 - val_accuracy: 0.7899\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5530 - accuracy: 0.7789 - val_loss: 0.5175 - val_accuracy: 0.7899\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 123us/sample - loss: 0.5056 - accuracy: 0.7979 - val_loss: 0.5163 - val_accuracy: 0.7899\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 113us/sample - loss: 0.5400 - accuracy: 0.7642 - val_loss: 0.5160 - val_accuracy: 0.7899\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5347 - accuracy: 0.7726 - val_loss: 0.5154 - val_accuracy: 0.7899\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5367 - accuracy: 0.7789 - val_loss: 0.5151 - val_accuracy: 0.7899\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5416 - accuracy: 0.7853 - val_loss: 0.5149 - val_accuracy: 0.7899\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5303 - accuracy: 0.7747 - val_loss: 0.5144 - val_accuracy: 0.7899\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5246 - accuracy: 0.7874 - val_loss: 0.5136 - val_accuracy: 0.7899\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5147 - accuracy: 0.7789 - val_loss: 0.5130 - val_accuracy: 0.7899\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5339 - accuracy: 0.8021 - val_loss: 0.5126 - val_accuracy: 0.7899\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5357 - accuracy: 0.7705 - val_loss: 0.5124 - val_accuracy: 0.7899\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5424 - accuracy: 0.8000 - val_loss: 0.5121 - val_accuracy: 0.7899\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5320 - accuracy: 0.7832 - val_loss: 0.5116 - val_accuracy: 0.7899\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5331 - accuracy: 0.7832 - val_loss: 0.5112 - val_accuracy: 0.7899\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5118 - accuracy: 0.7874 - val_loss: 0.5106 - val_accuracy: 0.7899\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5081 - accuracy: 0.8063 - val_loss: 0.5103 - val_accuracy: 0.7899\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5128 - accuracy: 0.7979 - val_loss: 0.5096 - val_accuracy: 0.7899\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5106 - accuracy: 0.7958 - val_loss: 0.5096 - val_accuracy: 0.7899\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5184 - accuracy: 0.7789 - val_loss: 0.5103 - val_accuracy: 0.7899\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5152 - accuracy: 0.7958 - val_loss: 0.5095 - val_accuracy: 0.7899\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5416 - accuracy: 0.7853 - val_loss: 0.5095 - val_accuracy: 0.7899\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5162 - accuracy: 0.8042 - val_loss: 0.5087 - val_accuracy: 0.7899\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4980 - accuracy: 0.7958 - val_loss: 0.5083 - val_accuracy: 0.7899\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5020 - accuracy: 0.7937 - val_loss: 0.5081 - val_accuracy: 0.7899\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5228 - accuracy: 0.7747 - val_loss: 0.5082 - val_accuracy: 0.7899\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5412 - accuracy: 0.7663 - val_loss: 0.5083 - val_accuracy: 0.7899\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5044 - accuracy: 0.8084 - val_loss: 0.5077 - val_accuracy: 0.7899\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5258 - accuracy: 0.8063 - val_loss: 0.5071 - val_accuracy: 0.7899\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5195 - accuracy: 0.7811 - val_loss: 0.5063 - val_accuracy: 0.7899\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5078 - accuracy: 0.7895 - val_loss: 0.5069 - val_accuracy: 0.7899\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5148 - accuracy: 0.7832 - val_loss: 0.5066 - val_accuracy: 0.7899\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5279 - accuracy: 0.8021 - val_loss: 0.5061 - val_accuracy: 0.7899\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4988 - accuracy: 0.7874 - val_loss: 0.5056 - val_accuracy: 0.7899\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5181 - accuracy: 0.7937 - val_loss: 0.5051 - val_accuracy: 0.7899\n",
      "297/297 [==============================] - 0s 54us/sample - loss: 0.4326 - accuracy: 0.8350\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=4, activation=relu, total=   5.8s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7955 - accuracy: 0.6232 - val_loss: 0.5901 - val_accuracy: 0.6891\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.6876 - accuracy: 0.6611 - val_loss: 0.4875 - val_accuracy: 0.7479\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.6170 - accuracy: 0.7179 - val_loss: 0.4614 - val_accuracy: 0.7899\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5531 - accuracy: 0.7368 - val_loss: 0.4396 - val_accuracy: 0.7983\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5533 - accuracy: 0.7537 - val_loss: 0.4213 - val_accuracy: 0.7899\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5347 - accuracy: 0.7726 - val_loss: 0.4107 - val_accuracy: 0.8067\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.5058 - accuracy: 0.7663 - val_loss: 0.3997 - val_accuracy: 0.8151\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5126 - accuracy: 0.7937 - val_loss: 0.3937 - val_accuracy: 0.8403\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5173 - accuracy: 0.7684 - val_loss: 0.3875 - val_accuracy: 0.8151\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5168 - accuracy: 0.7705 - val_loss: 0.3796 - val_accuracy: 0.8403\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4851 - accuracy: 0.7768 - val_loss: 0.3838 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5029 - accuracy: 0.7895 - val_loss: 0.3784 - val_accuracy: 0.8571\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4721 - accuracy: 0.8000 - val_loss: 0.3692 - val_accuracy: 0.8487\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4529 - accuracy: 0.8000 - val_loss: 0.3734 - val_accuracy: 0.8487\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4707 - accuracy: 0.7874 - val_loss: 0.3613 - val_accuracy: 0.8571\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4736 - accuracy: 0.8084 - val_loss: 0.3531 - val_accuracy: 0.8655\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4895 - accuracy: 0.8105 - val_loss: 0.3515 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4663 - accuracy: 0.7895 - val_loss: 0.3512 - val_accuracy: 0.8571\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4465 - accuracy: 0.7705 - val_loss: 0.3530 - val_accuracy: 0.8739\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4357 - accuracy: 0.8147 - val_loss: 0.3510 - val_accuracy: 0.8739\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4654 - accuracy: 0.8021 - val_loss: 0.3510 - val_accuracy: 0.8739\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4697 - accuracy: 0.8042 - val_loss: 0.3543 - val_accuracy: 0.8487\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4121 - accuracy: 0.8189 - val_loss: 0.3538 - val_accuracy: 0.8655\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4812 - accuracy: 0.8000 - val_loss: 0.3423 - val_accuracy: 0.8739\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4213 - accuracy: 0.8126 - val_loss: 0.3479 - val_accuracy: 0.8655\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4433 - accuracy: 0.8000 - val_loss: 0.3451 - val_accuracy: 0.8739\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4175 - accuracy: 0.8105 - val_loss: 0.3429 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4456 - accuracy: 0.8063 - val_loss: 0.3432 - val_accuracy: 0.8739\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4614 - accuracy: 0.8189 - val_loss: 0.3425 - val_accuracy: 0.8739\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4668 - accuracy: 0.8021 - val_loss: 0.3430 - val_accuracy: 0.8824\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4557 - accuracy: 0.8000 - val_loss: 0.3416 - val_accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4567 - accuracy: 0.8042 - val_loss: 0.3425 - val_accuracy: 0.8739\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4374 - accuracy: 0.8232 - val_loss: 0.3423 - val_accuracy: 0.8824\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4261 - accuracy: 0.8232 - val_loss: 0.3394 - val_accuracy: 0.8824\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4161 - accuracy: 0.8232 - val_loss: 0.3374 - val_accuracy: 0.8824\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4293 - accuracy: 0.8211 - val_loss: 0.3402 - val_accuracy: 0.8824\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4396 - accuracy: 0.8189 - val_loss: 0.3412 - val_accuracy: 0.8824\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4363 - accuracy: 0.8084 - val_loss: 0.3376 - val_accuracy: 0.8824\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4284 - accuracy: 0.8211 - val_loss: 0.3374 - val_accuracy: 0.8824\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4263 - accuracy: 0.8168 - val_loss: 0.3355 - val_accuracy: 0.8908\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4195 - accuracy: 0.8253 - val_loss: 0.3343 - val_accuracy: 0.9076\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4183 - accuracy: 0.8253 - val_loss: 0.3360 - val_accuracy: 0.8992\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4411 - accuracy: 0.8084 - val_loss: 0.3401 - val_accuracy: 0.8992\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4257 - accuracy: 0.8211 - val_loss: 0.3427 - val_accuracy: 0.8908\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4151 - accuracy: 0.8400 - val_loss: 0.3427 - val_accuracy: 0.8908\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4386 - accuracy: 0.7979 - val_loss: 0.3394 - val_accuracy: 0.9076\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4485 - accuracy: 0.8126 - val_loss: 0.3412 - val_accuracy: 0.9076\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4140 - accuracy: 0.8147 - val_loss: 0.3436 - val_accuracy: 0.8992\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4523 - accuracy: 0.8000 - val_loss: 0.3449 - val_accuracy: 0.8824\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4016 - accuracy: 0.8274 - val_loss: 0.3432 - val_accuracy: 0.8908\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4071 - accuracy: 0.8232 - val_loss: 0.3444 - val_accuracy: 0.8824\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4086 - accuracy: 0.8274 - val_loss: 0.3437 - val_accuracy: 0.8908\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4132 - accuracy: 0.8295 - val_loss: 0.3462 - val_accuracy: 0.8824\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4008 - accuracy: 0.8442 - val_loss: 0.3442 - val_accuracy: 0.8992\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4192 - accuracy: 0.8211 - val_loss: 0.3442 - val_accuracy: 0.8908\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3903 - accuracy: 0.8505 - val_loss: 0.3459 - val_accuracy: 0.8908\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4347 - accuracy: 0.8105 - val_loss: 0.3455 - val_accuracy: 0.8908\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4132 - accuracy: 0.8316 - val_loss: 0.3451 - val_accuracy: 0.8908\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4093 - accuracy: 0.8253 - val_loss: 0.3531 - val_accuracy: 0.8824\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4135 - accuracy: 0.8211 - val_loss: 0.3569 - val_accuracy: 0.8824\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.3949 - accuracy: 0.8295 - val_loss: 0.3519 - val_accuracy: 0.8908\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4019 - accuracy: 0.8253 - val_loss: 0.3441 - val_accuracy: 0.8908\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4118 - accuracy: 0.8358 - val_loss: 0.3376 - val_accuracy: 0.8992\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4149 - accuracy: 0.8274 - val_loss: 0.3393 - val_accuracy: 0.9076\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 233us/sample - loss: 0.4186 - accuracy: 0.8126 - val_loss: 0.3434 - val_accuracy: 0.8908\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 117us/sample - loss: 0.4116 - accuracy: 0.8358 - val_loss: 0.3437 - val_accuracy: 0.8908\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 112us/sample - loss: 0.3955 - accuracy: 0.8295 - val_loss: 0.3501 - val_accuracy: 0.8655\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 119us/sample - loss: 0.4067 - accuracy: 0.8274 - val_loss: 0.3473 - val_accuracy: 0.8655\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4043 - accuracy: 0.8400 - val_loss: 0.3518 - val_accuracy: 0.8739\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4070 - accuracy: 0.8211 - val_loss: 0.3531 - val_accuracy: 0.8824\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4140 - accuracy: 0.8316 - val_loss: 0.3497 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4146 - accuracy: 0.8295 - val_loss: 0.3444 - val_accuracy: 0.8824\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4086 - accuracy: 0.8274 - val_loss: 0.3503 - val_accuracy: 0.8655\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3923 - accuracy: 0.8421 - val_loss: 0.3532 - val_accuracy: 0.8655\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4043 - accuracy: 0.8295 - val_loss: 0.3545 - val_accuracy: 0.8824\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3809 - accuracy: 0.8358 - val_loss: 0.3600 - val_accuracy: 0.8655\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4156 - accuracy: 0.8295 - val_loss: 0.3545 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3931 - accuracy: 0.8379 - val_loss: 0.3531 - val_accuracy: 0.8655\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.3958 - accuracy: 0.8337 - val_loss: 0.3540 - val_accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4136 - accuracy: 0.8295 - val_loss: 0.3591 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 115us/sample - loss: 0.4115 - accuracy: 0.8337 - val_loss: 0.3579 - val_accuracy: 0.8655\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4087 - accuracy: 0.8316 - val_loss: 0.3579 - val_accuracy: 0.8739\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4054 - accuracy: 0.8274 - val_loss: 0.3590 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.4107 - accuracy: 0.8337 - val_loss: 0.3592 - val_accuracy: 0.8739\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 124us/sample - loss: 0.3936 - accuracy: 0.8211 - val_loss: 0.3636 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4193 - accuracy: 0.8526 - val_loss: 0.3568 - val_accuracy: 0.8824\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 124us/sample - loss: 0.4117 - accuracy: 0.8274 - val_loss: 0.3549 - val_accuracy: 0.8739\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4129 - accuracy: 0.8253 - val_loss: 0.3559 - val_accuracy: 0.8824\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4079 - accuracy: 0.8379 - val_loss: 0.3567 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 120us/sample - loss: 0.3873 - accuracy: 0.8379 - val_loss: 0.3623 - val_accuracy: 0.8655\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 115us/sample - loss: 0.3999 - accuracy: 0.8505 - val_loss: 0.3598 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.3752 - accuracy: 0.8358 - val_loss: 0.3590 - val_accuracy: 0.8824\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.4077 - accuracy: 0.8316 - val_loss: 0.3533 - val_accuracy: 0.8824\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4025 - accuracy: 0.8189 - val_loss: 0.3576 - val_accuracy: 0.8824\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4144 - accuracy: 0.8084 - val_loss: 0.3634 - val_accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.3962 - accuracy: 0.8358 - val_loss: 0.3608 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4048 - accuracy: 0.8337 - val_loss: 0.3568 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3874 - accuracy: 0.8421 - val_loss: 0.3528 - val_accuracy: 0.8824\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4114 - accuracy: 0.8253 - val_loss: 0.3581 - val_accuracy: 0.8739\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4188 - accuracy: 0.8211 - val_loss: 0.3671 - val_accuracy: 0.8739\n",
      "297/297 [==============================] - 0s 57us/sample - loss: 0.4751 - accuracy: 0.8081\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu, total=   6.1s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7521 - accuracy: 0.6126 - val_loss: 0.4948 - val_accuracy: 0.7479\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5934 - accuracy: 0.7179 - val_loss: 0.4642 - val_accuracy: 0.7647\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5841 - accuracy: 0.7347 - val_loss: 0.4417 - val_accuracy: 0.7899\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5695 - accuracy: 0.7263 - val_loss: 0.4309 - val_accuracy: 0.7983\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 138us/sample - loss: 0.5366 - accuracy: 0.7495 - val_loss: 0.4179 - val_accuracy: 0.8067\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5581 - accuracy: 0.7495 - val_loss: 0.4071 - val_accuracy: 0.8235\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5247 - accuracy: 0.7663 - val_loss: 0.4032 - val_accuracy: 0.8319\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5194 - accuracy: 0.7832 - val_loss: 0.3973 - val_accuracy: 0.8319\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5343 - accuracy: 0.7537 - val_loss: 0.3929 - val_accuracy: 0.8319\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5326 - accuracy: 0.7474 - val_loss: 0.3919 - val_accuracy: 0.8235\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5332 - accuracy: 0.7705 - val_loss: 0.3892 - val_accuracy: 0.8151\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5331 - accuracy: 0.7747 - val_loss: 0.3883 - val_accuracy: 0.8487\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5139 - accuracy: 0.7789 - val_loss: 0.3885 - val_accuracy: 0.8487\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4948 - accuracy: 0.7832 - val_loss: 0.3917 - val_accuracy: 0.8403\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5183 - accuracy: 0.7705 - val_loss: 0.3853 - val_accuracy: 0.8487\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4948 - accuracy: 0.7726 - val_loss: 0.3781 - val_accuracy: 0.8487\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5002 - accuracy: 0.7916 - val_loss: 0.3779 - val_accuracy: 0.8487\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5204 - accuracy: 0.7832 - val_loss: 0.3793 - val_accuracy: 0.8487\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5157 - accuracy: 0.7853 - val_loss: 0.3810 - val_accuracy: 0.8487\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4776 - accuracy: 0.7895 - val_loss: 0.3785 - val_accuracy: 0.8571\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4671 - accuracy: 0.8084 - val_loss: 0.3727 - val_accuracy: 0.8655\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4759 - accuracy: 0.7726 - val_loss: 0.3683 - val_accuracy: 0.8571\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4535 - accuracy: 0.8063 - val_loss: 0.3649 - val_accuracy: 0.8571\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5003 - accuracy: 0.7958 - val_loss: 0.3676 - val_accuracy: 0.8655\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.4646 - accuracy: 0.8021 - val_loss: 0.3735 - val_accuracy: 0.8571\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4887 - accuracy: 0.8063 - val_loss: 0.3647 - val_accuracy: 0.8571\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4799 - accuracy: 0.7874 - val_loss: 0.3624 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4636 - accuracy: 0.7979 - val_loss: 0.3613 - val_accuracy: 0.8487\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4568 - accuracy: 0.7958 - val_loss: 0.3627 - val_accuracy: 0.8571\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4557 - accuracy: 0.8105 - val_loss: 0.3653 - val_accuracy: 0.8571\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4757 - accuracy: 0.8063 - val_loss: 0.3666 - val_accuracy: 0.8571\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4579 - accuracy: 0.7916 - val_loss: 0.3636 - val_accuracy: 0.8571\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4321 - accuracy: 0.8042 - val_loss: 0.3605 - val_accuracy: 0.8655\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4557 - accuracy: 0.8021 - val_loss: 0.3579 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4715 - accuracy: 0.7958 - val_loss: 0.3585 - val_accuracy: 0.8571\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4786 - accuracy: 0.7958 - val_loss: 0.3630 - val_accuracy: 0.8739\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4550 - accuracy: 0.8189 - val_loss: 0.3616 - val_accuracy: 0.8739\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4686 - accuracy: 0.7747 - val_loss: 0.3629 - val_accuracy: 0.8739\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4703 - accuracy: 0.8084 - val_loss: 0.3544 - val_accuracy: 0.8739\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4845 - accuracy: 0.7895 - val_loss: 0.3576 - val_accuracy: 0.8655\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4573 - accuracy: 0.7958 - val_loss: 0.3545 - val_accuracy: 0.8655\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4826 - accuracy: 0.7937 - val_loss: 0.3579 - val_accuracy: 0.8655\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4588 - accuracy: 0.7874 - val_loss: 0.3578 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4720 - accuracy: 0.7832 - val_loss: 0.3554 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4487 - accuracy: 0.8105 - val_loss: 0.3549 - val_accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 198us/sample - loss: 0.4499 - accuracy: 0.8189 - val_loss: 0.3558 - val_accuracy: 0.8655\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 116us/sample - loss: 0.4608 - accuracy: 0.8147 - val_loss: 0.3547 - val_accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4515 - accuracy: 0.8000 - val_loss: 0.3553 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4635 - accuracy: 0.8126 - val_loss: 0.3553 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4759 - accuracy: 0.8021 - val_loss: 0.3574 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4541 - accuracy: 0.7958 - val_loss: 0.3560 - val_accuracy: 0.8487\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4599 - accuracy: 0.8000 - val_loss: 0.3552 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4508 - accuracy: 0.8147 - val_loss: 0.3530 - val_accuracy: 0.8487\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4491 - accuracy: 0.8021 - val_loss: 0.3512 - val_accuracy: 0.8487\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4496 - accuracy: 0.8126 - val_loss: 0.3517 - val_accuracy: 0.8487\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4478 - accuracy: 0.8063 - val_loss: 0.3505 - val_accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4453 - accuracy: 0.8253 - val_loss: 0.3487 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4357 - accuracy: 0.8147 - val_loss: 0.3474 - val_accuracy: 0.8403\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 113us/sample - loss: 0.4491 - accuracy: 0.8189 - val_loss: 0.3475 - val_accuracy: 0.8487\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 115us/sample - loss: 0.4535 - accuracy: 0.8147 - val_loss: 0.3481 - val_accuracy: 0.8487\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4248 - accuracy: 0.8253 - val_loss: 0.3465 - val_accuracy: 0.8487\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4246 - accuracy: 0.8232 - val_loss: 0.3468 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4288 - accuracy: 0.8274 - val_loss: 0.3470 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.4346 - accuracy: 0.8232 - val_loss: 0.3490 - val_accuracy: 0.8655\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.4579 - accuracy: 0.8063 - val_loss: 0.3508 - val_accuracy: 0.8655\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4381 - accuracy: 0.8189 - val_loss: 0.3478 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4383 - accuracy: 0.8021 - val_loss: 0.3478 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4309 - accuracy: 0.8147 - val_loss: 0.3479 - val_accuracy: 0.8739\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 112us/sample - loss: 0.4419 - accuracy: 0.8147 - val_loss: 0.3519 - val_accuracy: 0.8739\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4503 - accuracy: 0.8126 - val_loss: 0.3501 - val_accuracy: 0.8655\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4326 - accuracy: 0.8147 - val_loss: 0.3481 - val_accuracy: 0.8739\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 119us/sample - loss: 0.4230 - accuracy: 0.8253 - val_loss: 0.3485 - val_accuracy: 0.8739\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4535 - accuracy: 0.8274 - val_loss: 0.3477 - val_accuracy: 0.8739\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4316 - accuracy: 0.8189 - val_loss: 0.3444 - val_accuracy: 0.8655\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 132us/sample - loss: 0.4427 - accuracy: 0.7874 - val_loss: 0.3430 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 138us/sample - loss: 0.4232 - accuracy: 0.8295 - val_loss: 0.3457 - val_accuracy: 0.8655\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4305 - accuracy: 0.8274 - val_loss: 0.3493 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 133us/sample - loss: 0.4214 - accuracy: 0.8232 - val_loss: 0.3550 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4292 - accuracy: 0.8063 - val_loss: 0.3502 - val_accuracy: 0.8739\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4417 - accuracy: 0.8126 - val_loss: 0.3488 - val_accuracy: 0.8739\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4245 - accuracy: 0.8316 - val_loss: 0.3492 - val_accuracy: 0.8739\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4136 - accuracy: 0.8316 - val_loss: 0.3518 - val_accuracy: 0.8739\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4313 - accuracy: 0.8253 - val_loss: 0.3488 - val_accuracy: 0.8739\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 121us/sample - loss: 0.4349 - accuracy: 0.8147 - val_loss: 0.3481 - val_accuracy: 0.8739\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4323 - accuracy: 0.8189 - val_loss: 0.3437 - val_accuracy: 0.8739\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3954 - accuracy: 0.8421 - val_loss: 0.3410 - val_accuracy: 0.8739\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4272 - accuracy: 0.8232 - val_loss: 0.3416 - val_accuracy: 0.8739\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4091 - accuracy: 0.8274 - val_loss: 0.3433 - val_accuracy: 0.8655\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4197 - accuracy: 0.8316 - val_loss: 0.3479 - val_accuracy: 0.8739\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4311 - accuracy: 0.8253 - val_loss: 0.3474 - val_accuracy: 0.8655\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4274 - accuracy: 0.8168 - val_loss: 0.3485 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4310 - accuracy: 0.8337 - val_loss: 0.3494 - val_accuracy: 0.8739\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4355 - accuracy: 0.8168 - val_loss: 0.3499 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4412 - accuracy: 0.8189 - val_loss: 0.3489 - val_accuracy: 0.8739\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4281 - accuracy: 0.8253 - val_loss: 0.3512 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4246 - accuracy: 0.8147 - val_loss: 0.3490 - val_accuracy: 0.8739\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4218 - accuracy: 0.7958 - val_loss: 0.3425 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4319 - accuracy: 0.8147 - val_loss: 0.3427 - val_accuracy: 0.8739\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4329 - accuracy: 0.8337 - val_loss: 0.3470 - val_accuracy: 0.8824\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4208 - accuracy: 0.8358 - val_loss: 0.3480 - val_accuracy: 0.8824\n",
      "297/297 [==============================] - 0s 53us/sample - loss: 0.4099 - accuracy: 0.8350\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu, total=   6.2s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7742 - accuracy: 0.5684 - val_loss: 0.5760 - val_accuracy: 0.6891\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.6404 - accuracy: 0.6568 - val_loss: 0.5477 - val_accuracy: 0.7059\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 117us/sample - loss: 0.5916 - accuracy: 0.6842 - val_loss: 0.5370 - val_accuracy: 0.7143\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.6068 - accuracy: 0.7200 - val_loss: 0.5282 - val_accuracy: 0.7395\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.5556 - accuracy: 0.7200 - val_loss: 0.5189 - val_accuracy: 0.7479\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5314 - accuracy: 0.7621 - val_loss: 0.5099 - val_accuracy: 0.7479\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5354 - accuracy: 0.7726 - val_loss: 0.4955 - val_accuracy: 0.7731\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5263 - accuracy: 0.7579 - val_loss: 0.4962 - val_accuracy: 0.7647\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5056 - accuracy: 0.7684 - val_loss: 0.4969 - val_accuracy: 0.7899\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5466 - accuracy: 0.7537 - val_loss: 0.4925 - val_accuracy: 0.7815\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5150 - accuracy: 0.7789 - val_loss: 0.4909 - val_accuracy: 0.7983\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4974 - accuracy: 0.8000 - val_loss: 0.4862 - val_accuracy: 0.7983\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5233 - accuracy: 0.7558 - val_loss: 0.4856 - val_accuracy: 0.7983\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5557 - accuracy: 0.7411 - val_loss: 0.4825 - val_accuracy: 0.7983\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4892 - accuracy: 0.7705 - val_loss: 0.4817 - val_accuracy: 0.7983\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5025 - accuracy: 0.7811 - val_loss: 0.4872 - val_accuracy: 0.8067\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.5057 - accuracy: 0.7768 - val_loss: 0.4858 - val_accuracy: 0.7983\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4693 - accuracy: 0.7979 - val_loss: 0.4839 - val_accuracy: 0.7983\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4797 - accuracy: 0.7916 - val_loss: 0.4779 - val_accuracy: 0.7815\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4716 - accuracy: 0.8147 - val_loss: 0.4738 - val_accuracy: 0.8067\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4731 - accuracy: 0.7979 - val_loss: 0.4709 - val_accuracy: 0.8067\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4871 - accuracy: 0.8063 - val_loss: 0.4710 - val_accuracy: 0.8067\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4845 - accuracy: 0.7832 - val_loss: 0.4733 - val_accuracy: 0.8151\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4766 - accuracy: 0.7832 - val_loss: 0.4725 - val_accuracy: 0.8067\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4701 - accuracy: 0.8084 - val_loss: 0.4726 - val_accuracy: 0.8067\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4514 - accuracy: 0.8126 - val_loss: 0.4728 - val_accuracy: 0.8151\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.5040 - accuracy: 0.7895 - val_loss: 0.4676 - val_accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4639 - accuracy: 0.8084 - val_loss: 0.4708 - val_accuracy: 0.8067\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4900 - accuracy: 0.7916 - val_loss: 0.4736 - val_accuracy: 0.8067\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4792 - accuracy: 0.8042 - val_loss: 0.4733 - val_accuracy: 0.8067\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4550 - accuracy: 0.8063 - val_loss: 0.4642 - val_accuracy: 0.8067\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4696 - accuracy: 0.8021 - val_loss: 0.4659 - val_accuracy: 0.8151\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4564 - accuracy: 0.8042 - val_loss: 0.4679 - val_accuracy: 0.8151\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4501 - accuracy: 0.8274 - val_loss: 0.4731 - val_accuracy: 0.8067\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4593 - accuracy: 0.8063 - val_loss: 0.4715 - val_accuracy: 0.8067\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4558 - accuracy: 0.8379 - val_loss: 0.4698 - val_accuracy: 0.8151\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4628 - accuracy: 0.8126 - val_loss: 0.4707 - val_accuracy: 0.8151\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4811 - accuracy: 0.8105 - val_loss: 0.4677 - val_accuracy: 0.8151\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4680 - accuracy: 0.7853 - val_loss: 0.4743 - val_accuracy: 0.8067\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4598 - accuracy: 0.7937 - val_loss: 0.4683 - val_accuracy: 0.8067\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4490 - accuracy: 0.8000 - val_loss: 0.4657 - val_accuracy: 0.8151\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4492 - accuracy: 0.7979 - val_loss: 0.4692 - val_accuracy: 0.8151\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4702 - accuracy: 0.7979 - val_loss: 0.4668 - val_accuracy: 0.8151\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4476 - accuracy: 0.8147 - val_loss: 0.4718 - val_accuracy: 0.8151\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4531 - accuracy: 0.8063 - val_loss: 0.4636 - val_accuracy: 0.8067\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4639 - accuracy: 0.8211 - val_loss: 0.4652 - val_accuracy: 0.8067\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4607 - accuracy: 0.8147 - val_loss: 0.4654 - val_accuracy: 0.8067\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4577 - accuracy: 0.8105 - val_loss: 0.4669 - val_accuracy: 0.8067\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4861 - accuracy: 0.8000 - val_loss: 0.4644 - val_accuracy: 0.8067\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4464 - accuracy: 0.8379 - val_loss: 0.4612 - val_accuracy: 0.8067\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4414 - accuracy: 0.8126 - val_loss: 0.4609 - val_accuracy: 0.8067\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4487 - accuracy: 0.8274 - val_loss: 0.4618 - val_accuracy: 0.8067\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4481 - accuracy: 0.8105 - val_loss: 0.4641 - val_accuracy: 0.8067\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4400 - accuracy: 0.8168 - val_loss: 0.4638 - val_accuracy: 0.8067\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4452 - accuracy: 0.8232 - val_loss: 0.4649 - val_accuracy: 0.8067\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4277 - accuracy: 0.8211 - val_loss: 0.4676 - val_accuracy: 0.8067\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4425 - accuracy: 0.8295 - val_loss: 0.4684 - val_accuracy: 0.8067\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4419 - accuracy: 0.8084 - val_loss: 0.4645 - val_accuracy: 0.8067\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4481 - accuracy: 0.8253 - val_loss: 0.4687 - val_accuracy: 0.7983\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4365 - accuracy: 0.8147 - val_loss: 0.4672 - val_accuracy: 0.7983\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4637 - accuracy: 0.8232 - val_loss: 0.4665 - val_accuracy: 0.7983\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4499 - accuracy: 0.8126 - val_loss: 0.4653 - val_accuracy: 0.8067\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4655 - accuracy: 0.8063 - val_loss: 0.4659 - val_accuracy: 0.7983\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4337 - accuracy: 0.8253 - val_loss: 0.4661 - val_accuracy: 0.7983\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4517 - accuracy: 0.8168 - val_loss: 0.4654 - val_accuracy: 0.7983\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4256 - accuracy: 0.8295 - val_loss: 0.4658 - val_accuracy: 0.8067\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4453 - accuracy: 0.8232 - val_loss: 0.4694 - val_accuracy: 0.7983\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4463 - accuracy: 0.8232 - val_loss: 0.4702 - val_accuracy: 0.7983\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4573 - accuracy: 0.8126 - val_loss: 0.4706 - val_accuracy: 0.7983\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4439 - accuracy: 0.7958 - val_loss: 0.4700 - val_accuracy: 0.7983\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4156 - accuracy: 0.8232 - val_loss: 0.4685 - val_accuracy: 0.7983\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4406 - accuracy: 0.8063 - val_loss: 0.4689 - val_accuracy: 0.7983\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4388 - accuracy: 0.8316 - val_loss: 0.4686 - val_accuracy: 0.7983\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4370 - accuracy: 0.8253 - val_loss: 0.4686 - val_accuracy: 0.7983\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4685 - accuracy: 0.8021 - val_loss: 0.4635 - val_accuracy: 0.7983\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4307 - accuracy: 0.8253 - val_loss: 0.4597 - val_accuracy: 0.7983\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4476 - accuracy: 0.8189 - val_loss: 0.4608 - val_accuracy: 0.7983\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4387 - accuracy: 0.8337 - val_loss: 0.4665 - val_accuracy: 0.7983\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4396 - accuracy: 0.8147 - val_loss: 0.4626 - val_accuracy: 0.7983\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4460 - accuracy: 0.8189 - val_loss: 0.4619 - val_accuracy: 0.7983\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4558 - accuracy: 0.8253 - val_loss: 0.4588 - val_accuracy: 0.7983\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4425 - accuracy: 0.8105 - val_loss: 0.4618 - val_accuracy: 0.7983\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4338 - accuracy: 0.8232 - val_loss: 0.4605 - val_accuracy: 0.7983\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4525 - accuracy: 0.8295 - val_loss: 0.4633 - val_accuracy: 0.7983\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4463 - accuracy: 0.8232 - val_loss: 0.4646 - val_accuracy: 0.7983\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4450 - accuracy: 0.8232 - val_loss: 0.4630 - val_accuracy: 0.7983\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4482 - accuracy: 0.8232 - val_loss: 0.4627 - val_accuracy: 0.7983\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4367 - accuracy: 0.8316 - val_loss: 0.4634 - val_accuracy: 0.8067\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4404 - accuracy: 0.8105 - val_loss: 0.4627 - val_accuracy: 0.7983\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4394 - accuracy: 0.8253 - val_loss: 0.4632 - val_accuracy: 0.7983\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4450 - accuracy: 0.8211 - val_loss: 0.4599 - val_accuracy: 0.7983\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4340 - accuracy: 0.8274 - val_loss: 0.4613 - val_accuracy: 0.7983\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4505 - accuracy: 0.8084 - val_loss: 0.4599 - val_accuracy: 0.7983\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4395 - accuracy: 0.8105 - val_loss: 0.4652 - val_accuracy: 0.7983\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4446 - accuracy: 0.8211 - val_loss: 0.4652 - val_accuracy: 0.7983\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4460 - accuracy: 0.8211 - val_loss: 0.4608 - val_accuracy: 0.7983\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4231 - accuracy: 0.8316 - val_loss: 0.4630 - val_accuracy: 0.7983\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4274 - accuracy: 0.8379 - val_loss: 0.4621 - val_accuracy: 0.7983\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4150 - accuracy: 0.8274 - val_loss: 0.4648 - val_accuracy: 0.7983\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4497 - accuracy: 0.8126 - val_loss: 0.4640 - val_accuracy: 0.7983\n",
      "297/297 [==============================] - 0s 53us/sample - loss: 0.3811 - accuracy: 0.8451\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=4, activation=selu, total=   6.0s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu .........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6580 - accuracy: 0.6211 - val_loss: 0.6109 - val_accuracy: 0.8571\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6090 - accuracy: 0.7389 - val_loss: 0.5706 - val_accuracy: 0.8655\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5774 - accuracy: 0.7621 - val_loss: 0.5445 - val_accuracy: 0.8824\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5693 - accuracy: 0.7726 - val_loss: 0.5255 - val_accuracy: 0.8739\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5486 - accuracy: 0.7663 - val_loss: 0.5119 - val_accuracy: 0.8655\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5353 - accuracy: 0.7621 - val_loss: 0.5012 - val_accuracy: 0.8655\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5385 - accuracy: 0.7432 - val_loss: 0.4920 - val_accuracy: 0.8655\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5264 - accuracy: 0.7726 - val_loss: 0.4843 - val_accuracy: 0.8571\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5157 - accuracy: 0.7621 - val_loss: 0.4779 - val_accuracy: 0.8571\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5123 - accuracy: 0.7768 - val_loss: 0.4717 - val_accuracy: 0.8571\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5083 - accuracy: 0.7874 - val_loss: 0.4661 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5049 - accuracy: 0.7853 - val_loss: 0.4611 - val_accuracy: 0.8571\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5010 - accuracy: 0.7832 - val_loss: 0.4569 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5010 - accuracy: 0.7811 - val_loss: 0.4528 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4883 - accuracy: 0.7853 - val_loss: 0.4490 - val_accuracy: 0.8571\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4827 - accuracy: 0.7979 - val_loss: 0.4457 - val_accuracy: 0.8571\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4772 - accuracy: 0.7916 - val_loss: 0.4429 - val_accuracy: 0.8487\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4760 - accuracy: 0.7958 - val_loss: 0.4399 - val_accuracy: 0.8655\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4732 - accuracy: 0.8021 - val_loss: 0.4364 - val_accuracy: 0.8739\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4757 - accuracy: 0.7937 - val_loss: 0.4337 - val_accuracy: 0.8824\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4670 - accuracy: 0.8021 - val_loss: 0.4310 - val_accuracy: 0.8824\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4678 - accuracy: 0.8168 - val_loss: 0.4288 - val_accuracy: 0.8908\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4651 - accuracy: 0.7937 - val_loss: 0.4264 - val_accuracy: 0.8824\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4600 - accuracy: 0.8084 - val_loss: 0.4242 - val_accuracy: 0.8824\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4473 - accuracy: 0.8147 - val_loss: 0.4221 - val_accuracy: 0.8739\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4544 - accuracy: 0.8126 - val_loss: 0.4200 - val_accuracy: 0.8739\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4576 - accuracy: 0.8189 - val_loss: 0.4180 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4512 - accuracy: 0.8253 - val_loss: 0.4163 - val_accuracy: 0.8655\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4549 - accuracy: 0.8021 - val_loss: 0.4146 - val_accuracy: 0.8655\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4509 - accuracy: 0.8211 - val_loss: 0.4130 - val_accuracy: 0.8655\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4495 - accuracy: 0.8084 - val_loss: 0.4116 - val_accuracy: 0.8655\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4510 - accuracy: 0.7979 - val_loss: 0.4099 - val_accuracy: 0.8655\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4529 - accuracy: 0.7979 - val_loss: 0.4084 - val_accuracy: 0.8655\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4488 - accuracy: 0.8189 - val_loss: 0.4068 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4565 - accuracy: 0.7979 - val_loss: 0.4057 - val_accuracy: 0.8655\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4448 - accuracy: 0.7895 - val_loss: 0.4042 - val_accuracy: 0.8655\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4426 - accuracy: 0.8147 - val_loss: 0.4031 - val_accuracy: 0.8571\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 116us/sample - loss: 0.4430 - accuracy: 0.8147 - val_loss: 0.4021 - val_accuracy: 0.8571\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4423 - accuracy: 0.8021 - val_loss: 0.4012 - val_accuracy: 0.8571\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4337 - accuracy: 0.8253 - val_loss: 0.4000 - val_accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4363 - accuracy: 0.8189 - val_loss: 0.3991 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4364 - accuracy: 0.8042 - val_loss: 0.3980 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4332 - accuracy: 0.8211 - val_loss: 0.3969 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4291 - accuracy: 0.8211 - val_loss: 0.3964 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4459 - accuracy: 0.8063 - val_loss: 0.3955 - val_accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4364 - accuracy: 0.8274 - val_loss: 0.3949 - val_accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4358 - accuracy: 0.8168 - val_loss: 0.3943 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4310 - accuracy: 0.8232 - val_loss: 0.3934 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4294 - accuracy: 0.8211 - val_loss: 0.3926 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4349 - accuracy: 0.8147 - val_loss: 0.3921 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4196 - accuracy: 0.8253 - val_loss: 0.3910 - val_accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4202 - accuracy: 0.8147 - val_loss: 0.3903 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4348 - accuracy: 0.8126 - val_loss: 0.3898 - val_accuracy: 0.8571\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4290 - accuracy: 0.8274 - val_loss: 0.3889 - val_accuracy: 0.8571\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4310 - accuracy: 0.8105 - val_loss: 0.3884 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4344 - accuracy: 0.8147 - val_loss: 0.3881 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4253 - accuracy: 0.8147 - val_loss: 0.3876 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4259 - accuracy: 0.8211 - val_loss: 0.3870 - val_accuracy: 0.8655\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4224 - accuracy: 0.8274 - val_loss: 0.3865 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4268 - accuracy: 0.8000 - val_loss: 0.3860 - val_accuracy: 0.8571\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4280 - accuracy: 0.8274 - val_loss: 0.3856 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4228 - accuracy: 0.8126 - val_loss: 0.3849 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4227 - accuracy: 0.8274 - val_loss: 0.3845 - val_accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4277 - accuracy: 0.8084 - val_loss: 0.3842 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4336 - accuracy: 0.8105 - val_loss: 0.3840 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4262 - accuracy: 0.8084 - val_loss: 0.3838 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4166 - accuracy: 0.8147 - val_loss: 0.3836 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4248 - accuracy: 0.8253 - val_loss: 0.3832 - val_accuracy: 0.8571\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4177 - accuracy: 0.8189 - val_loss: 0.3830 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4245 - accuracy: 0.8232 - val_loss: 0.3827 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4250 - accuracy: 0.8316 - val_loss: 0.3826 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4269 - accuracy: 0.8274 - val_loss: 0.3823 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4142 - accuracy: 0.8253 - val_loss: 0.3819 - val_accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4134 - accuracy: 0.8211 - val_loss: 0.3813 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4190 - accuracy: 0.8147 - val_loss: 0.3811 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4226 - accuracy: 0.8211 - val_loss: 0.3811 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4158 - accuracy: 0.8126 - val_loss: 0.3803 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4217 - accuracy: 0.8211 - val_loss: 0.3796 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4200 - accuracy: 0.8105 - val_loss: 0.3795 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4233 - accuracy: 0.8189 - val_loss: 0.3792 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4161 - accuracy: 0.8253 - val_loss: 0.3789 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4167 - accuracy: 0.8316 - val_loss: 0.3786 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4170 - accuracy: 0.8189 - val_loss: 0.3780 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4126 - accuracy: 0.8168 - val_loss: 0.3781 - val_accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4162 - accuracy: 0.8316 - val_loss: 0.3780 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4118 - accuracy: 0.8189 - val_loss: 0.3777 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4109 - accuracy: 0.8232 - val_loss: 0.3774 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4246 - accuracy: 0.8105 - val_loss: 0.3771 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4146 - accuracy: 0.8295 - val_loss: 0.3770 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4134 - accuracy: 0.8337 - val_loss: 0.3769 - val_accuracy: 0.8571\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4126 - accuracy: 0.8211 - val_loss: 0.3770 - val_accuracy: 0.8571\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4225 - accuracy: 0.8168 - val_loss: 0.3769 - val_accuracy: 0.8571\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4196 - accuracy: 0.8337 - val_loss: 0.3767 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4169 - accuracy: 0.8253 - val_loss: 0.3766 - val_accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4135 - accuracy: 0.8295 - val_loss: 0.3764 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4159 - accuracy: 0.8253 - val_loss: 0.3764 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4108 - accuracy: 0.8042 - val_loss: 0.3757 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4173 - accuracy: 0.8126 - val_loss: 0.3755 - val_accuracy: 0.8571\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4119 - accuracy: 0.8316 - val_loss: 0.3754 - val_accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4170 - accuracy: 0.8253 - val_loss: 0.3749 - val_accuracy: 0.8571\n",
      "297/297 [==============================] - 0s 51us/sample - loss: 0.4627 - accuracy: 0.8148\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu, total=   4.8s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu .........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7836 - accuracy: 0.4232 - val_loss: 0.7368 - val_accuracy: 0.4958\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.7138 - accuracy: 0.5558 - val_loss: 0.6820 - val_accuracy: 0.6303\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6689 - accuracy: 0.6105 - val_loss: 0.6442 - val_accuracy: 0.6639\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.6240 - accuracy: 0.6463 - val_loss: 0.6182 - val_accuracy: 0.6639\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6079 - accuracy: 0.6800 - val_loss: 0.5978 - val_accuracy: 0.6555\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5874 - accuracy: 0.6989 - val_loss: 0.5823 - val_accuracy: 0.6723\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5828 - accuracy: 0.7137 - val_loss: 0.5685 - val_accuracy: 0.6807\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5681 - accuracy: 0.7326 - val_loss: 0.5567 - val_accuracy: 0.6975\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5473 - accuracy: 0.7495 - val_loss: 0.5467 - val_accuracy: 0.7059\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5403 - accuracy: 0.7537 - val_loss: 0.5377 - val_accuracy: 0.7143\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5347 - accuracy: 0.7389 - val_loss: 0.5293 - val_accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5270 - accuracy: 0.7516 - val_loss: 0.5217 - val_accuracy: 0.7143\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5222 - accuracy: 0.7621 - val_loss: 0.5146 - val_accuracy: 0.7227\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5226 - accuracy: 0.7663 - val_loss: 0.5076 - val_accuracy: 0.7311\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5129 - accuracy: 0.7768 - val_loss: 0.5016 - val_accuracy: 0.7395\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5122 - accuracy: 0.7705 - val_loss: 0.4957 - val_accuracy: 0.7479\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5049 - accuracy: 0.7768 - val_loss: 0.4902 - val_accuracy: 0.7563\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4995 - accuracy: 0.7853 - val_loss: 0.4849 - val_accuracy: 0.7731\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5048 - accuracy: 0.7684 - val_loss: 0.4804 - val_accuracy: 0.7731\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4918 - accuracy: 0.7832 - val_loss: 0.4757 - val_accuracy: 0.7815\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4873 - accuracy: 0.7874 - val_loss: 0.4716 - val_accuracy: 0.7983\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4928 - accuracy: 0.7811 - val_loss: 0.4675 - val_accuracy: 0.7983\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4858 - accuracy: 0.7874 - val_loss: 0.4637 - val_accuracy: 0.7983\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4781 - accuracy: 0.7979 - val_loss: 0.4599 - val_accuracy: 0.7983\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4751 - accuracy: 0.7958 - val_loss: 0.4564 - val_accuracy: 0.7983\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4731 - accuracy: 0.7937 - val_loss: 0.4531 - val_accuracy: 0.7983\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4766 - accuracy: 0.7937 - val_loss: 0.4501 - val_accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4726 - accuracy: 0.8000 - val_loss: 0.4472 - val_accuracy: 0.8067\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4782 - accuracy: 0.7979 - val_loss: 0.4444 - val_accuracy: 0.8151\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4661 - accuracy: 0.8189 - val_loss: 0.4416 - val_accuracy: 0.8151\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4669 - accuracy: 0.8021 - val_loss: 0.4393 - val_accuracy: 0.8235\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4647 - accuracy: 0.8063 - val_loss: 0.4368 - val_accuracy: 0.8235\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4657 - accuracy: 0.8063 - val_loss: 0.4345 - val_accuracy: 0.8235\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4650 - accuracy: 0.8042 - val_loss: 0.4325 - val_accuracy: 0.8319\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4545 - accuracy: 0.8042 - val_loss: 0.4305 - val_accuracy: 0.8319\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4604 - accuracy: 0.8084 - val_loss: 0.4284 - val_accuracy: 0.8319\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4590 - accuracy: 0.8042 - val_loss: 0.4265 - val_accuracy: 0.8319\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4638 - accuracy: 0.7958 - val_loss: 0.4247 - val_accuracy: 0.8319\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4576 - accuracy: 0.8000 - val_loss: 0.4231 - val_accuracy: 0.8319\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4542 - accuracy: 0.8021 - val_loss: 0.4214 - val_accuracy: 0.8319\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4538 - accuracy: 0.8000 - val_loss: 0.4197 - val_accuracy: 0.8319\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4525 - accuracy: 0.8084 - val_loss: 0.4180 - val_accuracy: 0.8319\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4495 - accuracy: 0.8084 - val_loss: 0.4164 - val_accuracy: 0.8319\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4594 - accuracy: 0.8042 - val_loss: 0.4149 - val_accuracy: 0.8319\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4624 - accuracy: 0.8063 - val_loss: 0.4135 - val_accuracy: 0.8319\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4525 - accuracy: 0.8084 - val_loss: 0.4121 - val_accuracy: 0.8319\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4432 - accuracy: 0.8084 - val_loss: 0.4108 - val_accuracy: 0.8319\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4476 - accuracy: 0.8168 - val_loss: 0.4095 - val_accuracy: 0.8403\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4573 - accuracy: 0.8000 - val_loss: 0.4083 - val_accuracy: 0.8403\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4481 - accuracy: 0.8147 - val_loss: 0.4072 - val_accuracy: 0.8403\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4476 - accuracy: 0.8126 - val_loss: 0.4060 - val_accuracy: 0.8403\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4467 - accuracy: 0.8042 - val_loss: 0.4048 - val_accuracy: 0.8403\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4404 - accuracy: 0.8189 - val_loss: 0.4038 - val_accuracy: 0.8403\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4442 - accuracy: 0.8105 - val_loss: 0.4027 - val_accuracy: 0.8403\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4467 - accuracy: 0.8042 - val_loss: 0.4018 - val_accuracy: 0.8403\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4415 - accuracy: 0.8105 - val_loss: 0.4009 - val_accuracy: 0.8403\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4456 - accuracy: 0.8084 - val_loss: 0.4000 - val_accuracy: 0.8403\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4382 - accuracy: 0.8042 - val_loss: 0.3991 - val_accuracy: 0.8403\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4431 - accuracy: 0.8042 - val_loss: 0.3980 - val_accuracy: 0.8403\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4410 - accuracy: 0.8147 - val_loss: 0.3973 - val_accuracy: 0.8403\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4417 - accuracy: 0.8105 - val_loss: 0.3966 - val_accuracy: 0.8403\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4410 - accuracy: 0.8168 - val_loss: 0.3958 - val_accuracy: 0.8403\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4414 - accuracy: 0.8042 - val_loss: 0.3950 - val_accuracy: 0.8487\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4397 - accuracy: 0.8126 - val_loss: 0.3943 - val_accuracy: 0.8403\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4371 - accuracy: 0.8147 - val_loss: 0.3938 - val_accuracy: 0.8403\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4435 - accuracy: 0.8042 - val_loss: 0.3931 - val_accuracy: 0.8403\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4317 - accuracy: 0.8084 - val_loss: 0.3923 - val_accuracy: 0.8403\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4329 - accuracy: 0.8147 - val_loss: 0.3917 - val_accuracy: 0.8403\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4399 - accuracy: 0.8105 - val_loss: 0.3911 - val_accuracy: 0.8403\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4369 - accuracy: 0.8168 - val_loss: 0.3903 - val_accuracy: 0.8403\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4369 - accuracy: 0.8147 - val_loss: 0.3897 - val_accuracy: 0.8403\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4376 - accuracy: 0.8147 - val_loss: 0.3892 - val_accuracy: 0.8403\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4365 - accuracy: 0.8232 - val_loss: 0.3886 - val_accuracy: 0.8403\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4353 - accuracy: 0.8211 - val_loss: 0.3882 - val_accuracy: 0.8487\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4419 - accuracy: 0.8126 - val_loss: 0.3877 - val_accuracy: 0.8487\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4347 - accuracy: 0.8021 - val_loss: 0.3872 - val_accuracy: 0.8487\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4339 - accuracy: 0.8084 - val_loss: 0.3866 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4343 - accuracy: 0.8126 - val_loss: 0.3860 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4375 - accuracy: 0.8105 - val_loss: 0.3857 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4293 - accuracy: 0.8253 - val_loss: 0.3850 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4388 - accuracy: 0.8147 - val_loss: 0.3844 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4362 - accuracy: 0.8105 - val_loss: 0.3839 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4324 - accuracy: 0.8147 - val_loss: 0.3834 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4388 - accuracy: 0.8211 - val_loss: 0.3829 - val_accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4326 - accuracy: 0.8189 - val_loss: 0.3823 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4409 - accuracy: 0.8042 - val_loss: 0.3819 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4294 - accuracy: 0.8211 - val_loss: 0.3813 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4300 - accuracy: 0.8189 - val_loss: 0.3810 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4315 - accuracy: 0.8126 - val_loss: 0.3807 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4428 - accuracy: 0.8042 - val_loss: 0.3805 - val_accuracy: 0.8571\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4294 - accuracy: 0.8105 - val_loss: 0.3800 - val_accuracy: 0.8571\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4329 - accuracy: 0.8147 - val_loss: 0.3797 - val_accuracy: 0.8571\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4278 - accuracy: 0.8105 - val_loss: 0.3791 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4259 - accuracy: 0.8126 - val_loss: 0.3789 - val_accuracy: 0.8571\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4313 - accuracy: 0.8084 - val_loss: 0.3785 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4294 - accuracy: 0.8126 - val_loss: 0.3781 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4325 - accuracy: 0.8084 - val_loss: 0.3777 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4276 - accuracy: 0.8189 - val_loss: 0.3773 - val_accuracy: 0.8571\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4290 - accuracy: 0.8211 - val_loss: 0.3772 - val_accuracy: 0.8571\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4350 - accuracy: 0.8168 - val_loss: 0.3772 - val_accuracy: 0.8571\n",
      "297/297 [==============================] - 0s 51us/sample - loss: 0.4503 - accuracy: 0.8114\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu, total=   5.1s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu .........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6980 - accuracy: 0.5368 - val_loss: 0.6581 - val_accuracy: 0.6723\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6694 - accuracy: 0.6274 - val_loss: 0.6346 - val_accuracy: 0.6555\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6503 - accuracy: 0.6316 - val_loss: 0.6168 - val_accuracy: 0.6891\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6286 - accuracy: 0.6589 - val_loss: 0.6034 - val_accuracy: 0.6975\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.6083 - accuracy: 0.6547 - val_loss: 0.5926 - val_accuracy: 0.7059\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6021 - accuracy: 0.6568 - val_loss: 0.5836 - val_accuracy: 0.7227\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5793 - accuracy: 0.6926 - val_loss: 0.5757 - val_accuracy: 0.7227\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5723 - accuracy: 0.7116 - val_loss: 0.5686 - val_accuracy: 0.7395\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5658 - accuracy: 0.7137 - val_loss: 0.5626 - val_accuracy: 0.7479\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5548 - accuracy: 0.7263 - val_loss: 0.5574 - val_accuracy: 0.7563\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5466 - accuracy: 0.7263 - val_loss: 0.5523 - val_accuracy: 0.7647\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5411 - accuracy: 0.7368 - val_loss: 0.5481 - val_accuracy: 0.7647\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5357 - accuracy: 0.7347 - val_loss: 0.5440 - val_accuracy: 0.7563\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5300 - accuracy: 0.7432 - val_loss: 0.5403 - val_accuracy: 0.7731\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5200 - accuracy: 0.7853 - val_loss: 0.5369 - val_accuracy: 0.7815\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5196 - accuracy: 0.7937 - val_loss: 0.5335 - val_accuracy: 0.7815\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5149 - accuracy: 0.7705 - val_loss: 0.5304 - val_accuracy: 0.7815\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5051 - accuracy: 0.7853 - val_loss: 0.5277 - val_accuracy: 0.7815\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5072 - accuracy: 0.7916 - val_loss: 0.5249 - val_accuracy: 0.7983\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5023 - accuracy: 0.7726 - val_loss: 0.5227 - val_accuracy: 0.7983\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4918 - accuracy: 0.7958 - val_loss: 0.5204 - val_accuracy: 0.7983\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5015 - accuracy: 0.7874 - val_loss: 0.5184 - val_accuracy: 0.7899\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4840 - accuracy: 0.8063 - val_loss: 0.5162 - val_accuracy: 0.7899\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4881 - accuracy: 0.8084 - val_loss: 0.5143 - val_accuracy: 0.7899\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4865 - accuracy: 0.8063 - val_loss: 0.5124 - val_accuracy: 0.7899\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4834 - accuracy: 0.8084 - val_loss: 0.5108 - val_accuracy: 0.7899\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4830 - accuracy: 0.8042 - val_loss: 0.5094 - val_accuracy: 0.7899\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4766 - accuracy: 0.8042 - val_loss: 0.5079 - val_accuracy: 0.7899\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4786 - accuracy: 0.8042 - val_loss: 0.5064 - val_accuracy: 0.7899\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4747 - accuracy: 0.8063 - val_loss: 0.5053 - val_accuracy: 0.7899\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4776 - accuracy: 0.8000 - val_loss: 0.5039 - val_accuracy: 0.7899\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4778 - accuracy: 0.8168 - val_loss: 0.5027 - val_accuracy: 0.7983\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4780 - accuracy: 0.7979 - val_loss: 0.5016 - val_accuracy: 0.7983\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4792 - accuracy: 0.8147 - val_loss: 0.5007 - val_accuracy: 0.7983\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4684 - accuracy: 0.8105 - val_loss: 0.4998 - val_accuracy: 0.7983\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4730 - accuracy: 0.8021 - val_loss: 0.4987 - val_accuracy: 0.7983\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4775 - accuracy: 0.8000 - val_loss: 0.4979 - val_accuracy: 0.7983\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4593 - accuracy: 0.8168 - val_loss: 0.4970 - val_accuracy: 0.7899\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4598 - accuracy: 0.8147 - val_loss: 0.4963 - val_accuracy: 0.7899\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4550 - accuracy: 0.8021 - val_loss: 0.4954 - val_accuracy: 0.7899\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4587 - accuracy: 0.8211 - val_loss: 0.4947 - val_accuracy: 0.7899\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4636 - accuracy: 0.8168 - val_loss: 0.4940 - val_accuracy: 0.7899\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4646 - accuracy: 0.8084 - val_loss: 0.4933 - val_accuracy: 0.7899\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4560 - accuracy: 0.8168 - val_loss: 0.4928 - val_accuracy: 0.7899\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4550 - accuracy: 0.8147 - val_loss: 0.4921 - val_accuracy: 0.7899\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4564 - accuracy: 0.8253 - val_loss: 0.4915 - val_accuracy: 0.7815\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4590 - accuracy: 0.8126 - val_loss: 0.4910 - val_accuracy: 0.7899\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4541 - accuracy: 0.8105 - val_loss: 0.4903 - val_accuracy: 0.7899\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4549 - accuracy: 0.8084 - val_loss: 0.4901 - val_accuracy: 0.7815\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4563 - accuracy: 0.8168 - val_loss: 0.4895 - val_accuracy: 0.7815\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4502 - accuracy: 0.8211 - val_loss: 0.4892 - val_accuracy: 0.7815\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4572 - accuracy: 0.8126 - val_loss: 0.4889 - val_accuracy: 0.7815\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4452 - accuracy: 0.8147 - val_loss: 0.4887 - val_accuracy: 0.7815\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4543 - accuracy: 0.8189 - val_loss: 0.4881 - val_accuracy: 0.7815\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4471 - accuracy: 0.8063 - val_loss: 0.4874 - val_accuracy: 0.7815\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4505 - accuracy: 0.8147 - val_loss: 0.4870 - val_accuracy: 0.7731\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4429 - accuracy: 0.8295 - val_loss: 0.4868 - val_accuracy: 0.7731\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4424 - accuracy: 0.8232 - val_loss: 0.4865 - val_accuracy: 0.7731\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4508 - accuracy: 0.8211 - val_loss: 0.4862 - val_accuracy: 0.7731\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4472 - accuracy: 0.8042 - val_loss: 0.4859 - val_accuracy: 0.7731\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4408 - accuracy: 0.8084 - val_loss: 0.4857 - val_accuracy: 0.7731\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4400 - accuracy: 0.8168 - val_loss: 0.4856 - val_accuracy: 0.7731\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4427 - accuracy: 0.8168 - val_loss: 0.4853 - val_accuracy: 0.7731\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4498 - accuracy: 0.8126 - val_loss: 0.4850 - val_accuracy: 0.7731\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4472 - accuracy: 0.8084 - val_loss: 0.4846 - val_accuracy: 0.7731\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4392 - accuracy: 0.8189 - val_loss: 0.4845 - val_accuracy: 0.7731\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4500 - accuracy: 0.8147 - val_loss: 0.4842 - val_accuracy: 0.7731\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4512 - accuracy: 0.8126 - val_loss: 0.4840 - val_accuracy: 0.7731\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4516 - accuracy: 0.8105 - val_loss: 0.4839 - val_accuracy: 0.7731\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4488 - accuracy: 0.8126 - val_loss: 0.4836 - val_accuracy: 0.7731\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4379 - accuracy: 0.8211 - val_loss: 0.4835 - val_accuracy: 0.7731\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4399 - accuracy: 0.8168 - val_loss: 0.4830 - val_accuracy: 0.7731\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4448 - accuracy: 0.8105 - val_loss: 0.4829 - val_accuracy: 0.7815\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4382 - accuracy: 0.8126 - val_loss: 0.4827 - val_accuracy: 0.7815\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4462 - accuracy: 0.8126 - val_loss: 0.4824 - val_accuracy: 0.7815\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4346 - accuracy: 0.8189 - val_loss: 0.4824 - val_accuracy: 0.7731\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4474 - accuracy: 0.8168 - val_loss: 0.4819 - val_accuracy: 0.7815\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4455 - accuracy: 0.8189 - val_loss: 0.4817 - val_accuracy: 0.7815\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4449 - accuracy: 0.8189 - val_loss: 0.4817 - val_accuracy: 0.7815\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4495 - accuracy: 0.8105 - val_loss: 0.4815 - val_accuracy: 0.7815\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4451 - accuracy: 0.8147 - val_loss: 0.4812 - val_accuracy: 0.7815\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4410 - accuracy: 0.8147 - val_loss: 0.4810 - val_accuracy: 0.7815\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4443 - accuracy: 0.8105 - val_loss: 0.4809 - val_accuracy: 0.7815\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.4471 - accuracy: 0.8084 - val_loss: 0.4808 - val_accuracy: 0.7815\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4436 - accuracy: 0.8126 - val_loss: 0.4806 - val_accuracy: 0.7815\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4318 - accuracy: 0.8189 - val_loss: 0.4803 - val_accuracy: 0.7815\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4433 - accuracy: 0.8126 - val_loss: 0.4800 - val_accuracy: 0.7815\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4410 - accuracy: 0.8168 - val_loss: 0.4799 - val_accuracy: 0.7899\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4415 - accuracy: 0.8147 - val_loss: 0.4796 - val_accuracy: 0.7815\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4370 - accuracy: 0.8084 - val_loss: 0.4794 - val_accuracy: 0.7815\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4440 - accuracy: 0.8168 - val_loss: 0.4792 - val_accuracy: 0.7815\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4505 - accuracy: 0.8126 - val_loss: 0.4790 - val_accuracy: 0.7815\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4377 - accuracy: 0.8232 - val_loss: 0.4788 - val_accuracy: 0.7815\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4465 - accuracy: 0.8211 - val_loss: 0.4785 - val_accuracy: 0.7815\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4347 - accuracy: 0.8211 - val_loss: 0.4783 - val_accuracy: 0.7815\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4395 - accuracy: 0.8126 - val_loss: 0.4782 - val_accuracy: 0.7899\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4369 - accuracy: 0.8105 - val_loss: 0.4781 - val_accuracy: 0.7899\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4448 - accuracy: 0.8189 - val_loss: 0.4781 - val_accuracy: 0.7899\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4339 - accuracy: 0.8168 - val_loss: 0.4780 - val_accuracy: 0.7899\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4433 - accuracy: 0.8189 - val_loss: 0.4781 - val_accuracy: 0.7899\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.3961 - accuracy: 0.8283\n",
      "[CV]  optimizer=SGD, n_neurons=77, n_hidden=1, activation=elu, total=   4.8s\n",
      "[CV] optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu ......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.6863 - accuracy: 0.5305 - val_loss: 0.6584 - val_accuracy: 0.7227\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6569 - accuracy: 0.6379 - val_loss: 0.6177 - val_accuracy: 0.7227\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.6252 - accuracy: 0.6653 - val_loss: 0.5753 - val_accuracy: 0.7311\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.6020 - accuracy: 0.7011 - val_loss: 0.5277 - val_accuracy: 0.7815\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5477 - accuracy: 0.7621 - val_loss: 0.4755 - val_accuracy: 0.8403\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5309 - accuracy: 0.7768 - val_loss: 0.4307 - val_accuracy: 0.8739\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4933 - accuracy: 0.7684 - val_loss: 0.3955 - val_accuracy: 0.8908\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4630 - accuracy: 0.8147 - val_loss: 0.3746 - val_accuracy: 0.8824\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4702 - accuracy: 0.8147 - val_loss: 0.3786 - val_accuracy: 0.8739\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4739 - accuracy: 0.7979 - val_loss: 0.3722 - val_accuracy: 0.8824\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4399 - accuracy: 0.8126 - val_loss: 0.3592 - val_accuracy: 0.8739\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4368 - accuracy: 0.8379 - val_loss: 0.3492 - val_accuracy: 0.8739\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4076 - accuracy: 0.8168 - val_loss: 0.3459 - val_accuracy: 0.8908\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4362 - accuracy: 0.8274 - val_loss: 0.3465 - val_accuracy: 0.8992\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4198 - accuracy: 0.8379 - val_loss: 0.3560 - val_accuracy: 0.8571\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4198 - accuracy: 0.8232 - val_loss: 0.3403 - val_accuracy: 0.8992\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4259 - accuracy: 0.8189 - val_loss: 0.3509 - val_accuracy: 0.8824\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3897 - accuracy: 0.8442 - val_loss: 0.3502 - val_accuracy: 0.8655\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4173 - accuracy: 0.8295 - val_loss: 0.3423 - val_accuracy: 0.8739\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4041 - accuracy: 0.8400 - val_loss: 0.3474 - val_accuracy: 0.8655\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3917 - accuracy: 0.8484 - val_loss: 0.3450 - val_accuracy: 0.8655\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3921 - accuracy: 0.8274 - val_loss: 0.3462 - val_accuracy: 0.8571\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3765 - accuracy: 0.8611 - val_loss: 0.3431 - val_accuracy: 0.8739\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3704 - accuracy: 0.8463 - val_loss: 0.3363 - val_accuracy: 0.8824\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3886 - accuracy: 0.8463 - val_loss: 0.3520 - val_accuracy: 0.8571\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3869 - accuracy: 0.8505 - val_loss: 0.3471 - val_accuracy: 0.8655\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3815 - accuracy: 0.8421 - val_loss: 0.3391 - val_accuracy: 0.8824\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3892 - accuracy: 0.8484 - val_loss: 0.3375 - val_accuracy: 0.8824\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3784 - accuracy: 0.8526 - val_loss: 0.3457 - val_accuracy: 0.8571\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3897 - accuracy: 0.8379 - val_loss: 0.3451 - val_accuracy: 0.8655\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3683 - accuracy: 0.8505 - val_loss: 0.3508 - val_accuracy: 0.8571\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3806 - accuracy: 0.8568 - val_loss: 0.3540 - val_accuracy: 0.8655\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3751 - accuracy: 0.8526 - val_loss: 0.3590 - val_accuracy: 0.8487\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3658 - accuracy: 0.8484 - val_loss: 0.3420 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3806 - accuracy: 0.8463 - val_loss: 0.3542 - val_accuracy: 0.8739\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3687 - accuracy: 0.8526 - val_loss: 0.3471 - val_accuracy: 0.8739\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3541 - accuracy: 0.8547 - val_loss: 0.3528 - val_accuracy: 0.8739\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3613 - accuracy: 0.8400 - val_loss: 0.3493 - val_accuracy: 0.8739\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3716 - accuracy: 0.8653 - val_loss: 0.3585 - val_accuracy: 0.8739\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3689 - accuracy: 0.8442 - val_loss: 0.3524 - val_accuracy: 0.8739\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3662 - accuracy: 0.8674 - val_loss: 0.3573 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3627 - accuracy: 0.8695 - val_loss: 0.3569 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3537 - accuracy: 0.8674 - val_loss: 0.3588 - val_accuracy: 0.8739\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3784 - accuracy: 0.8611 - val_loss: 0.3607 - val_accuracy: 0.8739\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3502 - accuracy: 0.8463 - val_loss: 0.3643 - val_accuracy: 0.8739\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3581 - accuracy: 0.8526 - val_loss: 0.3740 - val_accuracy: 0.8739\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3644 - accuracy: 0.8421 - val_loss: 0.3620 - val_accuracy: 0.8824\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3422 - accuracy: 0.8526 - val_loss: 0.3574 - val_accuracy: 0.8824\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3576 - accuracy: 0.8611 - val_loss: 0.3550 - val_accuracy: 0.8824\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3422 - accuracy: 0.8611 - val_loss: 0.3619 - val_accuracy: 0.8739\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3299 - accuracy: 0.8611 - val_loss: 0.3771 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3621 - accuracy: 0.8505 - val_loss: 0.3680 - val_accuracy: 0.8739\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3418 - accuracy: 0.8526 - val_loss: 0.3764 - val_accuracy: 0.8739\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3640 - accuracy: 0.8568 - val_loss: 0.3639 - val_accuracy: 0.8739\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3496 - accuracy: 0.8568 - val_loss: 0.3687 - val_accuracy: 0.8739\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3570 - accuracy: 0.8547 - val_loss: 0.3731 - val_accuracy: 0.8739\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3524 - accuracy: 0.8505 - val_loss: 0.3732 - val_accuracy: 0.8739\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3395 - accuracy: 0.8526 - val_loss: 0.3660 - val_accuracy: 0.8739\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3454 - accuracy: 0.8589 - val_loss: 0.3739 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3601 - accuracy: 0.8611 - val_loss: 0.3742 - val_accuracy: 0.8655\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3525 - accuracy: 0.8653 - val_loss: 0.3671 - val_accuracy: 0.8655\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3325 - accuracy: 0.8737 - val_loss: 0.3807 - val_accuracy: 0.8487\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3618 - accuracy: 0.8568 - val_loss: 0.3834 - val_accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3281 - accuracy: 0.8737 - val_loss: 0.4092 - val_accuracy: 0.8403\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3293 - accuracy: 0.8737 - val_loss: 0.3867 - val_accuracy: 0.8655\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3402 - accuracy: 0.8716 - val_loss: 0.3832 - val_accuracy: 0.8739\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3325 - accuracy: 0.8716 - val_loss: 0.3895 - val_accuracy: 0.8739\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.3403 - accuracy: 0.8674 - val_loss: 0.3929 - val_accuracy: 0.8571\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3380 - accuracy: 0.8547 - val_loss: 0.3843 - val_accuracy: 0.8655\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3322 - accuracy: 0.8611 - val_loss: 0.3914 - val_accuracy: 0.8655\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3373 - accuracy: 0.8632 - val_loss: 0.3925 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3554 - accuracy: 0.8632 - val_loss: 0.3971 - val_accuracy: 0.8487\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3095 - accuracy: 0.8653 - val_loss: 0.4049 - val_accuracy: 0.8487\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3361 - accuracy: 0.8716 - val_loss: 0.4121 - val_accuracy: 0.8487\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3006 - accuracy: 0.8737 - val_loss: 0.4177 - val_accuracy: 0.8403\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3228 - accuracy: 0.8611 - val_loss: 0.4295 - val_accuracy: 0.8403\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3074 - accuracy: 0.8611 - val_loss: 0.4079 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3044 - accuracy: 0.8674 - val_loss: 0.4283 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3380 - accuracy: 0.8484 - val_loss: 0.4161 - val_accuracy: 0.8487\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3263 - accuracy: 0.8695 - val_loss: 0.4033 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3172 - accuracy: 0.8674 - val_loss: 0.4040 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3356 - accuracy: 0.8505 - val_loss: 0.4090 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.2947 - accuracy: 0.8779 - val_loss: 0.4195 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3110 - accuracy: 0.8632 - val_loss: 0.4288 - val_accuracy: 0.8655\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3361 - accuracy: 0.8653 - val_loss: 0.4226 - val_accuracy: 0.8487\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3082 - accuracy: 0.8779 - val_loss: 0.4290 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 116us/sample - loss: 0.3106 - accuracy: 0.8653 - val_loss: 0.4266 - val_accuracy: 0.8487\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3226 - accuracy: 0.8758 - val_loss: 0.4296 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3089 - accuracy: 0.8737 - val_loss: 0.4746 - val_accuracy: 0.8235\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3092 - accuracy: 0.8695 - val_loss: 0.4849 - val_accuracy: 0.8403\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3120 - accuracy: 0.8589 - val_loss: 0.4845 - val_accuracy: 0.8403\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3048 - accuracy: 0.8674 - val_loss: 0.4640 - val_accuracy: 0.8403\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3225 - accuracy: 0.8568 - val_loss: 0.4302 - val_accuracy: 0.8571\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3188 - accuracy: 0.8716 - val_loss: 0.4470 - val_accuracy: 0.8403\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3121 - accuracy: 0.8695 - val_loss: 0.4362 - val_accuracy: 0.8487\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.2987 - accuracy: 0.8821 - val_loss: 0.4422 - val_accuracy: 0.8487\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3115 - accuracy: 0.8589 - val_loss: 0.4311 - val_accuracy: 0.8487\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3010 - accuracy: 0.8905 - val_loss: 0.4829 - val_accuracy: 0.8235\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3048 - accuracy: 0.8695 - val_loss: 0.4716 - val_accuracy: 0.8319\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3054 - accuracy: 0.8800 - val_loss: 0.4425 - val_accuracy: 0.8487\n",
      "297/297 [==============================] - 0s 52us/sample - loss: 0.6491 - accuracy: 0.7811\n",
      "[CV]  optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu, total=   6.1s\n",
      "[CV] optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu ......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7256 - accuracy: 0.3642 - val_loss: 0.6937 - val_accuracy: 0.5882\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.6942 - accuracy: 0.4884 - val_loss: 0.6837 - val_accuracy: 0.7059\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.6819 - accuracy: 0.6526 - val_loss: 0.6768 - val_accuracy: 0.6975\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.6738 - accuracy: 0.6800 - val_loss: 0.6639 - val_accuracy: 0.7143\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6573 - accuracy: 0.7305 - val_loss: 0.6350 - val_accuracy: 0.7983\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.6246 - accuracy: 0.7621 - val_loss: 0.5755 - val_accuracy: 0.8319\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.5768 - accuracy: 0.7663 - val_loss: 0.4923 - val_accuracy: 0.8403\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.5355 - accuracy: 0.7832 - val_loss: 0.4245 - val_accuracy: 0.8403\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4974 - accuracy: 0.7979 - val_loss: 0.3892 - val_accuracy: 0.8571\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4761 - accuracy: 0.8063 - val_loss: 0.3868 - val_accuracy: 0.8319\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4491 - accuracy: 0.8295 - val_loss: 0.3755 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4856 - accuracy: 0.8147 - val_loss: 0.3708 - val_accuracy: 0.8655\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4624 - accuracy: 0.8084 - val_loss: 0.3680 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4494 - accuracy: 0.8105 - val_loss: 0.3658 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4665 - accuracy: 0.8253 - val_loss: 0.3657 - val_accuracy: 0.8824\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4373 - accuracy: 0.8189 - val_loss: 0.3638 - val_accuracy: 0.8571\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4366 - accuracy: 0.8189 - val_loss: 0.3652 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4204 - accuracy: 0.8379 - val_loss: 0.3583 - val_accuracy: 0.8739\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4320 - accuracy: 0.8211 - val_loss: 0.3586 - val_accuracy: 0.8655\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4237 - accuracy: 0.8316 - val_loss: 0.3612 - val_accuracy: 0.8739\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4354 - accuracy: 0.8168 - val_loss: 0.3598 - val_accuracy: 0.8571\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3957 - accuracy: 0.8337 - val_loss: 0.3608 - val_accuracy: 0.8655\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4061 - accuracy: 0.8442 - val_loss: 0.3590 - val_accuracy: 0.8571\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4156 - accuracy: 0.8316 - val_loss: 0.3603 - val_accuracy: 0.8655\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4070 - accuracy: 0.8295 - val_loss: 0.3582 - val_accuracy: 0.8487\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4010 - accuracy: 0.8337 - val_loss: 0.3593 - val_accuracy: 0.8487\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4127 - accuracy: 0.8316 - val_loss: 0.3548 - val_accuracy: 0.8655\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.3907 - accuracy: 0.8379 - val_loss: 0.3620 - val_accuracy: 0.8824\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3984 - accuracy: 0.8421 - val_loss: 0.3609 - val_accuracy: 0.8571\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4095 - accuracy: 0.8379 - val_loss: 0.3643 - val_accuracy: 0.8571\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3848 - accuracy: 0.8358 - val_loss: 0.3649 - val_accuracy: 0.8655\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3851 - accuracy: 0.8505 - val_loss: 0.3657 - val_accuracy: 0.8824\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3940 - accuracy: 0.8358 - val_loss: 0.3603 - val_accuracy: 0.8739\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3979 - accuracy: 0.8316 - val_loss: 0.3621 - val_accuracy: 0.8739\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4032 - accuracy: 0.8421 - val_loss: 0.3638 - val_accuracy: 0.8824\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3899 - accuracy: 0.8421 - val_loss: 0.3713 - val_accuracy: 0.8571\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3799 - accuracy: 0.8568 - val_loss: 0.3693 - val_accuracy: 0.8655\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3876 - accuracy: 0.8442 - val_loss: 0.3709 - val_accuracy: 0.8571\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3903 - accuracy: 0.8526 - val_loss: 0.3677 - val_accuracy: 0.8655\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3711 - accuracy: 0.8463 - val_loss: 0.3686 - val_accuracy: 0.8655\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3684 - accuracy: 0.8316 - val_loss: 0.3706 - val_accuracy: 0.8487\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3877 - accuracy: 0.8484 - val_loss: 0.3747 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 120us/sample - loss: 0.3929 - accuracy: 0.8484 - val_loss: 0.3756 - val_accuracy: 0.8655\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3939 - accuracy: 0.8505 - val_loss: 0.3770 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 136us/sample - loss: 0.3904 - accuracy: 0.8421 - val_loss: 0.3837 - val_accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 137us/sample - loss: 0.3797 - accuracy: 0.8505 - val_loss: 0.3863 - val_accuracy: 0.8487\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3777 - accuracy: 0.8337 - val_loss: 0.3792 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3730 - accuracy: 0.8421 - val_loss: 0.3841 - val_accuracy: 0.8487\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3779 - accuracy: 0.8463 - val_loss: 0.3922 - val_accuracy: 0.8739\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3822 - accuracy: 0.8484 - val_loss: 0.3880 - val_accuracy: 0.8739\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4120 - accuracy: 0.8463 - val_loss: 0.3942 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3733 - accuracy: 0.8400 - val_loss: 0.3962 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3689 - accuracy: 0.8484 - val_loss: 0.3888 - val_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3670 - accuracy: 0.8442 - val_loss: 0.3913 - val_accuracy: 0.8655\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3593 - accuracy: 0.8379 - val_loss: 0.3891 - val_accuracy: 0.8655\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3624 - accuracy: 0.8484 - val_loss: 0.3902 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3728 - accuracy: 0.8547 - val_loss: 0.3961 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3818 - accuracy: 0.8505 - val_loss: 0.4053 - val_accuracy: 0.8739\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3711 - accuracy: 0.8526 - val_loss: 0.4018 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3635 - accuracy: 0.8505 - val_loss: 0.4053 - val_accuracy: 0.8487\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.3614 - accuracy: 0.8484 - val_loss: 0.4098 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.3636 - accuracy: 0.8484 - val_loss: 0.4024 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3674 - accuracy: 0.8526 - val_loss: 0.4093 - val_accuracy: 0.8487\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3744 - accuracy: 0.8379 - val_loss: 0.4133 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3811 - accuracy: 0.8484 - val_loss: 0.4074 - val_accuracy: 0.8319\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3690 - accuracy: 0.8484 - val_loss: 0.4041 - val_accuracy: 0.8403\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3664 - accuracy: 0.8526 - val_loss: 0.4054 - val_accuracy: 0.8571\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3522 - accuracy: 0.8442 - val_loss: 0.4108 - val_accuracy: 0.8319\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3695 - accuracy: 0.8505 - val_loss: 0.4066 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3571 - accuracy: 0.8547 - val_loss: 0.4171 - val_accuracy: 0.8487\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3636 - accuracy: 0.8442 - val_loss: 0.4139 - val_accuracy: 0.8739\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 113us/sample - loss: 0.3496 - accuracy: 0.8568 - val_loss: 0.4136 - val_accuracy: 0.8487\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3460 - accuracy: 0.8695 - val_loss: 0.4187 - val_accuracy: 0.8487\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3453 - accuracy: 0.8526 - val_loss: 0.4260 - val_accuracy: 0.8487\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3448 - accuracy: 0.8526 - val_loss: 0.4209 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3650 - accuracy: 0.8526 - val_loss: 0.4257 - val_accuracy: 0.8487\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3577 - accuracy: 0.8442 - val_loss: 0.4215 - val_accuracy: 0.8487\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3575 - accuracy: 0.8421 - val_loss: 0.4215 - val_accuracy: 0.8403\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3624 - accuracy: 0.8463 - val_loss: 0.4148 - val_accuracy: 0.8487\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3587 - accuracy: 0.8484 - val_loss: 0.4176 - val_accuracy: 0.8487\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3590 - accuracy: 0.8568 - val_loss: 0.4215 - val_accuracy: 0.8487\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3448 - accuracy: 0.8632 - val_loss: 0.4255 - val_accuracy: 0.8403\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3461 - accuracy: 0.8463 - val_loss: 0.4299 - val_accuracy: 0.8235\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3352 - accuracy: 0.8632 - val_loss: 0.4346 - val_accuracy: 0.8319\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3558 - accuracy: 0.8484 - val_loss: 0.4383 - val_accuracy: 0.8319\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3415 - accuracy: 0.8547 - val_loss: 0.4411 - val_accuracy: 0.8403\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3524 - accuracy: 0.8589 - val_loss: 0.4510 - val_accuracy: 0.8319\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3469 - accuracy: 0.8568 - val_loss: 0.4590 - val_accuracy: 0.8403\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3284 - accuracy: 0.8526 - val_loss: 0.4668 - val_accuracy: 0.8319\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3514 - accuracy: 0.8442 - val_loss: 0.4619 - val_accuracy: 0.8319\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3417 - accuracy: 0.8484 - val_loss: 0.4571 - val_accuracy: 0.8235\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.3271 - accuracy: 0.8568 - val_loss: 0.4649 - val_accuracy: 0.8235\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3448 - accuracy: 0.8547 - val_loss: 0.4739 - val_accuracy: 0.8319\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3548 - accuracy: 0.8611 - val_loss: 0.4716 - val_accuracy: 0.8319\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3481 - accuracy: 0.8526 - val_loss: 0.4805 - val_accuracy: 0.8403\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3477 - accuracy: 0.8526 - val_loss: 0.4765 - val_accuracy: 0.8403\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3549 - accuracy: 0.8611 - val_loss: 0.4778 - val_accuracy: 0.8235\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3524 - accuracy: 0.8547 - val_loss: 0.4831 - val_accuracy: 0.8319\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3345 - accuracy: 0.8674 - val_loss: 0.4806 - val_accuracy: 0.8403\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3546 - accuracy: 0.8653 - val_loss: 0.4901 - val_accuracy: 0.8319\n",
      "297/297 [==============================] - 0s 55us/sample - loss: 0.5248 - accuracy: 0.8316\n",
      "[CV]  optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu, total=   6.2s\n",
      "[CV] optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu ......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.6790 - accuracy: 0.5684 - val_loss: 0.6425 - val_accuracy: 0.7143\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.6365 - accuracy: 0.6316 - val_loss: 0.6022 - val_accuracy: 0.6723\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5995 - accuracy: 0.6737 - val_loss: 0.5729 - val_accuracy: 0.7395\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.5603 - accuracy: 0.7116 - val_loss: 0.5398 - val_accuracy: 0.7983\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.5511 - accuracy: 0.7284 - val_loss: 0.5169 - val_accuracy: 0.8067\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5172 - accuracy: 0.7663 - val_loss: 0.5021 - val_accuracy: 0.7815\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4962 - accuracy: 0.8063 - val_loss: 0.4901 - val_accuracy: 0.7899\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4895 - accuracy: 0.8126 - val_loss: 0.4822 - val_accuracy: 0.7899\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4870 - accuracy: 0.7937 - val_loss: 0.4716 - val_accuracy: 0.7983\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4959 - accuracy: 0.8147 - val_loss: 0.4646 - val_accuracy: 0.7899\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4948 - accuracy: 0.8021 - val_loss: 0.4630 - val_accuracy: 0.7899\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.4633 - accuracy: 0.8105 - val_loss: 0.4573 - val_accuracy: 0.7983\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4535 - accuracy: 0.8274 - val_loss: 0.4576 - val_accuracy: 0.8151\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4222 - accuracy: 0.8295 - val_loss: 0.4518 - val_accuracy: 0.8067\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4339 - accuracy: 0.8253 - val_loss: 0.4561 - val_accuracy: 0.8067\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4274 - accuracy: 0.8211 - val_loss: 0.4583 - val_accuracy: 0.8067\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4454 - accuracy: 0.8316 - val_loss: 0.4570 - val_accuracy: 0.8067\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4255 - accuracy: 0.8253 - val_loss: 0.4569 - val_accuracy: 0.8067\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4280 - accuracy: 0.8463 - val_loss: 0.4598 - val_accuracy: 0.8067\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.4009 - accuracy: 0.8379 - val_loss: 0.4619 - val_accuracy: 0.8151\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4234 - accuracy: 0.8400 - val_loss: 0.4617 - val_accuracy: 0.8235\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.4228 - accuracy: 0.8295 - val_loss: 0.4633 - val_accuracy: 0.8151\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3912 - accuracy: 0.8505 - val_loss: 0.4643 - val_accuracy: 0.8067\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.4182 - accuracy: 0.8358 - val_loss: 0.4649 - val_accuracy: 0.8151\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4090 - accuracy: 0.8316 - val_loss: 0.4677 - val_accuracy: 0.8151\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4032 - accuracy: 0.8421 - val_loss: 0.4703 - val_accuracy: 0.8151\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3957 - accuracy: 0.8484 - val_loss: 0.4795 - val_accuracy: 0.8151\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4040 - accuracy: 0.8421 - val_loss: 0.4791 - val_accuracy: 0.8067\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.4074 - accuracy: 0.8442 - val_loss: 0.4787 - val_accuracy: 0.8067\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.4030 - accuracy: 0.8463 - val_loss: 0.4824 - val_accuracy: 0.8067\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3877 - accuracy: 0.8505 - val_loss: 0.4763 - val_accuracy: 0.8067\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.4120 - accuracy: 0.8547 - val_loss: 0.4786 - val_accuracy: 0.8067\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3883 - accuracy: 0.8484 - val_loss: 0.4845 - val_accuracy: 0.8067\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3872 - accuracy: 0.8526 - val_loss: 0.4872 - val_accuracy: 0.8067\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3890 - accuracy: 0.8421 - val_loss: 0.4891 - val_accuracy: 0.8067\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4031 - accuracy: 0.8400 - val_loss: 0.4904 - val_accuracy: 0.7983\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3939 - accuracy: 0.8421 - val_loss: 0.4866 - val_accuracy: 0.7983\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.3878 - accuracy: 0.8442 - val_loss: 0.4930 - val_accuracy: 0.7983\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3915 - accuracy: 0.8547 - val_loss: 0.4973 - val_accuracy: 0.8067\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3503 - accuracy: 0.8589 - val_loss: 0.5089 - val_accuracy: 0.7983\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4023 - accuracy: 0.8442 - val_loss: 0.5022 - val_accuracy: 0.7983\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3706 - accuracy: 0.8484 - val_loss: 0.5030 - val_accuracy: 0.8067\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3779 - accuracy: 0.8526 - val_loss: 0.5066 - val_accuracy: 0.7983\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3709 - accuracy: 0.8589 - val_loss: 0.5120 - val_accuracy: 0.7983\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3484 - accuracy: 0.8589 - val_loss: 0.5226 - val_accuracy: 0.7899\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3770 - accuracy: 0.8484 - val_loss: 0.5210 - val_accuracy: 0.7899\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3776 - accuracy: 0.8589 - val_loss: 0.5282 - val_accuracy: 0.7899\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3526 - accuracy: 0.8589 - val_loss: 0.5268 - val_accuracy: 0.8067\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3640 - accuracy: 0.8589 - val_loss: 0.5317 - val_accuracy: 0.7899\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3691 - accuracy: 0.8716 - val_loss: 0.5280 - val_accuracy: 0.8151\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3789 - accuracy: 0.8674 - val_loss: 0.5288 - val_accuracy: 0.7899\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3521 - accuracy: 0.8653 - val_loss: 0.5254 - val_accuracy: 0.7899\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3778 - accuracy: 0.8463 - val_loss: 0.5313 - val_accuracy: 0.7815\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3653 - accuracy: 0.8526 - val_loss: 0.5399 - val_accuracy: 0.7899\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3540 - accuracy: 0.8526 - val_loss: 0.5385 - val_accuracy: 0.7899\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3837 - accuracy: 0.8484 - val_loss: 0.5341 - val_accuracy: 0.7815\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3663 - accuracy: 0.8611 - val_loss: 0.5442 - val_accuracy: 0.7815\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3678 - accuracy: 0.8589 - val_loss: 0.5462 - val_accuracy: 0.8067\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3543 - accuracy: 0.8568 - val_loss: 0.5527 - val_accuracy: 0.7983\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3670 - accuracy: 0.8611 - val_loss: 0.5605 - val_accuracy: 0.7983\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 111us/sample - loss: 0.3486 - accuracy: 0.8674 - val_loss: 0.5530 - val_accuracy: 0.7983\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3316 - accuracy: 0.8695 - val_loss: 0.5714 - val_accuracy: 0.7815\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 137us/sample - loss: 0.3843 - accuracy: 0.8547 - val_loss: 0.5470 - val_accuracy: 0.7815\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 148us/sample - loss: 0.3487 - accuracy: 0.8547 - val_loss: 0.5531 - val_accuracy: 0.7983\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 115us/sample - loss: 0.3579 - accuracy: 0.8674 - val_loss: 0.5566 - val_accuracy: 0.7983\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 112us/sample - loss: 0.3492 - accuracy: 0.8653 - val_loss: 0.5741 - val_accuracy: 0.7815\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 129us/sample - loss: 0.3680 - accuracy: 0.8484 - val_loss: 0.5706 - val_accuracy: 0.7815\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3729 - accuracy: 0.8547 - val_loss: 0.5607 - val_accuracy: 0.7815\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3456 - accuracy: 0.8674 - val_loss: 0.5790 - val_accuracy: 0.7815\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 107us/sample - loss: 0.3433 - accuracy: 0.8653 - val_loss: 0.5859 - val_accuracy: 0.8067\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3630 - accuracy: 0.8505 - val_loss: 0.5875 - val_accuracy: 0.7815\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.3414 - accuracy: 0.8779 - val_loss: 0.5806 - val_accuracy: 0.7815\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3446 - accuracy: 0.8611 - val_loss: 0.6002 - val_accuracy: 0.7899\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3379 - accuracy: 0.8611 - val_loss: 0.6039 - val_accuracy: 0.7899\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3511 - accuracy: 0.8589 - val_loss: 0.5937 - val_accuracy: 0.7815\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3620 - accuracy: 0.8421 - val_loss: 0.5931 - val_accuracy: 0.7815\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3493 - accuracy: 0.8695 - val_loss: 0.6010 - val_accuracy: 0.7815\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3586 - accuracy: 0.8611 - val_loss: 0.5995 - val_accuracy: 0.7815\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3381 - accuracy: 0.8632 - val_loss: 0.5914 - val_accuracy: 0.7815\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3535 - accuracy: 0.8653 - val_loss: 0.5821 - val_accuracy: 0.8067\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3531 - accuracy: 0.8589 - val_loss: 0.5880 - val_accuracy: 0.8067\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3393 - accuracy: 0.8526 - val_loss: 0.5881 - val_accuracy: 0.7983\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3665 - accuracy: 0.8589 - val_loss: 0.5888 - val_accuracy: 0.7815\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3642 - accuracy: 0.8589 - val_loss: 0.5789 - val_accuracy: 0.7983\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3470 - accuracy: 0.8674 - val_loss: 0.5847 - val_accuracy: 0.7983\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3547 - accuracy: 0.8611 - val_loss: 0.5882 - val_accuracy: 0.7983\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3522 - accuracy: 0.8674 - val_loss: 0.5953 - val_accuracy: 0.7983\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3351 - accuracy: 0.8695 - val_loss: 0.6223 - val_accuracy: 0.7815\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3374 - accuracy: 0.8674 - val_loss: 0.6151 - val_accuracy: 0.7983\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.3428 - accuracy: 0.8589 - val_loss: 0.6094 - val_accuracy: 0.7983\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3349 - accuracy: 0.8611 - val_loss: 0.6158 - val_accuracy: 0.7983\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3330 - accuracy: 0.8695 - val_loss: 0.6182 - val_accuracy: 0.7983\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3274 - accuracy: 0.8695 - val_loss: 0.6045 - val_accuracy: 0.7983\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3307 - accuracy: 0.8653 - val_loss: 0.6055 - val_accuracy: 0.7983\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3336 - accuracy: 0.8632 - val_loss: 0.6261 - val_accuracy: 0.7983\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.3159 - accuracy: 0.8695 - val_loss: 0.6421 - val_accuracy: 0.7983\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3229 - accuracy: 0.8653 - val_loss: 0.6580 - val_accuracy: 0.7983\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.3249 - accuracy: 0.8779 - val_loss: 0.6459 - val_accuracy: 0.7983\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.3315 - accuracy: 0.8674 - val_loss: 0.6418 - val_accuracy: 0.7815\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 123us/sample - loss: 0.3216 - accuracy: 0.8611 - val_loss: 0.6442 - val_accuracy: 0.7983\n",
      "297/297 [==============================] - 0s 53us/sample - loss: 0.4560 - accuracy: 0.8350\n",
      "[CV]  optimizer=Adam, n_neurons=100, n_hidden=4, activation=relu, total=   6.3s\n",
      "[CV] optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu ........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.7246 - accuracy: 0.4505 - val_loss: 0.6743 - val_accuracy: 0.6303\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6879 - accuracy: 0.5495 - val_loss: 0.6349 - val_accuracy: 0.7311\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6567 - accuracy: 0.6274 - val_loss: 0.6065 - val_accuracy: 0.7059\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6392 - accuracy: 0.6358 - val_loss: 0.5855 - val_accuracy: 0.7143\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6186 - accuracy: 0.6695 - val_loss: 0.5696 - val_accuracy: 0.7143\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6211 - accuracy: 0.6295 - val_loss: 0.5564 - val_accuracy: 0.7143\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6001 - accuracy: 0.6674 - val_loss: 0.5461 - val_accuracy: 0.7143\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5994 - accuracy: 0.6716 - val_loss: 0.5376 - val_accuracy: 0.7143\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5813 - accuracy: 0.6905 - val_loss: 0.5291 - val_accuracy: 0.7227\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5879 - accuracy: 0.6758 - val_loss: 0.5223 - val_accuracy: 0.7143\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5716 - accuracy: 0.7053 - val_loss: 0.5157 - val_accuracy: 0.7311\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5748 - accuracy: 0.6905 - val_loss: 0.5091 - val_accuracy: 0.7395\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5541 - accuracy: 0.7411 - val_loss: 0.5026 - val_accuracy: 0.7479\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5634 - accuracy: 0.7221 - val_loss: 0.4980 - val_accuracy: 0.7563\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5447 - accuracy: 0.7179 - val_loss: 0.4925 - val_accuracy: 0.7815\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5420 - accuracy: 0.7411 - val_loss: 0.4876 - val_accuracy: 0.8067\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5374 - accuracy: 0.7432 - val_loss: 0.4829 - val_accuracy: 0.8067\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5218 - accuracy: 0.7579 - val_loss: 0.4768 - val_accuracy: 0.8067\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5298 - accuracy: 0.7558 - val_loss: 0.4726 - val_accuracy: 0.8319\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5127 - accuracy: 0.7663 - val_loss: 0.4674 - val_accuracy: 0.8319\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5111 - accuracy: 0.7663 - val_loss: 0.4629 - val_accuracy: 0.8319\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5079 - accuracy: 0.7579 - val_loss: 0.4598 - val_accuracy: 0.8235\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5166 - accuracy: 0.7621 - val_loss: 0.4556 - val_accuracy: 0.8235\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5076 - accuracy: 0.7705 - val_loss: 0.4522 - val_accuracy: 0.8235\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5226 - accuracy: 0.7642 - val_loss: 0.4490 - val_accuracy: 0.8151\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5254 - accuracy: 0.7495 - val_loss: 0.4469 - val_accuracy: 0.8151\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4915 - accuracy: 0.7747 - val_loss: 0.4434 - val_accuracy: 0.8235\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.4856 - accuracy: 0.8084 - val_loss: 0.4406 - val_accuracy: 0.8235\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5053 - accuracy: 0.7811 - val_loss: 0.4378 - val_accuracy: 0.8319\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4929 - accuracy: 0.7811 - val_loss: 0.4357 - val_accuracy: 0.8319\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4982 - accuracy: 0.7768 - val_loss: 0.4326 - val_accuracy: 0.8319\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4893 - accuracy: 0.7705 - val_loss: 0.4301 - val_accuracy: 0.8403\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4981 - accuracy: 0.7747 - val_loss: 0.4281 - val_accuracy: 0.8319\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4889 - accuracy: 0.7789 - val_loss: 0.4256 - val_accuracy: 0.8319\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4961 - accuracy: 0.7874 - val_loss: 0.4232 - val_accuracy: 0.8403\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4867 - accuracy: 0.8042 - val_loss: 0.4210 - val_accuracy: 0.8487\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4785 - accuracy: 0.7895 - val_loss: 0.4195 - val_accuracy: 0.8487\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4738 - accuracy: 0.7958 - val_loss: 0.4167 - val_accuracy: 0.8487\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4860 - accuracy: 0.7811 - val_loss: 0.4150 - val_accuracy: 0.8487\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4811 - accuracy: 0.7705 - val_loss: 0.4121 - val_accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4732 - accuracy: 0.7916 - val_loss: 0.4105 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4564 - accuracy: 0.8063 - val_loss: 0.4078 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4795 - accuracy: 0.7832 - val_loss: 0.4057 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4542 - accuracy: 0.8084 - val_loss: 0.4040 - val_accuracy: 0.8487\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4644 - accuracy: 0.8021 - val_loss: 0.4019 - val_accuracy: 0.8487\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4892 - accuracy: 0.7874 - val_loss: 0.4011 - val_accuracy: 0.8487\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4579 - accuracy: 0.7979 - val_loss: 0.3988 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4546 - accuracy: 0.8063 - val_loss: 0.3973 - val_accuracy: 0.8487\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4674 - accuracy: 0.8147 - val_loss: 0.3964 - val_accuracy: 0.8487\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4522 - accuracy: 0.8000 - val_loss: 0.3945 - val_accuracy: 0.8487\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4679 - accuracy: 0.8042 - val_loss: 0.3943 - val_accuracy: 0.8487\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4715 - accuracy: 0.8042 - val_loss: 0.3932 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4690 - accuracy: 0.8000 - val_loss: 0.3911 - val_accuracy: 0.8571\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4585 - accuracy: 0.8105 - val_loss: 0.3903 - val_accuracy: 0.8571\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4786 - accuracy: 0.7979 - val_loss: 0.3891 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4643 - accuracy: 0.7937 - val_loss: 0.3876 - val_accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4491 - accuracy: 0.8084 - val_loss: 0.3859 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4454 - accuracy: 0.7979 - val_loss: 0.3842 - val_accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4627 - accuracy: 0.8000 - val_loss: 0.3827 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4645 - accuracy: 0.8000 - val_loss: 0.3823 - val_accuracy: 0.8571\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4391 - accuracy: 0.8063 - val_loss: 0.3810 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4414 - accuracy: 0.8084 - val_loss: 0.3802 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4611 - accuracy: 0.8189 - val_loss: 0.3797 - val_accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4676 - accuracy: 0.8000 - val_loss: 0.3790 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4450 - accuracy: 0.8189 - val_loss: 0.3779 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4496 - accuracy: 0.8189 - val_loss: 0.3779 - val_accuracy: 0.8655\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4439 - accuracy: 0.7958 - val_loss: 0.3772 - val_accuracy: 0.8655\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4522 - accuracy: 0.8021 - val_loss: 0.3761 - val_accuracy: 0.8655\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4508 - accuracy: 0.8147 - val_loss: 0.3750 - val_accuracy: 0.8655\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4464 - accuracy: 0.8295 - val_loss: 0.3746 - val_accuracy: 0.8655\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4534 - accuracy: 0.8084 - val_loss: 0.3737 - val_accuracy: 0.8655\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4375 - accuracy: 0.8105 - val_loss: 0.3732 - val_accuracy: 0.8655\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4624 - accuracy: 0.8042 - val_loss: 0.3729 - val_accuracy: 0.8655\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4435 - accuracy: 0.8232 - val_loss: 0.3726 - val_accuracy: 0.8739\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4536 - accuracy: 0.8042 - val_loss: 0.3714 - val_accuracy: 0.8655\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4462 - accuracy: 0.8105 - val_loss: 0.3712 - val_accuracy: 0.8739\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4277 - accuracy: 0.8232 - val_loss: 0.3696 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4464 - accuracy: 0.8021 - val_loss: 0.3694 - val_accuracy: 0.8655\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4430 - accuracy: 0.8126 - val_loss: 0.3681 - val_accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4269 - accuracy: 0.8274 - val_loss: 0.3667 - val_accuracy: 0.8655\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4255 - accuracy: 0.8253 - val_loss: 0.3662 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4319 - accuracy: 0.8063 - val_loss: 0.3656 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4290 - accuracy: 0.8253 - val_loss: 0.3644 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4301 - accuracy: 0.8274 - val_loss: 0.3641 - val_accuracy: 0.8655\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4328 - accuracy: 0.8105 - val_loss: 0.3633 - val_accuracy: 0.8655\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4277 - accuracy: 0.8211 - val_loss: 0.3636 - val_accuracy: 0.8655\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4534 - accuracy: 0.8042 - val_loss: 0.3638 - val_accuracy: 0.8655\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4417 - accuracy: 0.8084 - val_loss: 0.3628 - val_accuracy: 0.8655\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4435 - accuracy: 0.8021 - val_loss: 0.3630 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4536 - accuracy: 0.7958 - val_loss: 0.3617 - val_accuracy: 0.8655\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4511 - accuracy: 0.8168 - val_loss: 0.3612 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4483 - accuracy: 0.8253 - val_loss: 0.3613 - val_accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4315 - accuracy: 0.8316 - val_loss: 0.3623 - val_accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4460 - accuracy: 0.8063 - val_loss: 0.3622 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4462 - accuracy: 0.7979 - val_loss: 0.3626 - val_accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4383 - accuracy: 0.8084 - val_loss: 0.3631 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4395 - accuracy: 0.8168 - val_loss: 0.3615 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4315 - accuracy: 0.8105 - val_loss: 0.3603 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4357 - accuracy: 0.8063 - val_loss: 0.3597 - val_accuracy: 0.8655\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4405 - accuracy: 0.8232 - val_loss: 0.3602 - val_accuracy: 0.8655\n",
      "297/297 [==============================] - 0s 50us/sample - loss: 0.4726 - accuracy: 0.8013\n",
      "[CV]  optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu, total=   5.0s\n",
      "[CV] optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu ........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.7228 - accuracy: 0.4989 - val_loss: 0.6971 - val_accuracy: 0.5966\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.7130 - accuracy: 0.5411 - val_loss: 0.6900 - val_accuracy: 0.6218\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.7054 - accuracy: 0.5663 - val_loss: 0.6833 - val_accuracy: 0.6303\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6964 - accuracy: 0.5958 - val_loss: 0.6779 - val_accuracy: 0.6471\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6974 - accuracy: 0.5937 - val_loss: 0.6731 - val_accuracy: 0.6555\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.6756 - accuracy: 0.6379 - val_loss: 0.6689 - val_accuracy: 0.6387\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6909 - accuracy: 0.6063 - val_loss: 0.6649 - val_accuracy: 0.6387\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6793 - accuracy: 0.6147 - val_loss: 0.6614 - val_accuracy: 0.6471\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6746 - accuracy: 0.6295 - val_loss: 0.6578 - val_accuracy: 0.6471\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6682 - accuracy: 0.6379 - val_loss: 0.6543 - val_accuracy: 0.6471\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6667 - accuracy: 0.6316 - val_loss: 0.6511 - val_accuracy: 0.6471\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6637 - accuracy: 0.6295 - val_loss: 0.6481 - val_accuracy: 0.6471\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6603 - accuracy: 0.6358 - val_loss: 0.6453 - val_accuracy: 0.6471\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6650 - accuracy: 0.6337 - val_loss: 0.6427 - val_accuracy: 0.6471\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6634 - accuracy: 0.6211 - val_loss: 0.6401 - val_accuracy: 0.6471\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6492 - accuracy: 0.6400 - val_loss: 0.6372 - val_accuracy: 0.6471\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6485 - accuracy: 0.6463 - val_loss: 0.6345 - val_accuracy: 0.6471\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6399 - accuracy: 0.6505 - val_loss: 0.6314 - val_accuracy: 0.6471\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6427 - accuracy: 0.6547 - val_loss: 0.6285 - val_accuracy: 0.6471\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.6504 - accuracy: 0.6421 - val_loss: 0.6259 - val_accuracy: 0.6471\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6462 - accuracy: 0.6400 - val_loss: 0.6236 - val_accuracy: 0.6471\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.6283 - accuracy: 0.6589 - val_loss: 0.6208 - val_accuracy: 0.6471\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6382 - accuracy: 0.6505 - val_loss: 0.6184 - val_accuracy: 0.6555\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6368 - accuracy: 0.6547 - val_loss: 0.6158 - val_accuracy: 0.6555\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6272 - accuracy: 0.6589 - val_loss: 0.6128 - val_accuracy: 0.6555\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6188 - accuracy: 0.6674 - val_loss: 0.6090 - val_accuracy: 0.6555\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6148 - accuracy: 0.6653 - val_loss: 0.6053 - val_accuracy: 0.6639\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6236 - accuracy: 0.6568 - val_loss: 0.6023 - val_accuracy: 0.6723\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6212 - accuracy: 0.6716 - val_loss: 0.5988 - val_accuracy: 0.6723\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6176 - accuracy: 0.6505 - val_loss: 0.5952 - val_accuracy: 0.6723\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6219 - accuracy: 0.6526 - val_loss: 0.5916 - val_accuracy: 0.6807\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6134 - accuracy: 0.6568 - val_loss: 0.5876 - val_accuracy: 0.6891\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6118 - accuracy: 0.6611 - val_loss: 0.5836 - val_accuracy: 0.6975\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6090 - accuracy: 0.6632 - val_loss: 0.5803 - val_accuracy: 0.6975\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6029 - accuracy: 0.6674 - val_loss: 0.5765 - val_accuracy: 0.6975\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5959 - accuracy: 0.6653 - val_loss: 0.5721 - val_accuracy: 0.6975\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5971 - accuracy: 0.6611 - val_loss: 0.5687 - val_accuracy: 0.6975\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5957 - accuracy: 0.6695 - val_loss: 0.5648 - val_accuracy: 0.6975\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5897 - accuracy: 0.6842 - val_loss: 0.5606 - val_accuracy: 0.7059\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5977 - accuracy: 0.6800 - val_loss: 0.5569 - val_accuracy: 0.7143\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5562 - accuracy: 0.7095 - val_loss: 0.5517 - val_accuracy: 0.7311\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5814 - accuracy: 0.6905 - val_loss: 0.5477 - val_accuracy: 0.7311\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5829 - accuracy: 0.6842 - val_loss: 0.5441 - val_accuracy: 0.7311\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5735 - accuracy: 0.6863 - val_loss: 0.5400 - val_accuracy: 0.7395\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5705 - accuracy: 0.7053 - val_loss: 0.5361 - val_accuracy: 0.7479\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5580 - accuracy: 0.6863 - val_loss: 0.5318 - val_accuracy: 0.7479\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5646 - accuracy: 0.6905 - val_loss: 0.5280 - val_accuracy: 0.7647\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5411 - accuracy: 0.7200 - val_loss: 0.5233 - val_accuracy: 0.7647\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5624 - accuracy: 0.7116 - val_loss: 0.5197 - val_accuracy: 0.7647\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5560 - accuracy: 0.7095 - val_loss: 0.5160 - val_accuracy: 0.7647\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5548 - accuracy: 0.7200 - val_loss: 0.5124 - val_accuracy: 0.7731\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5415 - accuracy: 0.7326 - val_loss: 0.5085 - val_accuracy: 0.7815\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5459 - accuracy: 0.7242 - val_loss: 0.5051 - val_accuracy: 0.7815\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5490 - accuracy: 0.7221 - val_loss: 0.5020 - val_accuracy: 0.7815\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5494 - accuracy: 0.7411 - val_loss: 0.4988 - val_accuracy: 0.7983\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5491 - accuracy: 0.7242 - val_loss: 0.4958 - val_accuracy: 0.7983\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5244 - accuracy: 0.7389 - val_loss: 0.4920 - val_accuracy: 0.7983\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5352 - accuracy: 0.7326 - val_loss: 0.4889 - val_accuracy: 0.8151\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5305 - accuracy: 0.7305 - val_loss: 0.4853 - val_accuracy: 0.8067\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5487 - accuracy: 0.7284 - val_loss: 0.4830 - val_accuracy: 0.8235\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5354 - accuracy: 0.7221 - val_loss: 0.4804 - val_accuracy: 0.8235\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5214 - accuracy: 0.7579 - val_loss: 0.4771 - val_accuracy: 0.8319\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5271 - accuracy: 0.7474 - val_loss: 0.4746 - val_accuracy: 0.8319\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5215 - accuracy: 0.7495 - val_loss: 0.4719 - val_accuracy: 0.8319\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5206 - accuracy: 0.7642 - val_loss: 0.4694 - val_accuracy: 0.8319\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5212 - accuracy: 0.7474 - val_loss: 0.4671 - val_accuracy: 0.8319\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5192 - accuracy: 0.7537 - val_loss: 0.4644 - val_accuracy: 0.8235\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5136 - accuracy: 0.7579 - val_loss: 0.4623 - val_accuracy: 0.8235\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5119 - accuracy: 0.7516 - val_loss: 0.4601 - val_accuracy: 0.8235\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5192 - accuracy: 0.7558 - val_loss: 0.4583 - val_accuracy: 0.8235\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5001 - accuracy: 0.7705 - val_loss: 0.4560 - val_accuracy: 0.8319\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5183 - accuracy: 0.7495 - val_loss: 0.4539 - val_accuracy: 0.8319\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5131 - accuracy: 0.7663 - val_loss: 0.4517 - val_accuracy: 0.8319\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4840 - accuracy: 0.7895 - val_loss: 0.4493 - val_accuracy: 0.8403\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4930 - accuracy: 0.7958 - val_loss: 0.4472 - val_accuracy: 0.8319\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5104 - accuracy: 0.7705 - val_loss: 0.4454 - val_accuracy: 0.8319\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5052 - accuracy: 0.7621 - val_loss: 0.4431 - val_accuracy: 0.8319\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5019 - accuracy: 0.7747 - val_loss: 0.4412 - val_accuracy: 0.8319\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5184 - accuracy: 0.7663 - val_loss: 0.4399 - val_accuracy: 0.8319\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5170 - accuracy: 0.7916 - val_loss: 0.4388 - val_accuracy: 0.8403\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4801 - accuracy: 0.8021 - val_loss: 0.4365 - val_accuracy: 0.8403\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4913 - accuracy: 0.7811 - val_loss: 0.4348 - val_accuracy: 0.8403\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4871 - accuracy: 0.7853 - val_loss: 0.4333 - val_accuracy: 0.8403\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5052 - accuracy: 0.7916 - val_loss: 0.4314 - val_accuracy: 0.8487\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5108 - accuracy: 0.7726 - val_loss: 0.4304 - val_accuracy: 0.8487\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5093 - accuracy: 0.7811 - val_loss: 0.4291 - val_accuracy: 0.8487\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4836 - accuracy: 0.7705 - val_loss: 0.4275 - val_accuracy: 0.8487\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5024 - accuracy: 0.7895 - val_loss: 0.4264 - val_accuracy: 0.8487\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5048 - accuracy: 0.7642 - val_loss: 0.4256 - val_accuracy: 0.8487\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4922 - accuracy: 0.7895 - val_loss: 0.4244 - val_accuracy: 0.8487\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4807 - accuracy: 0.7958 - val_loss: 0.4224 - val_accuracy: 0.8487\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4930 - accuracy: 0.7768 - val_loss: 0.4210 - val_accuracy: 0.8487\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4643 - accuracy: 0.8021 - val_loss: 0.4191 - val_accuracy: 0.8487\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4841 - accuracy: 0.7874 - val_loss: 0.4179 - val_accuracy: 0.8487\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5038 - accuracy: 0.7747 - val_loss: 0.4172 - val_accuracy: 0.8487\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4983 - accuracy: 0.7832 - val_loss: 0.4169 - val_accuracy: 0.8571\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4760 - accuracy: 0.7768 - val_loss: 0.4156 - val_accuracy: 0.8571\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4810 - accuracy: 0.7789 - val_loss: 0.4143 - val_accuracy: 0.8571\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.4733 - accuracy: 0.8126 - val_loss: 0.4130 - val_accuracy: 0.8487\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4741 - accuracy: 0.7916 - val_loss: 0.4119 - val_accuracy: 0.8487\n",
      "297/297 [==============================] - 0s 51us/sample - loss: 0.4821 - accuracy: 0.7946\n",
      "[CV]  optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu, total=   5.0s\n",
      "[CV] optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu ........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.6826 - accuracy: 0.5453 - val_loss: 0.6707 - val_accuracy: 0.6303\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.6686 - accuracy: 0.6295 - val_loss: 0.6598 - val_accuracy: 0.7143\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6645 - accuracy: 0.6211 - val_loss: 0.6512 - val_accuracy: 0.6807\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6419 - accuracy: 0.6800 - val_loss: 0.6428 - val_accuracy: 0.6723\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6449 - accuracy: 0.6505 - val_loss: 0.6363 - val_accuracy: 0.6891\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.6416 - accuracy: 0.6547 - val_loss: 0.6300 - val_accuracy: 0.6891\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6359 - accuracy: 0.6632 - val_loss: 0.6244 - val_accuracy: 0.6891\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6228 - accuracy: 0.6758 - val_loss: 0.6191 - val_accuracy: 0.6891\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6257 - accuracy: 0.6484 - val_loss: 0.6141 - val_accuracy: 0.6975\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6133 - accuracy: 0.7179 - val_loss: 0.6098 - val_accuracy: 0.6975\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6017 - accuracy: 0.6968 - val_loss: 0.6056 - val_accuracy: 0.6975\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.6067 - accuracy: 0.6926 - val_loss: 0.6017 - val_accuracy: 0.6975\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6111 - accuracy: 0.6905 - val_loss: 0.5981 - val_accuracy: 0.6891\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.6004 - accuracy: 0.6842 - val_loss: 0.5947 - val_accuracy: 0.6891\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5826 - accuracy: 0.7368 - val_loss: 0.5911 - val_accuracy: 0.6975\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5790 - accuracy: 0.7158 - val_loss: 0.5869 - val_accuracy: 0.7059\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5909 - accuracy: 0.6926 - val_loss: 0.5839 - val_accuracy: 0.7143\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5691 - accuracy: 0.7074 - val_loss: 0.5805 - val_accuracy: 0.7143\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5741 - accuracy: 0.7305 - val_loss: 0.5774 - val_accuracy: 0.7227\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5744 - accuracy: 0.7095 - val_loss: 0.5745 - val_accuracy: 0.7311\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5710 - accuracy: 0.7032 - val_loss: 0.5722 - val_accuracy: 0.7311\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5617 - accuracy: 0.7284 - val_loss: 0.5696 - val_accuracy: 0.7311\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5524 - accuracy: 0.7411 - val_loss: 0.5670 - val_accuracy: 0.7395\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5444 - accuracy: 0.7537 - val_loss: 0.5643 - val_accuracy: 0.7479\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5561 - accuracy: 0.7347 - val_loss: 0.5618 - val_accuracy: 0.7647\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5651 - accuracy: 0.7074 - val_loss: 0.5596 - val_accuracy: 0.7647\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5414 - accuracy: 0.7642 - val_loss: 0.5574 - val_accuracy: 0.7647\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5649 - accuracy: 0.7411 - val_loss: 0.5554 - val_accuracy: 0.7647\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5402 - accuracy: 0.7495 - val_loss: 0.5533 - val_accuracy: 0.7563\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5336 - accuracy: 0.7326 - val_loss: 0.5515 - val_accuracy: 0.7563\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5605 - accuracy: 0.7326 - val_loss: 0.5500 - val_accuracy: 0.7563\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5415 - accuracy: 0.7453 - val_loss: 0.5484 - val_accuracy: 0.7563\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 180us/sample - loss: 0.5368 - accuracy: 0.7621 - val_loss: 0.5468 - val_accuracy: 0.7563\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 134us/sample - loss: 0.5413 - accuracy: 0.7516 - val_loss: 0.5450 - val_accuracy: 0.7647\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5194 - accuracy: 0.7684 - val_loss: 0.5434 - val_accuracy: 0.7731\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.5167 - accuracy: 0.7832 - val_loss: 0.5421 - val_accuracy: 0.7731\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.5311 - accuracy: 0.7705 - val_loss: 0.5410 - val_accuracy: 0.7815\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.5194 - accuracy: 0.7811 - val_loss: 0.5397 - val_accuracy: 0.7899\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 136us/sample - loss: 0.5250 - accuracy: 0.7726 - val_loss: 0.5384 - val_accuracy: 0.7899\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.5397 - accuracy: 0.7747 - val_loss: 0.5370 - val_accuracy: 0.7815\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5077 - accuracy: 0.7768 - val_loss: 0.5359 - val_accuracy: 0.7815\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5162 - accuracy: 0.7768 - val_loss: 0.5350 - val_accuracy: 0.7731\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4997 - accuracy: 0.7747 - val_loss: 0.5339 - val_accuracy: 0.7731\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5187 - accuracy: 0.7853 - val_loss: 0.5326 - val_accuracy: 0.7731\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.5072 - accuracy: 0.8063 - val_loss: 0.5316 - val_accuracy: 0.7731\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5194 - accuracy: 0.7832 - val_loss: 0.5305 - val_accuracy: 0.7731\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5054 - accuracy: 0.7853 - val_loss: 0.5296 - val_accuracy: 0.7731\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4981 - accuracy: 0.7937 - val_loss: 0.5291 - val_accuracy: 0.7731\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4943 - accuracy: 0.8000 - val_loss: 0.5284 - val_accuracy: 0.7731\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5090 - accuracy: 0.7979 - val_loss: 0.5280 - val_accuracy: 0.7731\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5168 - accuracy: 0.7768 - val_loss: 0.5271 - val_accuracy: 0.7731\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5259 - accuracy: 0.7789 - val_loss: 0.5265 - val_accuracy: 0.7731\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4963 - accuracy: 0.7874 - val_loss: 0.5258 - val_accuracy: 0.7731\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5084 - accuracy: 0.7811 - val_loss: 0.5248 - val_accuracy: 0.7731\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4820 - accuracy: 0.7916 - val_loss: 0.5240 - val_accuracy: 0.7731\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.5063 - accuracy: 0.7705 - val_loss: 0.5233 - val_accuracy: 0.7731\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4893 - accuracy: 0.7705 - val_loss: 0.5225 - val_accuracy: 0.7731\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5205 - accuracy: 0.8042 - val_loss: 0.5218 - val_accuracy: 0.7731\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4894 - accuracy: 0.8063 - val_loss: 0.5212 - val_accuracy: 0.7731\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4960 - accuracy: 0.7895 - val_loss: 0.5204 - val_accuracy: 0.7731\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 114us/sample - loss: 0.4922 - accuracy: 0.7937 - val_loss: 0.5198 - val_accuracy: 0.7731\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5013 - accuracy: 0.7937 - val_loss: 0.5196 - val_accuracy: 0.7731\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5054 - accuracy: 0.7916 - val_loss: 0.5188 - val_accuracy: 0.7731\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5073 - accuracy: 0.7747 - val_loss: 0.5183 - val_accuracy: 0.7731\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4744 - accuracy: 0.7853 - val_loss: 0.5177 - val_accuracy: 0.7731\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4767 - accuracy: 0.7979 - val_loss: 0.5173 - val_accuracy: 0.7731\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4968 - accuracy: 0.7832 - val_loss: 0.5168 - val_accuracy: 0.7731\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4895 - accuracy: 0.7937 - val_loss: 0.5160 - val_accuracy: 0.7731\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4994 - accuracy: 0.7937 - val_loss: 0.5153 - val_accuracy: 0.7731\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5033 - accuracy: 0.7979 - val_loss: 0.5148 - val_accuracy: 0.7731\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4904 - accuracy: 0.7832 - val_loss: 0.5148 - val_accuracy: 0.7731\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4931 - accuracy: 0.7916 - val_loss: 0.5143 - val_accuracy: 0.7731\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4959 - accuracy: 0.7895 - val_loss: 0.5140 - val_accuracy: 0.7731\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4725 - accuracy: 0.8000 - val_loss: 0.5131 - val_accuracy: 0.7731\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4770 - accuracy: 0.7979 - val_loss: 0.5123 - val_accuracy: 0.7731\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4816 - accuracy: 0.8021 - val_loss: 0.5117 - val_accuracy: 0.7731\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4754 - accuracy: 0.7958 - val_loss: 0.5114 - val_accuracy: 0.7731\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4779 - accuracy: 0.7916 - val_loss: 0.5110 - val_accuracy: 0.7731\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4878 - accuracy: 0.7937 - val_loss: 0.5103 - val_accuracy: 0.7731\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.4686 - accuracy: 0.8105 - val_loss: 0.5103 - val_accuracy: 0.7731\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4850 - accuracy: 0.8084 - val_loss: 0.5098 - val_accuracy: 0.7731\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4761 - accuracy: 0.7916 - val_loss: 0.5092 - val_accuracy: 0.7731\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4832 - accuracy: 0.8063 - val_loss: 0.5091 - val_accuracy: 0.7731\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4747 - accuracy: 0.7937 - val_loss: 0.5085 - val_accuracy: 0.7731\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4728 - accuracy: 0.8126 - val_loss: 0.5078 - val_accuracy: 0.7731\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4740 - accuracy: 0.7916 - val_loss: 0.5078 - val_accuracy: 0.7731\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4927 - accuracy: 0.7789 - val_loss: 0.5079 - val_accuracy: 0.7815\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4709 - accuracy: 0.8021 - val_loss: 0.5080 - val_accuracy: 0.7815\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4941 - accuracy: 0.7916 - val_loss: 0.5080 - val_accuracy: 0.7815\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4696 - accuracy: 0.7958 - val_loss: 0.5078 - val_accuracy: 0.7815\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4708 - accuracy: 0.8042 - val_loss: 0.5073 - val_accuracy: 0.7815\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4730 - accuracy: 0.8147 - val_loss: 0.5070 - val_accuracy: 0.7815\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4769 - accuracy: 0.8084 - val_loss: 0.5064 - val_accuracy: 0.7815\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4860 - accuracy: 0.7958 - val_loss: 0.5059 - val_accuracy: 0.7815\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4719 - accuracy: 0.7979 - val_loss: 0.5058 - val_accuracy: 0.7815\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4695 - accuracy: 0.7937 - val_loss: 0.5053 - val_accuracy: 0.7815\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4493 - accuracy: 0.8189 - val_loss: 0.5049 - val_accuracy: 0.7815\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4762 - accuracy: 0.8063 - val_loss: 0.5045 - val_accuracy: 0.7815\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4835 - accuracy: 0.8021 - val_loss: 0.5039 - val_accuracy: 0.7815\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4620 - accuracy: 0.8147 - val_loss: 0.5035 - val_accuracy: 0.7815\n",
      "297/297 [==============================] - 0s 51us/sample - loss: 0.4181 - accuracy: 0.8316\n",
      "[CV]  optimizer=SGD, n_neurons=42, n_hidden=2, activation=relu, total=   5.6s\n",
      "[CV] optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6501 - accuracy: 0.6589 - val_loss: 0.5635 - val_accuracy: 0.7647\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 106us/sample - loss: 0.5476 - accuracy: 0.7874 - val_loss: 0.4970 - val_accuracy: 0.8067\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4977 - accuracy: 0.7958 - val_loss: 0.4597 - val_accuracy: 0.8151\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4718 - accuracy: 0.8000 - val_loss: 0.4334 - val_accuracy: 0.8319\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4476 - accuracy: 0.8147 - val_loss: 0.4165 - val_accuracy: 0.8571\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4381 - accuracy: 0.8189 - val_loss: 0.4013 - val_accuracy: 0.8571\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4276 - accuracy: 0.8168 - val_loss: 0.3964 - val_accuracy: 0.8571\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4278 - accuracy: 0.8253 - val_loss: 0.3896 - val_accuracy: 0.8571\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 104us/sample - loss: 0.4222 - accuracy: 0.8253 - val_loss: 0.3811 - val_accuracy: 0.8571\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.4147 - accuracy: 0.8232 - val_loss: 0.3767 - val_accuracy: 0.8571\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4190 - accuracy: 0.8189 - val_loss: 0.3760 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4091 - accuracy: 0.8379 - val_loss: 0.3734 - val_accuracy: 0.8571\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4084 - accuracy: 0.8253 - val_loss: 0.3704 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4155 - accuracy: 0.8274 - val_loss: 0.3712 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4037 - accuracy: 0.8232 - val_loss: 0.3722 - val_accuracy: 0.8655\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4079 - accuracy: 0.8232 - val_loss: 0.3689 - val_accuracy: 0.8655\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4039 - accuracy: 0.8337 - val_loss: 0.3695 - val_accuracy: 0.8655\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4027 - accuracy: 0.8232 - val_loss: 0.3674 - val_accuracy: 0.8655\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3996 - accuracy: 0.8379 - val_loss: 0.3683 - val_accuracy: 0.8739\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4065 - accuracy: 0.8337 - val_loss: 0.3658 - val_accuracy: 0.8739\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3997 - accuracy: 0.8337 - val_loss: 0.3661 - val_accuracy: 0.8739\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3998 - accuracy: 0.8274 - val_loss: 0.3661 - val_accuracy: 0.8739\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4000 - accuracy: 0.8274 - val_loss: 0.3666 - val_accuracy: 0.8739\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4043 - accuracy: 0.8400 - val_loss: 0.3661 - val_accuracy: 0.8739\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3981 - accuracy: 0.8253 - val_loss: 0.3646 - val_accuracy: 0.8739\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3959 - accuracy: 0.8400 - val_loss: 0.3682 - val_accuracy: 0.8739\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3983 - accuracy: 0.8358 - val_loss: 0.3652 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3998 - accuracy: 0.8358 - val_loss: 0.3681 - val_accuracy: 0.8739\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3953 - accuracy: 0.8379 - val_loss: 0.3698 - val_accuracy: 0.8739\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3991 - accuracy: 0.8442 - val_loss: 0.3689 - val_accuracy: 0.8739\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3975 - accuracy: 0.8316 - val_loss: 0.3677 - val_accuracy: 0.8739\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4009 - accuracy: 0.8295 - val_loss: 0.3670 - val_accuracy: 0.8739\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 109us/sample - loss: 0.3924 - accuracy: 0.8295 - val_loss: 0.3675 - val_accuracy: 0.8739\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3937 - accuracy: 0.8400 - val_loss: 0.3733 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3997 - accuracy: 0.8316 - val_loss: 0.3675 - val_accuracy: 0.8739\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3981 - accuracy: 0.8274 - val_loss: 0.3686 - val_accuracy: 0.8739\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3943 - accuracy: 0.8337 - val_loss: 0.3702 - val_accuracy: 0.8655\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3988 - accuracy: 0.8400 - val_loss: 0.3724 - val_accuracy: 0.8571\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3925 - accuracy: 0.8379 - val_loss: 0.3715 - val_accuracy: 0.8655\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4001 - accuracy: 0.8358 - val_loss: 0.3767 - val_accuracy: 0.8487\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3993 - accuracy: 0.8337 - val_loss: 0.3735 - val_accuracy: 0.8571\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3919 - accuracy: 0.8358 - val_loss: 0.3751 - val_accuracy: 0.8571\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3949 - accuracy: 0.8379 - val_loss: 0.3757 - val_accuracy: 0.8571\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3932 - accuracy: 0.8337 - val_loss: 0.3750 - val_accuracy: 0.8571\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3944 - accuracy: 0.8232 - val_loss: 0.3784 - val_accuracy: 0.8403\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3895 - accuracy: 0.8358 - val_loss: 0.3778 - val_accuracy: 0.8403\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3910 - accuracy: 0.8358 - val_loss: 0.3826 - val_accuracy: 0.8403\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3899 - accuracy: 0.8232 - val_loss: 0.3789 - val_accuracy: 0.8487\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3830 - accuracy: 0.8400 - val_loss: 0.3764 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3983 - accuracy: 0.8358 - val_loss: 0.3797 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4039 - accuracy: 0.8295 - val_loss: 0.3786 - val_accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3899 - accuracy: 0.8295 - val_loss: 0.3734 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3855 - accuracy: 0.8421 - val_loss: 0.3776 - val_accuracy: 0.8487\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3955 - accuracy: 0.8295 - val_loss: 0.3800 - val_accuracy: 0.8487\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3902 - accuracy: 0.8421 - val_loss: 0.3847 - val_accuracy: 0.8319\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3920 - accuracy: 0.8274 - val_loss: 0.3801 - val_accuracy: 0.8487\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3922 - accuracy: 0.8316 - val_loss: 0.3791 - val_accuracy: 0.8487\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3915 - accuracy: 0.8232 - val_loss: 0.3784 - val_accuracy: 0.8487\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3922 - accuracy: 0.8442 - val_loss: 0.3763 - val_accuracy: 0.8487\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3881 - accuracy: 0.8316 - val_loss: 0.3810 - val_accuracy: 0.8487\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3927 - accuracy: 0.8400 - val_loss: 0.3778 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3904 - accuracy: 0.8316 - val_loss: 0.3864 - val_accuracy: 0.8319\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3856 - accuracy: 0.8379 - val_loss: 0.3818 - val_accuracy: 0.8487\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3914 - accuracy: 0.8400 - val_loss: 0.3845 - val_accuracy: 0.8403\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3925 - accuracy: 0.8295 - val_loss: 0.3806 - val_accuracy: 0.8487\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4018 - accuracy: 0.8295 - val_loss: 0.3846 - val_accuracy: 0.8487\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3809 - accuracy: 0.8421 - val_loss: 0.3817 - val_accuracy: 0.8487\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3938 - accuracy: 0.8337 - val_loss: 0.3843 - val_accuracy: 0.8487\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3948 - accuracy: 0.8442 - val_loss: 0.3823 - val_accuracy: 0.8487\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3863 - accuracy: 0.8316 - val_loss: 0.3864 - val_accuracy: 0.8403\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3873 - accuracy: 0.8379 - val_loss: 0.3852 - val_accuracy: 0.8487\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3906 - accuracy: 0.8358 - val_loss: 0.3828 - val_accuracy: 0.8487\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3883 - accuracy: 0.8400 - val_loss: 0.3827 - val_accuracy: 0.8487\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3901 - accuracy: 0.8316 - val_loss: 0.3830 - val_accuracy: 0.8487\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3874 - accuracy: 0.8379 - val_loss: 0.3849 - val_accuracy: 0.8487\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3871 - accuracy: 0.8337 - val_loss: 0.3883 - val_accuracy: 0.8487\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3903 - accuracy: 0.8505 - val_loss: 0.3877 - val_accuracy: 0.8487\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3912 - accuracy: 0.8400 - val_loss: 0.3892 - val_accuracy: 0.8487\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3891 - accuracy: 0.8379 - val_loss: 0.3870 - val_accuracy: 0.8487\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3935 - accuracy: 0.8379 - val_loss: 0.3914 - val_accuracy: 0.8319\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3908 - accuracy: 0.8379 - val_loss: 0.3884 - val_accuracy: 0.8487\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 101us/sample - loss: 0.3849 - accuracy: 0.8400 - val_loss: 0.3869 - val_accuracy: 0.8487\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3863 - accuracy: 0.8358 - val_loss: 0.3911 - val_accuracy: 0.8319\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3959 - accuracy: 0.8253 - val_loss: 0.3886 - val_accuracy: 0.8487\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3907 - accuracy: 0.8379 - val_loss: 0.3885 - val_accuracy: 0.8487\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3890 - accuracy: 0.8316 - val_loss: 0.3958 - val_accuracy: 0.8319\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3815 - accuracy: 0.8379 - val_loss: 0.3916 - val_accuracy: 0.8487\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3925 - accuracy: 0.8316 - val_loss: 0.3896 - val_accuracy: 0.8487\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3872 - accuracy: 0.8316 - val_loss: 0.3923 - val_accuracy: 0.8487\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3886 - accuracy: 0.8379 - val_loss: 0.3896 - val_accuracy: 0.8487\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3879 - accuracy: 0.8379 - val_loss: 0.3920 - val_accuracy: 0.8487\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3848 - accuracy: 0.8400 - val_loss: 0.3884 - val_accuracy: 0.8487\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3805 - accuracy: 0.8463 - val_loss: 0.3884 - val_accuracy: 0.8487\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3823 - accuracy: 0.8316 - val_loss: 0.3927 - val_accuracy: 0.8319\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3874 - accuracy: 0.8379 - val_loss: 0.3896 - val_accuracy: 0.8487\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3869 - accuracy: 0.8337 - val_loss: 0.3900 - val_accuracy: 0.8487\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3826 - accuracy: 0.8400 - val_loss: 0.3898 - val_accuracy: 0.8487\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3820 - accuracy: 0.8421 - val_loss: 0.3915 - val_accuracy: 0.8487\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3810 - accuracy: 0.8316 - val_loss: 0.3939 - val_accuracy: 0.8487\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3863 - accuracy: 0.8379 - val_loss: 0.3942 - val_accuracy: 0.8487\n",
      "297/297 [==============================] - 0s 48us/sample - loss: 0.4908 - accuracy: 0.7778\n",
      "[CV]  optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu, total=   5.1s\n",
      "[CV] optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6458 - accuracy: 0.6063 - val_loss: 0.5662 - val_accuracy: 0.7395\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5491 - accuracy: 0.7137 - val_loss: 0.5035 - val_accuracy: 0.7899\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5055 - accuracy: 0.7600 - val_loss: 0.4620 - val_accuracy: 0.8235\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4827 - accuracy: 0.7747 - val_loss: 0.4363 - val_accuracy: 0.8235\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4651 - accuracy: 0.8063 - val_loss: 0.4165 - val_accuracy: 0.8235\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4560 - accuracy: 0.8168 - val_loss: 0.4043 - val_accuracy: 0.8571\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4538 - accuracy: 0.8063 - val_loss: 0.3953 - val_accuracy: 0.8655\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4446 - accuracy: 0.8211 - val_loss: 0.3867 - val_accuracy: 0.8655\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4273 - accuracy: 0.8295 - val_loss: 0.3817 - val_accuracy: 0.8571\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 121us/sample - loss: 0.4329 - accuracy: 0.8189 - val_loss: 0.3761 - val_accuracy: 0.8571\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4381 - accuracy: 0.8126 - val_loss: 0.3726 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4329 - accuracy: 0.8147 - val_loss: 0.3688 - val_accuracy: 0.8571\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4330 - accuracy: 0.8211 - val_loss: 0.3671 - val_accuracy: 0.8571\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4231 - accuracy: 0.8358 - val_loss: 0.3629 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4278 - accuracy: 0.8105 - val_loss: 0.3645 - val_accuracy: 0.8487\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4240 - accuracy: 0.8168 - val_loss: 0.3625 - val_accuracy: 0.8571\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4263 - accuracy: 0.8147 - val_loss: 0.3617 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4162 - accuracy: 0.8232 - val_loss: 0.3584 - val_accuracy: 0.8571\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4150 - accuracy: 0.8274 - val_loss: 0.3583 - val_accuracy: 0.8571\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4308 - accuracy: 0.8126 - val_loss: 0.3559 - val_accuracy: 0.8571\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.4256 - accuracy: 0.8211 - val_loss: 0.3547 - val_accuracy: 0.8571\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.4272 - accuracy: 0.8105 - val_loss: 0.3561 - val_accuracy: 0.8655\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4164 - accuracy: 0.8295 - val_loss: 0.3549 - val_accuracy: 0.8655\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4173 - accuracy: 0.8168 - val_loss: 0.3523 - val_accuracy: 0.8571\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4243 - accuracy: 0.8232 - val_loss: 0.3530 - val_accuracy: 0.8655\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4184 - accuracy: 0.8232 - val_loss: 0.3515 - val_accuracy: 0.8655\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4176 - accuracy: 0.8232 - val_loss: 0.3531 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4138 - accuracy: 0.8232 - val_loss: 0.3499 - val_accuracy: 0.8655\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4212 - accuracy: 0.8147 - val_loss: 0.3484 - val_accuracy: 0.8655\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4161 - accuracy: 0.8211 - val_loss: 0.3465 - val_accuracy: 0.8655\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4179 - accuracy: 0.8189 - val_loss: 0.3481 - val_accuracy: 0.8739\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4174 - accuracy: 0.8211 - val_loss: 0.3505 - val_accuracy: 0.8739\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4173 - accuracy: 0.8211 - val_loss: 0.3501 - val_accuracy: 0.8739\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4159 - accuracy: 0.8316 - val_loss: 0.3492 - val_accuracy: 0.8739\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4163 - accuracy: 0.8232 - val_loss: 0.3510 - val_accuracy: 0.8655\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4261 - accuracy: 0.8274 - val_loss: 0.3516 - val_accuracy: 0.8655\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4179 - accuracy: 0.8147 - val_loss: 0.3485 - val_accuracy: 0.8739\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4171 - accuracy: 0.8253 - val_loss: 0.3473 - val_accuracy: 0.8739\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4165 - accuracy: 0.8211 - val_loss: 0.3476 - val_accuracy: 0.8739\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4274 - accuracy: 0.8168 - val_loss: 0.3461 - val_accuracy: 0.8739\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4113 - accuracy: 0.8232 - val_loss: 0.3450 - val_accuracy: 0.8739\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4203 - accuracy: 0.8295 - val_loss: 0.3454 - val_accuracy: 0.8739\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4228 - accuracy: 0.8253 - val_loss: 0.3468 - val_accuracy: 0.8739\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4047 - accuracy: 0.8295 - val_loss: 0.3444 - val_accuracy: 0.8739\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4164 - accuracy: 0.8253 - val_loss: 0.3460 - val_accuracy: 0.8739\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4108 - accuracy: 0.8253 - val_loss: 0.3443 - val_accuracy: 0.8739\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4210 - accuracy: 0.8253 - val_loss: 0.3446 - val_accuracy: 0.8739\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4222 - accuracy: 0.8211 - val_loss: 0.3460 - val_accuracy: 0.8655\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4169 - accuracy: 0.8316 - val_loss: 0.3462 - val_accuracy: 0.8655\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4154 - accuracy: 0.8232 - val_loss: 0.3459 - val_accuracy: 0.8655\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4138 - accuracy: 0.8253 - val_loss: 0.3486 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4140 - accuracy: 0.8211 - val_loss: 0.3472 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4131 - accuracy: 0.8295 - val_loss: 0.3467 - val_accuracy: 0.8739\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4149 - accuracy: 0.8211 - val_loss: 0.3440 - val_accuracy: 0.8739\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4172 - accuracy: 0.8189 - val_loss: 0.3460 - val_accuracy: 0.8655\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4040 - accuracy: 0.8189 - val_loss: 0.3451 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 117us/sample - loss: 0.4179 - accuracy: 0.8211 - val_loss: 0.3452 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 108us/sample - loss: 0.4191 - accuracy: 0.8232 - val_loss: 0.3432 - val_accuracy: 0.8739\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4153 - accuracy: 0.8295 - val_loss: 0.3452 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4078 - accuracy: 0.8316 - val_loss: 0.3441 - val_accuracy: 0.8655\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4076 - accuracy: 0.8232 - val_loss: 0.3430 - val_accuracy: 0.8655\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4193 - accuracy: 0.8168 - val_loss: 0.3474 - val_accuracy: 0.8655\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4147 - accuracy: 0.8253 - val_loss: 0.3471 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4138 - accuracy: 0.8379 - val_loss: 0.3454 - val_accuracy: 0.8655\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4189 - accuracy: 0.8274 - val_loss: 0.3452 - val_accuracy: 0.8655\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4121 - accuracy: 0.8211 - val_loss: 0.3441 - val_accuracy: 0.8655\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4080 - accuracy: 0.8189 - val_loss: 0.3449 - val_accuracy: 0.8655\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4147 - accuracy: 0.8168 - val_loss: 0.3442 - val_accuracy: 0.8655\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4209 - accuracy: 0.8211 - val_loss: 0.3440 - val_accuracy: 0.8655\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4142 - accuracy: 0.8232 - val_loss: 0.3416 - val_accuracy: 0.8739\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4099 - accuracy: 0.8232 - val_loss: 0.3433 - val_accuracy: 0.8655\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4143 - accuracy: 0.8316 - val_loss: 0.3418 - val_accuracy: 0.8655\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4043 - accuracy: 0.8337 - val_loss: 0.3423 - val_accuracy: 0.8655\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4129 - accuracy: 0.8147 - val_loss: 0.3436 - val_accuracy: 0.8655\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4100 - accuracy: 0.8232 - val_loss: 0.3450 - val_accuracy: 0.8655\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4132 - accuracy: 0.8274 - val_loss: 0.3459 - val_accuracy: 0.8655\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4174 - accuracy: 0.8168 - val_loss: 0.3444 - val_accuracy: 0.8655\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4078 - accuracy: 0.8358 - val_loss: 0.3466 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4114 - accuracy: 0.8295 - val_loss: 0.3438 - val_accuracy: 0.8655\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4135 - accuracy: 0.8253 - val_loss: 0.3452 - val_accuracy: 0.8655\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4098 - accuracy: 0.8358 - val_loss: 0.3427 - val_accuracy: 0.8655\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4125 - accuracy: 0.8253 - val_loss: 0.3420 - val_accuracy: 0.8655\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4101 - accuracy: 0.8253 - val_loss: 0.3414 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4078 - accuracy: 0.8211 - val_loss: 0.3423 - val_accuracy: 0.8655\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4211 - accuracy: 0.8274 - val_loss: 0.3422 - val_accuracy: 0.8655\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4094 - accuracy: 0.8316 - val_loss: 0.3428 - val_accuracy: 0.8655\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4084 - accuracy: 0.8253 - val_loss: 0.3435 - val_accuracy: 0.8655\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4141 - accuracy: 0.8232 - val_loss: 0.3418 - val_accuracy: 0.8655\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4099 - accuracy: 0.8295 - val_loss: 0.3406 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4118 - accuracy: 0.8253 - val_loss: 0.3415 - val_accuracy: 0.8739\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4033 - accuracy: 0.8400 - val_loss: 0.3398 - val_accuracy: 0.8739\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4063 - accuracy: 0.8274 - val_loss: 0.3440 - val_accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4090 - accuracy: 0.8295 - val_loss: 0.3398 - val_accuracy: 0.8739\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4055 - accuracy: 0.8295 - val_loss: 0.3397 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4116 - accuracy: 0.8337 - val_loss: 0.3406 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4126 - accuracy: 0.8274 - val_loss: 0.3413 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4164 - accuracy: 0.8253 - val_loss: 0.3440 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4078 - accuracy: 0.8253 - val_loss: 0.3428 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4070 - accuracy: 0.8316 - val_loss: 0.3419 - val_accuracy: 0.8655\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4092 - accuracy: 0.8337 - val_loss: 0.3399 - val_accuracy: 0.8655\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.4306 - accuracy: 0.8148\n",
      "[CV]  optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu, total=   5.1s\n",
      "[CV] optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6126 - accuracy: 0.6695 - val_loss: 0.5656 - val_accuracy: 0.7311\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5404 - accuracy: 0.7726 - val_loss: 0.5352 - val_accuracy: 0.7647\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5124 - accuracy: 0.7937 - val_loss: 0.5131 - val_accuracy: 0.7899\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4874 - accuracy: 0.7958 - val_loss: 0.5014 - val_accuracy: 0.7815\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4732 - accuracy: 0.8126 - val_loss: 0.4941 - val_accuracy: 0.7731\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4627 - accuracy: 0.8105 - val_loss: 0.4901 - val_accuracy: 0.7815\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4496 - accuracy: 0.8084 - val_loss: 0.4864 - val_accuracy: 0.7899\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4493 - accuracy: 0.8126 - val_loss: 0.4835 - val_accuracy: 0.7815\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4461 - accuracy: 0.8126 - val_loss: 0.4805 - val_accuracy: 0.7815\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4589 - accuracy: 0.8084 - val_loss: 0.4776 - val_accuracy: 0.7815\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4540 - accuracy: 0.8126 - val_loss: 0.4766 - val_accuracy: 0.7815\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4411 - accuracy: 0.8063 - val_loss: 0.4743 - val_accuracy: 0.7899\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4368 - accuracy: 0.8232 - val_loss: 0.4740 - val_accuracy: 0.7815\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4335 - accuracy: 0.8084 - val_loss: 0.4734 - val_accuracy: 0.7899\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4411 - accuracy: 0.8126 - val_loss: 0.4718 - val_accuracy: 0.7983\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4298 - accuracy: 0.8105 - val_loss: 0.4695 - val_accuracy: 0.7815\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4344 - accuracy: 0.8147 - val_loss: 0.4690 - val_accuracy: 0.7899\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4359 - accuracy: 0.8232 - val_loss: 0.4679 - val_accuracy: 0.7899\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4384 - accuracy: 0.8211 - val_loss: 0.4663 - val_accuracy: 0.7983\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4347 - accuracy: 0.8211 - val_loss: 0.4663 - val_accuracy: 0.7983\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4312 - accuracy: 0.8211 - val_loss: 0.4662 - val_accuracy: 0.8067\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4379 - accuracy: 0.8253 - val_loss: 0.4638 - val_accuracy: 0.7983\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4391 - accuracy: 0.8168 - val_loss: 0.4628 - val_accuracy: 0.7983\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4323 - accuracy: 0.8211 - val_loss: 0.4631 - val_accuracy: 0.8067\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4334 - accuracy: 0.8232 - val_loss: 0.4634 - val_accuracy: 0.8067\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4263 - accuracy: 0.8337 - val_loss: 0.4642 - val_accuracy: 0.8067\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4359 - accuracy: 0.8253 - val_loss: 0.4622 - val_accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4328 - accuracy: 0.8211 - val_loss: 0.4627 - val_accuracy: 0.7983\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4433 - accuracy: 0.8147 - val_loss: 0.4614 - val_accuracy: 0.8151\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4267 - accuracy: 0.8232 - val_loss: 0.4619 - val_accuracy: 0.8067\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4288 - accuracy: 0.8274 - val_loss: 0.4620 - val_accuracy: 0.8151\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4284 - accuracy: 0.8274 - val_loss: 0.4614 - val_accuracy: 0.8151\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4173 - accuracy: 0.8337 - val_loss: 0.4618 - val_accuracy: 0.8151\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4275 - accuracy: 0.8253 - val_loss: 0.4610 - val_accuracy: 0.8067\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4261 - accuracy: 0.8274 - val_loss: 0.4617 - val_accuracy: 0.8067\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4311 - accuracy: 0.8316 - val_loss: 0.4606 - val_accuracy: 0.8151\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4301 - accuracy: 0.8295 - val_loss: 0.4608 - val_accuracy: 0.8151\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4305 - accuracy: 0.8358 - val_loss: 0.4609 - val_accuracy: 0.8151\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4344 - accuracy: 0.8274 - val_loss: 0.4623 - val_accuracy: 0.8067\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4359 - accuracy: 0.8274 - val_loss: 0.4623 - val_accuracy: 0.8067\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4283 - accuracy: 0.8358 - val_loss: 0.4611 - val_accuracy: 0.8151\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4270 - accuracy: 0.8274 - val_loss: 0.4610 - val_accuracy: 0.8151\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4218 - accuracy: 0.8295 - val_loss: 0.4623 - val_accuracy: 0.8067\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4224 - accuracy: 0.8316 - val_loss: 0.4627 - val_accuracy: 0.7983\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4273 - accuracy: 0.8253 - val_loss: 0.4602 - val_accuracy: 0.8067\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4259 - accuracy: 0.8253 - val_loss: 0.4605 - val_accuracy: 0.8067\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4203 - accuracy: 0.8358 - val_loss: 0.4619 - val_accuracy: 0.8067\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4288 - accuracy: 0.8253 - val_loss: 0.4603 - val_accuracy: 0.8067\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4187 - accuracy: 0.8379 - val_loss: 0.4598 - val_accuracy: 0.8067\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4238 - accuracy: 0.8168 - val_loss: 0.4599 - val_accuracy: 0.8067\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4253 - accuracy: 0.8337 - val_loss: 0.4588 - val_accuracy: 0.8067\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4303 - accuracy: 0.8295 - val_loss: 0.4592 - val_accuracy: 0.8067\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4165 - accuracy: 0.8253 - val_loss: 0.4598 - val_accuracy: 0.7983\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4254 - accuracy: 0.8274 - val_loss: 0.4603 - val_accuracy: 0.7983\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4214 - accuracy: 0.8274 - val_loss: 0.4617 - val_accuracy: 0.7983\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4269 - accuracy: 0.8337 - val_loss: 0.4597 - val_accuracy: 0.8067\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4215 - accuracy: 0.8274 - val_loss: 0.4602 - val_accuracy: 0.8067\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4322 - accuracy: 0.8316 - val_loss: 0.4618 - val_accuracy: 0.7983\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4271 - accuracy: 0.8337 - val_loss: 0.4620 - val_accuracy: 0.7983\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4187 - accuracy: 0.8379 - val_loss: 0.4607 - val_accuracy: 0.7983\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4231 - accuracy: 0.8295 - val_loss: 0.4602 - val_accuracy: 0.8067\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4226 - accuracy: 0.8274 - val_loss: 0.4605 - val_accuracy: 0.8067\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4234 - accuracy: 0.8295 - val_loss: 0.4598 - val_accuracy: 0.8067\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4184 - accuracy: 0.8337 - val_loss: 0.4602 - val_accuracy: 0.8067\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4233 - accuracy: 0.8253 - val_loss: 0.4600 - val_accuracy: 0.7983\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4292 - accuracy: 0.8295 - val_loss: 0.4591 - val_accuracy: 0.7983\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4198 - accuracy: 0.8358 - val_loss: 0.4596 - val_accuracy: 0.8067\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4171 - accuracy: 0.8274 - val_loss: 0.4609 - val_accuracy: 0.7983\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4216 - accuracy: 0.8232 - val_loss: 0.4609 - val_accuracy: 0.7983\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4223 - accuracy: 0.8274 - val_loss: 0.4603 - val_accuracy: 0.7983\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4153 - accuracy: 0.8379 - val_loss: 0.4611 - val_accuracy: 0.7983\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4207 - accuracy: 0.8253 - val_loss: 0.4624 - val_accuracy: 0.7983\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4210 - accuracy: 0.8253 - val_loss: 0.4596 - val_accuracy: 0.7983\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4160 - accuracy: 0.8421 - val_loss: 0.4590 - val_accuracy: 0.7983\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4231 - accuracy: 0.8232 - val_loss: 0.4588 - val_accuracy: 0.8067\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4250 - accuracy: 0.8295 - val_loss: 0.4597 - val_accuracy: 0.7983\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4196 - accuracy: 0.8316 - val_loss: 0.4605 - val_accuracy: 0.7983\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4226 - accuracy: 0.8358 - val_loss: 0.4631 - val_accuracy: 0.8067\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4279 - accuracy: 0.8253 - val_loss: 0.4635 - val_accuracy: 0.8067\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4227 - accuracy: 0.8253 - val_loss: 0.4625 - val_accuracy: 0.8067\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4177 - accuracy: 0.8295 - val_loss: 0.4617 - val_accuracy: 0.7983\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.4120 - accuracy: 0.8295 - val_loss: 0.4615 - val_accuracy: 0.7983\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4138 - accuracy: 0.8358 - val_loss: 0.4638 - val_accuracy: 0.7983\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4203 - accuracy: 0.8274 - val_loss: 0.4624 - val_accuracy: 0.8067\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4150 - accuracy: 0.8379 - val_loss: 0.4645 - val_accuracy: 0.8067\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4095 - accuracy: 0.8400 - val_loss: 0.4630 - val_accuracy: 0.7983\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4176 - accuracy: 0.8400 - val_loss: 0.4631 - val_accuracy: 0.7983\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4164 - accuracy: 0.8253 - val_loss: 0.4615 - val_accuracy: 0.7983\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4220 - accuracy: 0.8316 - val_loss: 0.4642 - val_accuracy: 0.7983\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4154 - accuracy: 0.8316 - val_loss: 0.4655 - val_accuracy: 0.8067\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4135 - accuracy: 0.8337 - val_loss: 0.4641 - val_accuracy: 0.8067\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4164 - accuracy: 0.8253 - val_loss: 0.4645 - val_accuracy: 0.8067\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4247 - accuracy: 0.8337 - val_loss: 0.4630 - val_accuracy: 0.8067\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4121 - accuracy: 0.8421 - val_loss: 0.4631 - val_accuracy: 0.8067\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4218 - accuracy: 0.8316 - val_loss: 0.4627 - val_accuracy: 0.8067\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4143 - accuracy: 0.8400 - val_loss: 0.4637 - val_accuracy: 0.8067\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4110 - accuracy: 0.8337 - val_loss: 0.4628 - val_accuracy: 0.8067\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4238 - accuracy: 0.8358 - val_loss: 0.4639 - val_accuracy: 0.8067\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4152 - accuracy: 0.8400 - val_loss: 0.4639 - val_accuracy: 0.8067\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4128 - accuracy: 0.8358 - val_loss: 0.4668 - val_accuracy: 0.8067\n",
      "297/297 [==============================] - 0s 52us/sample - loss: 0.3881 - accuracy: 0.8384\n",
      "[CV]  optimizer=RMSprop, n_neurons=77, n_hidden=1, activation=elu, total=   5.0s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6148 - accuracy: 0.6632 - val_loss: 0.5527 - val_accuracy: 0.8151\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5831 - accuracy: 0.6842 - val_loss: 0.5128 - val_accuracy: 0.7899\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5539 - accuracy: 0.7137 - val_loss: 0.4844 - val_accuracy: 0.7899\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5240 - accuracy: 0.7558 - val_loss: 0.4625 - val_accuracy: 0.8151\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5143 - accuracy: 0.7726 - val_loss: 0.4438 - val_accuracy: 0.8319\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4984 - accuracy: 0.7768 - val_loss: 0.4274 - val_accuracy: 0.8487\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4901 - accuracy: 0.7916 - val_loss: 0.4126 - val_accuracy: 0.8739\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4796 - accuracy: 0.8000 - val_loss: 0.4038 - val_accuracy: 0.8655\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4495 - accuracy: 0.7958 - val_loss: 0.3923 - val_accuracy: 0.8824\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4520 - accuracy: 0.8042 - val_loss: 0.3833 - val_accuracy: 0.8824\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4410 - accuracy: 0.8168 - val_loss: 0.3781 - val_accuracy: 0.8824\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4214 - accuracy: 0.8105 - val_loss: 0.3709 - val_accuracy: 0.8739\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4324 - accuracy: 0.8232 - val_loss: 0.3669 - val_accuracy: 0.8739\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4221 - accuracy: 0.8105 - val_loss: 0.3673 - val_accuracy: 0.8739\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4212 - accuracy: 0.8189 - val_loss: 0.3627 - val_accuracy: 0.8824\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4098 - accuracy: 0.8211 - val_loss: 0.3624 - val_accuracy: 0.8824\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4284 - accuracy: 0.8084 - val_loss: 0.3601 - val_accuracy: 0.8655\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4159 - accuracy: 0.8168 - val_loss: 0.3571 - val_accuracy: 0.8655\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4022 - accuracy: 0.8211 - val_loss: 0.3573 - val_accuracy: 0.8739\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4222 - accuracy: 0.8168 - val_loss: 0.3570 - val_accuracy: 0.8739\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4015 - accuracy: 0.8358 - val_loss: 0.3582 - val_accuracy: 0.8739\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4015 - accuracy: 0.8274 - val_loss: 0.3547 - val_accuracy: 0.8739\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4078 - accuracy: 0.8253 - val_loss: 0.3516 - val_accuracy: 0.8908\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4069 - accuracy: 0.8189 - val_loss: 0.3537 - val_accuracy: 0.8739\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3915 - accuracy: 0.8463 - val_loss: 0.3478 - val_accuracy: 0.8824\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4080 - accuracy: 0.8147 - val_loss: 0.3480 - val_accuracy: 0.8824\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3868 - accuracy: 0.8358 - val_loss: 0.3477 - val_accuracy: 0.8824\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3985 - accuracy: 0.8232 - val_loss: 0.3482 - val_accuracy: 0.8655\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3724 - accuracy: 0.8421 - val_loss: 0.3439 - val_accuracy: 0.8824\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3849 - accuracy: 0.8379 - val_loss: 0.3483 - val_accuracy: 0.8655\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3613 - accuracy: 0.8505 - val_loss: 0.3428 - val_accuracy: 0.8739\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3757 - accuracy: 0.8400 - val_loss: 0.3402 - val_accuracy: 0.8824\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3713 - accuracy: 0.8526 - val_loss: 0.3469 - val_accuracy: 0.8655\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3778 - accuracy: 0.8611 - val_loss: 0.3498 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3812 - accuracy: 0.8400 - val_loss: 0.3462 - val_accuracy: 0.8655\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3886 - accuracy: 0.8526 - val_loss: 0.3435 - val_accuracy: 0.8655\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3824 - accuracy: 0.8463 - val_loss: 0.3495 - val_accuracy: 0.8655\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3648 - accuracy: 0.8568 - val_loss: 0.3506 - val_accuracy: 0.8655\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3796 - accuracy: 0.8379 - val_loss: 0.3481 - val_accuracy: 0.8655\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3622 - accuracy: 0.8505 - val_loss: 0.3447 - val_accuracy: 0.8824\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3665 - accuracy: 0.8442 - val_loss: 0.3500 - val_accuracy: 0.8655\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3872 - accuracy: 0.8274 - val_loss: 0.3490 - val_accuracy: 0.8739\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3658 - accuracy: 0.8463 - val_loss: 0.3445 - val_accuracy: 0.8824\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3801 - accuracy: 0.8589 - val_loss: 0.3458 - val_accuracy: 0.8824\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3711 - accuracy: 0.8463 - val_loss: 0.3485 - val_accuracy: 0.8739\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3691 - accuracy: 0.8505 - val_loss: 0.3418 - val_accuracy: 0.8908\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3717 - accuracy: 0.8547 - val_loss: 0.3482 - val_accuracy: 0.8739\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3706 - accuracy: 0.8421 - val_loss: 0.3462 - val_accuracy: 0.8739\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3704 - accuracy: 0.8505 - val_loss: 0.3406 - val_accuracy: 0.8908\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3450 - accuracy: 0.8568 - val_loss: 0.3446 - val_accuracy: 0.8739\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3522 - accuracy: 0.8463 - val_loss: 0.3452 - val_accuracy: 0.8739\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3503 - accuracy: 0.8505 - val_loss: 0.3447 - val_accuracy: 0.8739\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.3583 - accuracy: 0.8547 - val_loss: 0.3490 - val_accuracy: 0.8739\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3626 - accuracy: 0.8484 - val_loss: 0.3462 - val_accuracy: 0.8824\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3685 - accuracy: 0.8505 - val_loss: 0.3421 - val_accuracy: 0.8824\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3531 - accuracy: 0.8526 - val_loss: 0.3433 - val_accuracy: 0.8824\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3510 - accuracy: 0.8589 - val_loss: 0.3483 - val_accuracy: 0.8824\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3596 - accuracy: 0.8400 - val_loss: 0.3500 - val_accuracy: 0.8739\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3481 - accuracy: 0.8442 - val_loss: 0.3561 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3649 - accuracy: 0.8442 - val_loss: 0.3568 - val_accuracy: 0.8571\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3484 - accuracy: 0.8547 - val_loss: 0.3509 - val_accuracy: 0.8655\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3410 - accuracy: 0.8758 - val_loss: 0.3602 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3550 - accuracy: 0.8484 - val_loss: 0.3532 - val_accuracy: 0.8824\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3417 - accuracy: 0.8716 - val_loss: 0.3530 - val_accuracy: 0.8739\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3601 - accuracy: 0.8484 - val_loss: 0.3509 - val_accuracy: 0.8824\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3445 - accuracy: 0.8526 - val_loss: 0.3571 - val_accuracy: 0.8739\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3553 - accuracy: 0.8611 - val_loss: 0.3559 - val_accuracy: 0.8824\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.3656 - accuracy: 0.8253 - val_loss: 0.3516 - val_accuracy: 0.8824\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.3423 - accuracy: 0.8611 - val_loss: 0.3541 - val_accuracy: 0.8824\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.3507 - accuracy: 0.8611 - val_loss: 0.3546 - val_accuracy: 0.8824\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3462 - accuracy: 0.8547 - val_loss: 0.3556 - val_accuracy: 0.8824\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.3469 - accuracy: 0.8674 - val_loss: 0.3526 - val_accuracy: 0.8824\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3525 - accuracy: 0.8442 - val_loss: 0.3562 - val_accuracy: 0.8824\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3519 - accuracy: 0.8589 - val_loss: 0.3520 - val_accuracy: 0.8824\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 122us/sample - loss: 0.3514 - accuracy: 0.8421 - val_loss: 0.3557 - val_accuracy: 0.8824\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.3571 - accuracy: 0.8505 - val_loss: 0.3642 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3470 - accuracy: 0.8611 - val_loss: 0.3559 - val_accuracy: 0.8824\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3292 - accuracy: 0.8800 - val_loss: 0.3614 - val_accuracy: 0.8739\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.3540 - accuracy: 0.8589 - val_loss: 0.3610 - val_accuracy: 0.8739\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3651 - accuracy: 0.8526 - val_loss: 0.3575 - val_accuracy: 0.8824\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3511 - accuracy: 0.8589 - val_loss: 0.3661 - val_accuracy: 0.8655\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3656 - accuracy: 0.8463 - val_loss: 0.3704 - val_accuracy: 0.8655\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3481 - accuracy: 0.8674 - val_loss: 0.3710 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3315 - accuracy: 0.8505 - val_loss: 0.3610 - val_accuracy: 0.8824\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3514 - accuracy: 0.8547 - val_loss: 0.3611 - val_accuracy: 0.8824\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3403 - accuracy: 0.8442 - val_loss: 0.3615 - val_accuracy: 0.8739\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3588 - accuracy: 0.8568 - val_loss: 0.3624 - val_accuracy: 0.8739\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3403 - accuracy: 0.8526 - val_loss: 0.3729 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3379 - accuracy: 0.8547 - val_loss: 0.3746 - val_accuracy: 0.8655\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3538 - accuracy: 0.8421 - val_loss: 0.3721 - val_accuracy: 0.8739\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3394 - accuracy: 0.8611 - val_loss: 0.3741 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 102us/sample - loss: 0.3664 - accuracy: 0.8484 - val_loss: 0.3778 - val_accuracy: 0.8655\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.3407 - accuracy: 0.8526 - val_loss: 0.3758 - val_accuracy: 0.8739\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3397 - accuracy: 0.8589 - val_loss: 0.3727 - val_accuracy: 0.8739\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3492 - accuracy: 0.8653 - val_loss: 0.3778 - val_accuracy: 0.8655\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3447 - accuracy: 0.8547 - val_loss: 0.3696 - val_accuracy: 0.8739\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3438 - accuracy: 0.8589 - val_loss: 0.3738 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3363 - accuracy: 0.8611 - val_loss: 0.3744 - val_accuracy: 0.8655\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3322 - accuracy: 0.8611 - val_loss: 0.3733 - val_accuracy: 0.8655\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3371 - accuracy: 0.8568 - val_loss: 0.3710 - val_accuracy: 0.8655\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.5062 - accuracy: 0.7879\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu, total=   5.3s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.7149 - accuracy: 0.5158 - val_loss: 0.6624 - val_accuracy: 0.6723\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.6595 - accuracy: 0.6800 - val_loss: 0.6354 - val_accuracy: 0.7647\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 121us/sample - loss: 0.6397 - accuracy: 0.7074 - val_loss: 0.6130 - val_accuracy: 0.7563\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 122us/sample - loss: 0.6145 - accuracy: 0.7347 - val_loss: 0.5845 - val_accuracy: 0.7815\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 110us/sample - loss: 0.5868 - accuracy: 0.7305 - val_loss: 0.5484 - val_accuracy: 0.7815\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 98us/sample - loss: 0.5486 - accuracy: 0.7663 - val_loss: 0.5026 - val_accuracy: 0.8067\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 121us/sample - loss: 0.5141 - accuracy: 0.7811 - val_loss: 0.4650 - val_accuracy: 0.8151\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5005 - accuracy: 0.7916 - val_loss: 0.4377 - val_accuracy: 0.8319\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4854 - accuracy: 0.7916 - val_loss: 0.4178 - val_accuracy: 0.8487\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4736 - accuracy: 0.7853 - val_loss: 0.4065 - val_accuracy: 0.8487\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4631 - accuracy: 0.8063 - val_loss: 0.3967 - val_accuracy: 0.8571\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4472 - accuracy: 0.8105 - val_loss: 0.3832 - val_accuracy: 0.8487\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4579 - accuracy: 0.8000 - val_loss: 0.3753 - val_accuracy: 0.8487\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4512 - accuracy: 0.8168 - val_loss: 0.3741 - val_accuracy: 0.8571\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4430 - accuracy: 0.8168 - val_loss: 0.3708 - val_accuracy: 0.8655\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4435 - accuracy: 0.8042 - val_loss: 0.3673 - val_accuracy: 0.8487\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 97us/sample - loss: 0.4294 - accuracy: 0.8232 - val_loss: 0.3658 - val_accuracy: 0.8655\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4311 - accuracy: 0.8211 - val_loss: 0.3672 - val_accuracy: 0.8739\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4316 - accuracy: 0.8232 - val_loss: 0.3646 - val_accuracy: 0.8655\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4139 - accuracy: 0.8337 - val_loss: 0.3613 - val_accuracy: 0.8655\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4236 - accuracy: 0.8253 - val_loss: 0.3586 - val_accuracy: 0.8655\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4143 - accuracy: 0.8274 - val_loss: 0.3579 - val_accuracy: 0.8655\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4112 - accuracy: 0.8379 - val_loss: 0.3573 - val_accuracy: 0.8908\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4161 - accuracy: 0.8274 - val_loss: 0.3555 - val_accuracy: 0.8824\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4072 - accuracy: 0.8253 - val_loss: 0.3559 - val_accuracy: 0.8739\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4132 - accuracy: 0.8253 - val_loss: 0.3548 - val_accuracy: 0.8739\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4208 - accuracy: 0.8274 - val_loss: 0.3551 - val_accuracy: 0.8739\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4189 - accuracy: 0.8421 - val_loss: 0.3561 - val_accuracy: 0.8739\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3939 - accuracy: 0.8379 - val_loss: 0.3545 - val_accuracy: 0.8824\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3763 - accuracy: 0.8442 - val_loss: 0.3484 - val_accuracy: 0.8571\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4240 - accuracy: 0.8253 - val_loss: 0.3494 - val_accuracy: 0.8824\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4051 - accuracy: 0.8274 - val_loss: 0.3502 - val_accuracy: 0.8824\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4061 - accuracy: 0.8295 - val_loss: 0.3513 - val_accuracy: 0.8824\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4007 - accuracy: 0.8168 - val_loss: 0.3489 - val_accuracy: 0.8655\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4123 - accuracy: 0.8168 - val_loss: 0.3467 - val_accuracy: 0.8655\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4118 - accuracy: 0.8232 - val_loss: 0.3540 - val_accuracy: 0.8908\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4087 - accuracy: 0.8421 - val_loss: 0.3560 - val_accuracy: 0.8739\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3979 - accuracy: 0.8253 - val_loss: 0.3512 - val_accuracy: 0.8739\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4112 - accuracy: 0.8274 - val_loss: 0.3512 - val_accuracy: 0.8824\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3892 - accuracy: 0.8337 - val_loss: 0.3468 - val_accuracy: 0.8655\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3959 - accuracy: 0.8189 - val_loss: 0.3468 - val_accuracy: 0.8655\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3917 - accuracy: 0.8295 - val_loss: 0.3491 - val_accuracy: 0.8739\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3983 - accuracy: 0.8274 - val_loss: 0.3473 - val_accuracy: 0.8655\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3939 - accuracy: 0.8168 - val_loss: 0.3477 - val_accuracy: 0.8824\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3853 - accuracy: 0.8421 - val_loss: 0.3487 - val_accuracy: 0.8824\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3801 - accuracy: 0.8442 - val_loss: 0.3455 - val_accuracy: 0.8655\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3917 - accuracy: 0.8358 - val_loss: 0.3449 - val_accuracy: 0.8824\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3778 - accuracy: 0.8358 - val_loss: 0.3444 - val_accuracy: 0.8824\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3870 - accuracy: 0.8400 - val_loss: 0.3445 - val_accuracy: 0.8739\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3810 - accuracy: 0.8337 - val_loss: 0.3424 - val_accuracy: 0.8739\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3916 - accuracy: 0.8253 - val_loss: 0.3418 - val_accuracy: 0.8739\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3999 - accuracy: 0.8316 - val_loss: 0.3448 - val_accuracy: 0.8739\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3946 - accuracy: 0.8316 - val_loss: 0.3468 - val_accuracy: 0.8824\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4007 - accuracy: 0.8400 - val_loss: 0.3446 - val_accuracy: 0.8824\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3852 - accuracy: 0.8295 - val_loss: 0.3445 - val_accuracy: 0.8824\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3901 - accuracy: 0.8274 - val_loss: 0.3489 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3775 - accuracy: 0.8274 - val_loss: 0.3469 - val_accuracy: 0.8824\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3803 - accuracy: 0.8400 - val_loss: 0.3467 - val_accuracy: 0.8655\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3893 - accuracy: 0.8337 - val_loss: 0.3455 - val_accuracy: 0.8824\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3926 - accuracy: 0.8211 - val_loss: 0.3423 - val_accuracy: 0.8824\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3805 - accuracy: 0.8316 - val_loss: 0.3433 - val_accuracy: 0.8824\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3885 - accuracy: 0.8211 - val_loss: 0.3433 - val_accuracy: 0.8655\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4026 - accuracy: 0.8337 - val_loss: 0.3444 - val_accuracy: 0.8739\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3843 - accuracy: 0.8379 - val_loss: 0.3440 - val_accuracy: 0.8739\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3916 - accuracy: 0.8358 - val_loss: 0.3465 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3898 - accuracy: 0.8358 - val_loss: 0.3453 - val_accuracy: 0.8824\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.3927 - accuracy: 0.8316 - val_loss: 0.3452 - val_accuracy: 0.8824\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 100us/sample - loss: 0.3771 - accuracy: 0.8463 - val_loss: 0.3452 - val_accuracy: 0.8824\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3878 - accuracy: 0.8442 - val_loss: 0.3459 - val_accuracy: 0.8824\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3961 - accuracy: 0.8358 - val_loss: 0.3451 - val_accuracy: 0.8824\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3741 - accuracy: 0.8463 - val_loss: 0.3440 - val_accuracy: 0.8824\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3762 - accuracy: 0.8379 - val_loss: 0.3449 - val_accuracy: 0.8824\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3820 - accuracy: 0.8316 - val_loss: 0.3423 - val_accuracy: 0.8739\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3926 - accuracy: 0.8358 - val_loss: 0.3408 - val_accuracy: 0.8655\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3737 - accuracy: 0.8316 - val_loss: 0.3418 - val_accuracy: 0.8824\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3605 - accuracy: 0.8484 - val_loss: 0.3419 - val_accuracy: 0.8655\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3797 - accuracy: 0.8379 - val_loss: 0.3425 - val_accuracy: 0.8824\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3776 - accuracy: 0.8316 - val_loss: 0.3416 - val_accuracy: 0.8739\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3802 - accuracy: 0.8316 - val_loss: 0.3413 - val_accuracy: 0.8739\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3786 - accuracy: 0.8316 - val_loss: 0.3434 - val_accuracy: 0.8824\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3666 - accuracy: 0.8337 - val_loss: 0.3434 - val_accuracy: 0.8824\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3637 - accuracy: 0.8421 - val_loss: 0.3466 - val_accuracy: 0.8655\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3853 - accuracy: 0.8421 - val_loss: 0.3468 - val_accuracy: 0.8655\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 95us/sample - loss: 0.3790 - accuracy: 0.8463 - val_loss: 0.3493 - val_accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3873 - accuracy: 0.8232 - val_loss: 0.3459 - val_accuracy: 0.8824\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3727 - accuracy: 0.8589 - val_loss: 0.3500 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3753 - accuracy: 0.8358 - val_loss: 0.3483 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3746 - accuracy: 0.8274 - val_loss: 0.3491 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3768 - accuracy: 0.8232 - val_loss: 0.3479 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3756 - accuracy: 0.8484 - val_loss: 0.3446 - val_accuracy: 0.8655\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3646 - accuracy: 0.8400 - val_loss: 0.3489 - val_accuracy: 0.8655\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3850 - accuracy: 0.8421 - val_loss: 0.3435 - val_accuracy: 0.8739\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3606 - accuracy: 0.8484 - val_loss: 0.3447 - val_accuracy: 0.8655\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3735 - accuracy: 0.8400 - val_loss: 0.3447 - val_accuracy: 0.8655\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3790 - accuracy: 0.8526 - val_loss: 0.3453 - val_accuracy: 0.8571\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3824 - accuracy: 0.8442 - val_loss: 0.3457 - val_accuracy: 0.8655\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3683 - accuracy: 0.8484 - val_loss: 0.3467 - val_accuracy: 0.8655\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3589 - accuracy: 0.8358 - val_loss: 0.3448 - val_accuracy: 0.8739\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3658 - accuracy: 0.8484 - val_loss: 0.3423 - val_accuracy: 0.8824\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3510 - accuracy: 0.8568 - val_loss: 0.3457 - val_accuracy: 0.8571\n",
      "297/297 [==============================] - 0s 51us/sample - loss: 0.4325 - accuracy: 0.8249\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu, total=   5.3s\n",
      "[CV] optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu .......\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6701 - accuracy: 0.6547 - val_loss: 0.6574 - val_accuracy: 0.6891\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.6432 - accuracy: 0.6905 - val_loss: 0.6324 - val_accuracy: 0.7143\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.6062 - accuracy: 0.7242 - val_loss: 0.6072 - val_accuracy: 0.7227\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5799 - accuracy: 0.7158 - val_loss: 0.5782 - val_accuracy: 0.7311\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5551 - accuracy: 0.7263 - val_loss: 0.5541 - val_accuracy: 0.7563\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.5162 - accuracy: 0.7916 - val_loss: 0.5350 - val_accuracy: 0.7647\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.5063 - accuracy: 0.7747 - val_loss: 0.5213 - val_accuracy: 0.7731\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5119 - accuracy: 0.8000 - val_loss: 0.5131 - val_accuracy: 0.7731\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.5008 - accuracy: 0.7874 - val_loss: 0.5042 - val_accuracy: 0.7731\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4769 - accuracy: 0.8021 - val_loss: 0.4983 - val_accuracy: 0.7731\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4644 - accuracy: 0.8168 - val_loss: 0.4947 - val_accuracy: 0.7815\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4573 - accuracy: 0.8063 - val_loss: 0.4929 - val_accuracy: 0.7731\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4551 - accuracy: 0.8147 - val_loss: 0.4930 - val_accuracy: 0.7731\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4619 - accuracy: 0.8084 - val_loss: 0.4872 - val_accuracy: 0.7731\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4476 - accuracy: 0.8168 - val_loss: 0.4854 - val_accuracy: 0.7731\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4520 - accuracy: 0.8105 - val_loss: 0.4818 - val_accuracy: 0.7731\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4515 - accuracy: 0.8084 - val_loss: 0.4819 - val_accuracy: 0.7731\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4463 - accuracy: 0.8211 - val_loss: 0.4801 - val_accuracy: 0.7815\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4508 - accuracy: 0.8126 - val_loss: 0.4769 - val_accuracy: 0.7815\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4393 - accuracy: 0.8147 - val_loss: 0.4791 - val_accuracy: 0.7815\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4397 - accuracy: 0.8337 - val_loss: 0.4775 - val_accuracy: 0.7731\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4346 - accuracy: 0.8295 - val_loss: 0.4747 - val_accuracy: 0.7815\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4290 - accuracy: 0.8147 - val_loss: 0.4738 - val_accuracy: 0.7731\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4338 - accuracy: 0.8211 - val_loss: 0.4730 - val_accuracy: 0.7899\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4253 - accuracy: 0.8274 - val_loss: 0.4750 - val_accuracy: 0.7899\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4359 - accuracy: 0.8211 - val_loss: 0.4758 - val_accuracy: 0.7815\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4152 - accuracy: 0.8337 - val_loss: 0.4758 - val_accuracy: 0.7815\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4139 - accuracy: 0.8295 - val_loss: 0.4783 - val_accuracy: 0.7815\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4173 - accuracy: 0.8379 - val_loss: 0.4758 - val_accuracy: 0.7815\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4041 - accuracy: 0.8253 - val_loss: 0.4780 - val_accuracy: 0.7983\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4068 - accuracy: 0.8253 - val_loss: 0.4817 - val_accuracy: 0.7983\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4130 - accuracy: 0.8295 - val_loss: 0.4797 - val_accuracy: 0.7983\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4106 - accuracy: 0.8379 - val_loss: 0.4808 - val_accuracy: 0.7983\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3984 - accuracy: 0.8274 - val_loss: 0.4817 - val_accuracy: 0.7983\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4304 - accuracy: 0.8316 - val_loss: 0.4821 - val_accuracy: 0.7983\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4082 - accuracy: 0.8316 - val_loss: 0.4792 - val_accuracy: 0.7983\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4007 - accuracy: 0.8358 - val_loss: 0.4796 - val_accuracy: 0.7983\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4063 - accuracy: 0.8295 - val_loss: 0.4830 - val_accuracy: 0.7983\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3996 - accuracy: 0.8358 - val_loss: 0.4832 - val_accuracy: 0.7983\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4032 - accuracy: 0.8463 - val_loss: 0.4843 - val_accuracy: 0.7983\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4180 - accuracy: 0.8358 - val_loss: 0.4848 - val_accuracy: 0.7899\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3935 - accuracy: 0.8421 - val_loss: 0.4844 - val_accuracy: 0.7899\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4017 - accuracy: 0.8400 - val_loss: 0.4847 - val_accuracy: 0.7899\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3844 - accuracy: 0.8547 - val_loss: 0.4880 - val_accuracy: 0.7899\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3886 - accuracy: 0.8505 - val_loss: 0.4905 - val_accuracy: 0.7899\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4027 - accuracy: 0.8379 - val_loss: 0.4922 - val_accuracy: 0.7983\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3932 - accuracy: 0.8505 - val_loss: 0.4911 - val_accuracy: 0.7983\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3993 - accuracy: 0.8358 - val_loss: 0.4918 - val_accuracy: 0.7899\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3934 - accuracy: 0.8358 - val_loss: 0.4942 - val_accuracy: 0.7983\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3904 - accuracy: 0.8526 - val_loss: 0.4935 - val_accuracy: 0.7983\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4064 - accuracy: 0.8484 - val_loss: 0.4941 - val_accuracy: 0.7983\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3912 - accuracy: 0.8421 - val_loss: 0.4990 - val_accuracy: 0.7983\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3998 - accuracy: 0.8463 - val_loss: 0.4981 - val_accuracy: 0.7983\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3995 - accuracy: 0.8568 - val_loss: 0.4996 - val_accuracy: 0.7983\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 96us/sample - loss: 0.3917 - accuracy: 0.8547 - val_loss: 0.4987 - val_accuracy: 0.7983\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3921 - accuracy: 0.8400 - val_loss: 0.4994 - val_accuracy: 0.7983\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4041 - accuracy: 0.8337 - val_loss: 0.4998 - val_accuracy: 0.7899\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3727 - accuracy: 0.8611 - val_loss: 0.5018 - val_accuracy: 0.7899\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.3926 - accuracy: 0.8337 - val_loss: 0.5025 - val_accuracy: 0.7899\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3838 - accuracy: 0.8358 - val_loss: 0.5002 - val_accuracy: 0.7899\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3865 - accuracy: 0.8463 - val_loss: 0.5036 - val_accuracy: 0.7899\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3901 - accuracy: 0.8442 - val_loss: 0.5068 - val_accuracy: 0.7899\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3819 - accuracy: 0.8379 - val_loss: 0.5091 - val_accuracy: 0.7899\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3903 - accuracy: 0.8400 - val_loss: 0.5079 - val_accuracy: 0.7899\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3865 - accuracy: 0.8463 - val_loss: 0.5093 - val_accuracy: 0.7899\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3802 - accuracy: 0.8484 - val_loss: 0.5099 - val_accuracy: 0.7899\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3863 - accuracy: 0.8505 - val_loss: 0.5119 - val_accuracy: 0.7899\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3742 - accuracy: 0.8568 - val_loss: 0.5099 - val_accuracy: 0.7899\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.3848 - accuracy: 0.8421 - val_loss: 0.5117 - val_accuracy: 0.7899\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3837 - accuracy: 0.8463 - val_loss: 0.5128 - val_accuracy: 0.7899\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3837 - accuracy: 0.8421 - val_loss: 0.5122 - val_accuracy: 0.7899\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3933 - accuracy: 0.8358 - val_loss: 0.5116 - val_accuracy: 0.7899\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3765 - accuracy: 0.8421 - val_loss: 0.5125 - val_accuracy: 0.7899\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3828 - accuracy: 0.8484 - val_loss: 0.5129 - val_accuracy: 0.7899\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3874 - accuracy: 0.8547 - val_loss: 0.5099 - val_accuracy: 0.7899\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3820 - accuracy: 0.8505 - val_loss: 0.5149 - val_accuracy: 0.7899\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3983 - accuracy: 0.8400 - val_loss: 0.5180 - val_accuracy: 0.7899\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3850 - accuracy: 0.8463 - val_loss: 0.5192 - val_accuracy: 0.7899\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3830 - accuracy: 0.8379 - val_loss: 0.5173 - val_accuracy: 0.7983\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3741 - accuracy: 0.8526 - val_loss: 0.5210 - val_accuracy: 0.7983\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3708 - accuracy: 0.8484 - val_loss: 0.5229 - val_accuracy: 0.7983\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3818 - accuracy: 0.8526 - val_loss: 0.5245 - val_accuracy: 0.7899\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3721 - accuracy: 0.8611 - val_loss: 0.5255 - val_accuracy: 0.7899\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 103us/sample - loss: 0.3769 - accuracy: 0.8400 - val_loss: 0.5282 - val_accuracy: 0.7899\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 118us/sample - loss: 0.3831 - accuracy: 0.8568 - val_loss: 0.5254 - val_accuracy: 0.7899\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 105us/sample - loss: 0.3786 - accuracy: 0.8400 - val_loss: 0.5286 - val_accuracy: 0.7899\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.3817 - accuracy: 0.8653 - val_loss: 0.5264 - val_accuracy: 0.7899\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3744 - accuracy: 0.8547 - val_loss: 0.5312 - val_accuracy: 0.7899\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3790 - accuracy: 0.8421 - val_loss: 0.5338 - val_accuracy: 0.7899\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3779 - accuracy: 0.8379 - val_loss: 0.5305 - val_accuracy: 0.7899\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.3737 - accuracy: 0.8611 - val_loss: 0.5316 - val_accuracy: 0.7899\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3844 - accuracy: 0.8484 - val_loss: 0.5361 - val_accuracy: 0.7899\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3902 - accuracy: 0.8484 - val_loss: 0.5351 - val_accuracy: 0.7899\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3770 - accuracy: 0.8589 - val_loss: 0.5349 - val_accuracy: 0.7899\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.3787 - accuracy: 0.8442 - val_loss: 0.5360 - val_accuracy: 0.7899\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3883 - accuracy: 0.8274 - val_loss: 0.5368 - val_accuracy: 0.7899\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3607 - accuracy: 0.8589 - val_loss: 0.5393 - val_accuracy: 0.7899\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.3784 - accuracy: 0.8505 - val_loss: 0.5356 - val_accuracy: 0.7899\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3575 - accuracy: 0.8463 - val_loss: 0.5379 - val_accuracy: 0.7899\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3726 - accuracy: 0.8526 - val_loss: 0.5412 - val_accuracy: 0.7899\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.4123 - accuracy: 0.8283\n",
      "[CV]  optimizer=Adam, n_neurons=42, n_hidden=2, activation=relu, total=   5.2s\n",
      "[CV] optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.8406 - accuracy: 0.4400 - val_loss: 0.9058 - val_accuracy: 0.4034\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.7726 - accuracy: 0.5011 - val_loss: 0.8403 - val_accuracy: 0.5126\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.7278 - accuracy: 0.5305 - val_loss: 0.7865 - val_accuracy: 0.5630\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.6924 - accuracy: 0.6063 - val_loss: 0.7408 - val_accuracy: 0.6555\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6626 - accuracy: 0.6421 - val_loss: 0.7022 - val_accuracy: 0.6807\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6373 - accuracy: 0.6568 - val_loss: 0.6670 - val_accuracy: 0.6975\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6156 - accuracy: 0.6695 - val_loss: 0.6387 - val_accuracy: 0.7143\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5978 - accuracy: 0.6779 - val_loss: 0.6139 - val_accuracy: 0.7227\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5812 - accuracy: 0.6968 - val_loss: 0.5908 - val_accuracy: 0.7227\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5658 - accuracy: 0.7053 - val_loss: 0.5706 - val_accuracy: 0.7479\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5519 - accuracy: 0.7284 - val_loss: 0.5532 - val_accuracy: 0.7479\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5383 - accuracy: 0.7368 - val_loss: 0.5357 - val_accuracy: 0.7563\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5260 - accuracy: 0.7516 - val_loss: 0.5199 - val_accuracy: 0.7647\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5152 - accuracy: 0.7537 - val_loss: 0.5069 - val_accuracy: 0.7731\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5055 - accuracy: 0.7558 - val_loss: 0.4952 - val_accuracy: 0.7815\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4967 - accuracy: 0.7684 - val_loss: 0.4835 - val_accuracy: 0.7899\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.4887 - accuracy: 0.7789 - val_loss: 0.4729 - val_accuracy: 0.7815\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4812 - accuracy: 0.7811 - val_loss: 0.4630 - val_accuracy: 0.7815\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4740 - accuracy: 0.7853 - val_loss: 0.4529 - val_accuracy: 0.7899\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.4675 - accuracy: 0.7874 - val_loss: 0.4442 - val_accuracy: 0.7983\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4614 - accuracy: 0.7874 - val_loss: 0.4348 - val_accuracy: 0.7983\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4559 - accuracy: 0.7895 - val_loss: 0.4261 - val_accuracy: 0.8067\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4505 - accuracy: 0.7916 - val_loss: 0.4194 - val_accuracy: 0.8067\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4456 - accuracy: 0.7937 - val_loss: 0.4133 - val_accuracy: 0.8151\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4415 - accuracy: 0.7916 - val_loss: 0.4066 - val_accuracy: 0.8319\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4369 - accuracy: 0.7937 - val_loss: 0.3994 - val_accuracy: 0.8319\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4330 - accuracy: 0.7958 - val_loss: 0.3937 - val_accuracy: 0.8319\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4289 - accuracy: 0.8000 - val_loss: 0.3873 - val_accuracy: 0.8235\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4256 - accuracy: 0.8063 - val_loss: 0.3836 - val_accuracy: 0.8403\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4221 - accuracy: 0.8063 - val_loss: 0.3800 - val_accuracy: 0.8403\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4191 - accuracy: 0.8063 - val_loss: 0.3773 - val_accuracy: 0.8487\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4161 - accuracy: 0.8105 - val_loss: 0.3740 - val_accuracy: 0.8487\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4136 - accuracy: 0.8126 - val_loss: 0.3694 - val_accuracy: 0.8487\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4112 - accuracy: 0.8126 - val_loss: 0.3665 - val_accuracy: 0.8571\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4090 - accuracy: 0.8147 - val_loss: 0.3637 - val_accuracy: 0.8571\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4066 - accuracy: 0.8147 - val_loss: 0.3617 - val_accuracy: 0.8571\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4054 - accuracy: 0.8126 - val_loss: 0.3596 - val_accuracy: 0.8655\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4036 - accuracy: 0.8147 - val_loss: 0.3580 - val_accuracy: 0.8655\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4023 - accuracy: 0.8211 - val_loss: 0.3551 - val_accuracy: 0.8655\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4007 - accuracy: 0.8168 - val_loss: 0.3534 - val_accuracy: 0.8655\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3996 - accuracy: 0.8147 - val_loss: 0.3524 - val_accuracy: 0.8655\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3984 - accuracy: 0.8168 - val_loss: 0.3514 - val_accuracy: 0.8655\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3970 - accuracy: 0.8168 - val_loss: 0.3515 - val_accuracy: 0.8655\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3956 - accuracy: 0.8232 - val_loss: 0.3490 - val_accuracy: 0.8655\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3942 - accuracy: 0.8189 - val_loss: 0.3492 - val_accuracy: 0.8655\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3937 - accuracy: 0.8253 - val_loss: 0.3481 - val_accuracy: 0.8655\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3926 - accuracy: 0.8232 - val_loss: 0.3480 - val_accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3917 - accuracy: 0.8232 - val_loss: 0.3481 - val_accuracy: 0.8655\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3903 - accuracy: 0.8274 - val_loss: 0.3467 - val_accuracy: 0.8655\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3897 - accuracy: 0.8232 - val_loss: 0.3453 - val_accuracy: 0.8655\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3885 - accuracy: 0.8295 - val_loss: 0.3453 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3874 - accuracy: 0.8274 - val_loss: 0.3438 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3865 - accuracy: 0.8295 - val_loss: 0.3431 - val_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3856 - accuracy: 0.8253 - val_loss: 0.3434 - val_accuracy: 0.8655\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3847 - accuracy: 0.8316 - val_loss: 0.3448 - val_accuracy: 0.8655\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3841 - accuracy: 0.8274 - val_loss: 0.3452 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3828 - accuracy: 0.8274 - val_loss: 0.3441 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3823 - accuracy: 0.8295 - val_loss: 0.3429 - val_accuracy: 0.8655\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3812 - accuracy: 0.8274 - val_loss: 0.3417 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3812 - accuracy: 0.8295 - val_loss: 0.3426 - val_accuracy: 0.8655\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3802 - accuracy: 0.8295 - val_loss: 0.3437 - val_accuracy: 0.8655\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3797 - accuracy: 0.8295 - val_loss: 0.3440 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3786 - accuracy: 0.8232 - val_loss: 0.3404 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3783 - accuracy: 0.8316 - val_loss: 0.3404 - val_accuracy: 0.8655\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 78us/sample - loss: 0.3779 - accuracy: 0.8295 - val_loss: 0.3403 - val_accuracy: 0.8655\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3768 - accuracy: 0.8295 - val_loss: 0.3412 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3762 - accuracy: 0.8295 - val_loss: 0.3392 - val_accuracy: 0.8655\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3758 - accuracy: 0.8295 - val_loss: 0.3409 - val_accuracy: 0.8571\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3752 - accuracy: 0.8253 - val_loss: 0.3402 - val_accuracy: 0.8571\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3749 - accuracy: 0.8316 - val_loss: 0.3402 - val_accuracy: 0.8571\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3740 - accuracy: 0.8358 - val_loss: 0.3417 - val_accuracy: 0.8571\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3737 - accuracy: 0.8316 - val_loss: 0.3388 - val_accuracy: 0.8571\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3733 - accuracy: 0.8316 - val_loss: 0.3397 - val_accuracy: 0.8571\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3732 - accuracy: 0.8295 - val_loss: 0.3380 - val_accuracy: 0.8571\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3723 - accuracy: 0.8337 - val_loss: 0.3382 - val_accuracy: 0.8571\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3715 - accuracy: 0.8337 - val_loss: 0.3398 - val_accuracy: 0.8571\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3716 - accuracy: 0.8358 - val_loss: 0.3386 - val_accuracy: 0.8571\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3714 - accuracy: 0.8337 - val_loss: 0.3387 - val_accuracy: 0.8571\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3704 - accuracy: 0.8379 - val_loss: 0.3384 - val_accuracy: 0.8571\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3699 - accuracy: 0.8337 - val_loss: 0.3366 - val_accuracy: 0.8571\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3696 - accuracy: 0.8358 - val_loss: 0.3391 - val_accuracy: 0.8571\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3691 - accuracy: 0.8316 - val_loss: 0.3408 - val_accuracy: 0.8571\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3687 - accuracy: 0.8337 - val_loss: 0.3397 - val_accuracy: 0.8571\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3683 - accuracy: 0.8316 - val_loss: 0.3380 - val_accuracy: 0.8571\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3688 - accuracy: 0.8295 - val_loss: 0.3379 - val_accuracy: 0.8571\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3674 - accuracy: 0.8358 - val_loss: 0.3370 - val_accuracy: 0.8571\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3676 - accuracy: 0.8337 - val_loss: 0.3388 - val_accuracy: 0.8571\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3672 - accuracy: 0.8358 - val_loss: 0.3372 - val_accuracy: 0.8571\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3666 - accuracy: 0.8358 - val_loss: 0.3385 - val_accuracy: 0.8571\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3663 - accuracy: 0.8379 - val_loss: 0.3378 - val_accuracy: 0.8571\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3664 - accuracy: 0.8337 - val_loss: 0.3383 - val_accuracy: 0.8571\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.3654 - accuracy: 0.8337 - val_loss: 0.3387 - val_accuracy: 0.8571\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3657 - accuracy: 0.8337 - val_loss: 0.3389 - val_accuracy: 0.8487\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3648 - accuracy: 0.8316 - val_loss: 0.3387 - val_accuracy: 0.8487\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3650 - accuracy: 0.8316 - val_loss: 0.3382 - val_accuracy: 0.8487\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3643 - accuracy: 0.8337 - val_loss: 0.3412 - val_accuracy: 0.8487\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3640 - accuracy: 0.8379 - val_loss: 0.3407 - val_accuracy: 0.8487\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3638 - accuracy: 0.8337 - val_loss: 0.3392 - val_accuracy: 0.8487\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3634 - accuracy: 0.8358 - val_loss: 0.3409 - val_accuracy: 0.8487\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3632 - accuracy: 0.8337 - val_loss: 0.3410 - val_accuracy: 0.8487\n",
      "297/297 [==============================] - 0s 50us/sample - loss: 0.4979 - accuracy: 0.7643\n",
      "[CV]  optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu, total=   4.9s\n",
      "[CV] optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 2ms/sample - loss: 0.7220 - accuracy: 0.4568 - val_loss: 0.6965 - val_accuracy: 0.5630\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6892 - accuracy: 0.5516 - val_loss: 0.6763 - val_accuracy: 0.5882\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6702 - accuracy: 0.6042 - val_loss: 0.6617 - val_accuracy: 0.6555\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6559 - accuracy: 0.6505 - val_loss: 0.6506 - val_accuracy: 0.7059\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.6434 - accuracy: 0.6905 - val_loss: 0.6391 - val_accuracy: 0.7059\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6305 - accuracy: 0.6989 - val_loss: 0.6271 - val_accuracy: 0.6807\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6181 - accuracy: 0.6947 - val_loss: 0.6147 - val_accuracy: 0.6891\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6065 - accuracy: 0.6884 - val_loss: 0.6037 - val_accuracy: 0.6891\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5955 - accuracy: 0.6905 - val_loss: 0.5924 - val_accuracy: 0.6975\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5855 - accuracy: 0.6884 - val_loss: 0.5821 - val_accuracy: 0.6975\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5756 - accuracy: 0.6968 - val_loss: 0.5715 - val_accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5663 - accuracy: 0.6989 - val_loss: 0.5607 - val_accuracy: 0.7311\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5574 - accuracy: 0.7074 - val_loss: 0.5504 - val_accuracy: 0.7395\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5484 - accuracy: 0.7263 - val_loss: 0.5404 - val_accuracy: 0.7395\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5404 - accuracy: 0.7326 - val_loss: 0.5302 - val_accuracy: 0.7479\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5330 - accuracy: 0.7579 - val_loss: 0.5209 - val_accuracy: 0.7563\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.5256 - accuracy: 0.7600 - val_loss: 0.5118 - val_accuracy: 0.7731\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.5184 - accuracy: 0.7768 - val_loss: 0.5023 - val_accuracy: 0.7899\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5123 - accuracy: 0.7895 - val_loss: 0.4940 - val_accuracy: 0.8067\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5058 - accuracy: 0.8000 - val_loss: 0.4859 - val_accuracy: 0.8067\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5001 - accuracy: 0.8084 - val_loss: 0.4776 - val_accuracy: 0.8151\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4946 - accuracy: 0.8126 - val_loss: 0.4707 - val_accuracy: 0.8235\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4894 - accuracy: 0.8189 - val_loss: 0.4642 - val_accuracy: 0.8235\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4843 - accuracy: 0.8189 - val_loss: 0.4573 - val_accuracy: 0.8403\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4798 - accuracy: 0.8211 - val_loss: 0.4508 - val_accuracy: 0.8403\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4755 - accuracy: 0.8189 - val_loss: 0.4444 - val_accuracy: 0.8403\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4715 - accuracy: 0.8253 - val_loss: 0.4383 - val_accuracy: 0.8319\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4681 - accuracy: 0.8274 - val_loss: 0.4328 - val_accuracy: 0.8319\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4645 - accuracy: 0.8253 - val_loss: 0.4277 - val_accuracy: 0.8319\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4610 - accuracy: 0.8253 - val_loss: 0.4226 - val_accuracy: 0.8319\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4571 - accuracy: 0.8253 - val_loss: 0.4168 - val_accuracy: 0.8403\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4543 - accuracy: 0.8253 - val_loss: 0.4113 - val_accuracy: 0.8403\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4505 - accuracy: 0.8232 - val_loss: 0.4062 - val_accuracy: 0.8403\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4476 - accuracy: 0.8316 - val_loss: 0.4014 - val_accuracy: 0.8403\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4449 - accuracy: 0.8316 - val_loss: 0.3973 - val_accuracy: 0.8403\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4423 - accuracy: 0.8295 - val_loss: 0.3936 - val_accuracy: 0.8403\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4402 - accuracy: 0.8295 - val_loss: 0.3896 - val_accuracy: 0.8403\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4379 - accuracy: 0.8274 - val_loss: 0.3861 - val_accuracy: 0.8403\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4360 - accuracy: 0.8316 - val_loss: 0.3831 - val_accuracy: 0.8403\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4340 - accuracy: 0.8295 - val_loss: 0.3798 - val_accuracy: 0.8403\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4323 - accuracy: 0.8316 - val_loss: 0.3771 - val_accuracy: 0.8403\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4307 - accuracy: 0.8295 - val_loss: 0.3745 - val_accuracy: 0.8487\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4289 - accuracy: 0.8274 - val_loss: 0.3716 - val_accuracy: 0.8487\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4273 - accuracy: 0.8316 - val_loss: 0.3692 - val_accuracy: 0.8487\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4259 - accuracy: 0.8316 - val_loss: 0.3674 - val_accuracy: 0.8487\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4246 - accuracy: 0.8295 - val_loss: 0.3660 - val_accuracy: 0.8571\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4231 - accuracy: 0.8295 - val_loss: 0.3632 - val_accuracy: 0.8571\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4222 - accuracy: 0.8295 - val_loss: 0.3611 - val_accuracy: 0.8487\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4205 - accuracy: 0.8358 - val_loss: 0.3591 - val_accuracy: 0.8487\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4191 - accuracy: 0.8316 - val_loss: 0.3568 - val_accuracy: 0.8487\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4183 - accuracy: 0.8337 - val_loss: 0.3555 - val_accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4166 - accuracy: 0.8316 - val_loss: 0.3538 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4161 - accuracy: 0.8358 - val_loss: 0.3525 - val_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4149 - accuracy: 0.8337 - val_loss: 0.3513 - val_accuracy: 0.8655\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4138 - accuracy: 0.8337 - val_loss: 0.3498 - val_accuracy: 0.8655\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4130 - accuracy: 0.8337 - val_loss: 0.3485 - val_accuracy: 0.8655\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4117 - accuracy: 0.8337 - val_loss: 0.3471 - val_accuracy: 0.8655\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4109 - accuracy: 0.8337 - val_loss: 0.3466 - val_accuracy: 0.8655\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4106 - accuracy: 0.8358 - val_loss: 0.3453 - val_accuracy: 0.8655\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4096 - accuracy: 0.8337 - val_loss: 0.3445 - val_accuracy: 0.8739\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4087 - accuracy: 0.8337 - val_loss: 0.3438 - val_accuracy: 0.8739\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4081 - accuracy: 0.8337 - val_loss: 0.3424 - val_accuracy: 0.8655\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4071 - accuracy: 0.8358 - val_loss: 0.3421 - val_accuracy: 0.8739\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4065 - accuracy: 0.8316 - val_loss: 0.3411 - val_accuracy: 0.8739\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4060 - accuracy: 0.8358 - val_loss: 0.3414 - val_accuracy: 0.8908\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4051 - accuracy: 0.8337 - val_loss: 0.3400 - val_accuracy: 0.8739\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4043 - accuracy: 0.8337 - val_loss: 0.3399 - val_accuracy: 0.8908\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4038 - accuracy: 0.8295 - val_loss: 0.3386 - val_accuracy: 0.8824\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4034 - accuracy: 0.8316 - val_loss: 0.3384 - val_accuracy: 0.8908\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4023 - accuracy: 0.8316 - val_loss: 0.3385 - val_accuracy: 0.8908\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4019 - accuracy: 0.8337 - val_loss: 0.3371 - val_accuracy: 0.8908\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4012 - accuracy: 0.8295 - val_loss: 0.3363 - val_accuracy: 0.8908\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4009 - accuracy: 0.8337 - val_loss: 0.3364 - val_accuracy: 0.8908\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4002 - accuracy: 0.8337 - val_loss: 0.3352 - val_accuracy: 0.8824\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3997 - accuracy: 0.8337 - val_loss: 0.3359 - val_accuracy: 0.8908\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3992 - accuracy: 0.8337 - val_loss: 0.3353 - val_accuracy: 0.8908\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3984 - accuracy: 0.8337 - val_loss: 0.3342 - val_accuracy: 0.8824\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3981 - accuracy: 0.8337 - val_loss: 0.3339 - val_accuracy: 0.8908\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3982 - accuracy: 0.8337 - val_loss: 0.3341 - val_accuracy: 0.8908\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3971 - accuracy: 0.8337 - val_loss: 0.3329 - val_accuracy: 0.8824\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3966 - accuracy: 0.8337 - val_loss: 0.3325 - val_accuracy: 0.8824\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3965 - accuracy: 0.8337 - val_loss: 0.3325 - val_accuracy: 0.8908\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3956 - accuracy: 0.8337 - val_loss: 0.3335 - val_accuracy: 0.8908\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3961 - accuracy: 0.8337 - val_loss: 0.3329 - val_accuracy: 0.8908\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3951 - accuracy: 0.8316 - val_loss: 0.3327 - val_accuracy: 0.8908\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3945 - accuracy: 0.8316 - val_loss: 0.3325 - val_accuracy: 0.8908\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3941 - accuracy: 0.8337 - val_loss: 0.3316 - val_accuracy: 0.8824\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3936 - accuracy: 0.8337 - val_loss: 0.3320 - val_accuracy: 0.8992\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3933 - accuracy: 0.8337 - val_loss: 0.3323 - val_accuracy: 0.8992\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.3931 - accuracy: 0.8316 - val_loss: 0.3311 - val_accuracy: 0.8908\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3932 - accuracy: 0.8337 - val_loss: 0.3307 - val_accuracy: 0.8824\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3923 - accuracy: 0.8253 - val_loss: 0.3308 - val_accuracy: 0.8908\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3923 - accuracy: 0.8295 - val_loss: 0.3303 - val_accuracy: 0.8824\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3917 - accuracy: 0.8337 - val_loss: 0.3301 - val_accuracy: 0.8824\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3912 - accuracy: 0.8274 - val_loss: 0.3311 - val_accuracy: 0.8992\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3909 - accuracy: 0.8316 - val_loss: 0.3304 - val_accuracy: 0.8908\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3910 - accuracy: 0.8337 - val_loss: 0.3292 - val_accuracy: 0.8824\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.3908 - accuracy: 0.8316 - val_loss: 0.3293 - val_accuracy: 0.8908\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3903 - accuracy: 0.8274 - val_loss: 0.3294 - val_accuracy: 0.8908\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.3903 - accuracy: 0.8295 - val_loss: 0.3301 - val_accuracy: 0.8992\n",
      "297/297 [==============================] - 0s 49us/sample - loss: 0.4134 - accuracy: 0.8215\n",
      "[CV]  optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu, total=   5.3s\n",
      "[CV] optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu .....\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.6808 - accuracy: 0.6484 - val_loss: 0.6744 - val_accuracy: 0.7311\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.6750 - accuracy: 0.6968 - val_loss: 0.6684 - val_accuracy: 0.7311\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.6699 - accuracy: 0.7284 - val_loss: 0.6627 - val_accuracy: 0.7227\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6646 - accuracy: 0.7221 - val_loss: 0.6570 - val_accuracy: 0.7311\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.6594 - accuracy: 0.7284 - val_loss: 0.6512 - val_accuracy: 0.7311\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.6535 - accuracy: 0.7411 - val_loss: 0.6449 - val_accuracy: 0.7479\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6474 - accuracy: 0.7432 - val_loss: 0.6383 - val_accuracy: 0.7563\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6411 - accuracy: 0.7537 - val_loss: 0.6316 - val_accuracy: 0.7647\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.6349 - accuracy: 0.7600 - val_loss: 0.6254 - val_accuracy: 0.7815\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.6289 - accuracy: 0.7705 - val_loss: 0.6191 - val_accuracy: 0.7815\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6229 - accuracy: 0.7726 - val_loss: 0.6129 - val_accuracy: 0.7815\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.6170 - accuracy: 0.7726 - val_loss: 0.6070 - val_accuracy: 0.7899\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.6111 - accuracy: 0.7768 - val_loss: 0.6011 - val_accuracy: 0.7899\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.6044 - accuracy: 0.7768 - val_loss: 0.5951 - val_accuracy: 0.7815\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5973 - accuracy: 0.7874 - val_loss: 0.5890 - val_accuracy: 0.7731\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5901 - accuracy: 0.7853 - val_loss: 0.5827 - val_accuracy: 0.7563\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5834 - accuracy: 0.7979 - val_loss: 0.5771 - val_accuracy: 0.7731\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5766 - accuracy: 0.8000 - val_loss: 0.5717 - val_accuracy: 0.7731\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5703 - accuracy: 0.8021 - val_loss: 0.5665 - val_accuracy: 0.7815\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5639 - accuracy: 0.8021 - val_loss: 0.5612 - val_accuracy: 0.7983\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.5571 - accuracy: 0.8042 - val_loss: 0.5557 - val_accuracy: 0.7899\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.5507 - accuracy: 0.8042 - val_loss: 0.5505 - val_accuracy: 0.7899\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5444 - accuracy: 0.8021 - val_loss: 0.5452 - val_accuracy: 0.7983\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5380 - accuracy: 0.8000 - val_loss: 0.5398 - val_accuracy: 0.7983\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5316 - accuracy: 0.8021 - val_loss: 0.5344 - val_accuracy: 0.7899\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.5253 - accuracy: 0.8000 - val_loss: 0.5289 - val_accuracy: 0.8067\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.5190 - accuracy: 0.8042 - val_loss: 0.5241 - val_accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.5129 - accuracy: 0.8021 - val_loss: 0.5201 - val_accuracy: 0.8067\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.5067 - accuracy: 0.8000 - val_loss: 0.5156 - val_accuracy: 0.8067\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.5008 - accuracy: 0.8021 - val_loss: 0.5117 - val_accuracy: 0.8067\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4949 - accuracy: 0.8063 - val_loss: 0.5081 - val_accuracy: 0.8067\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4893 - accuracy: 0.8084 - val_loss: 0.5049 - val_accuracy: 0.8067\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 77us/sample - loss: 0.4840 - accuracy: 0.8126 - val_loss: 0.5020 - val_accuracy: 0.8067\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4786 - accuracy: 0.8105 - val_loss: 0.4991 - val_accuracy: 0.8067\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4738 - accuracy: 0.8168 - val_loss: 0.4965 - val_accuracy: 0.8067\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 79us/sample - loss: 0.4693 - accuracy: 0.8232 - val_loss: 0.4943 - val_accuracy: 0.8067\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4645 - accuracy: 0.8211 - val_loss: 0.4913 - val_accuracy: 0.7983\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4600 - accuracy: 0.8232 - val_loss: 0.4889 - val_accuracy: 0.7983\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4556 - accuracy: 0.8253 - val_loss: 0.4869 - val_accuracy: 0.7983\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4513 - accuracy: 0.8274 - val_loss: 0.4855 - val_accuracy: 0.8067\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4475 - accuracy: 0.8211 - val_loss: 0.4842 - val_accuracy: 0.7983\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4436 - accuracy: 0.8253 - val_loss: 0.4827 - val_accuracy: 0.7983\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4402 - accuracy: 0.8211 - val_loss: 0.4817 - val_accuracy: 0.7983\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4369 - accuracy: 0.8274 - val_loss: 0.4807 - val_accuracy: 0.7983\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4338 - accuracy: 0.8211 - val_loss: 0.4799 - val_accuracy: 0.7983\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4312 - accuracy: 0.8232 - val_loss: 0.4792 - val_accuracy: 0.7983\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4285 - accuracy: 0.8253 - val_loss: 0.4783 - val_accuracy: 0.7983\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4261 - accuracy: 0.8232 - val_loss: 0.4781 - val_accuracy: 0.7983\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4241 - accuracy: 0.8253 - val_loss: 0.4778 - val_accuracy: 0.7983\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4225 - accuracy: 0.8253 - val_loss: 0.4778 - val_accuracy: 0.7983\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4206 - accuracy: 0.8274 - val_loss: 0.4778 - val_accuracy: 0.7983\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4189 - accuracy: 0.8337 - val_loss: 0.4776 - val_accuracy: 0.7983\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4174 - accuracy: 0.8337 - val_loss: 0.4781 - val_accuracy: 0.7983\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4162 - accuracy: 0.8358 - val_loss: 0.4786 - val_accuracy: 0.7983\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4149 - accuracy: 0.8358 - val_loss: 0.4786 - val_accuracy: 0.7983\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 99us/sample - loss: 0.4139 - accuracy: 0.8337 - val_loss: 0.4792 - val_accuracy: 0.7983\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4129 - accuracy: 0.8421 - val_loss: 0.4798 - val_accuracy: 0.7983\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4122 - accuracy: 0.8358 - val_loss: 0.4797 - val_accuracy: 0.7983\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.4116 - accuracy: 0.8358 - val_loss: 0.4804 - val_accuracy: 0.7983\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4107 - accuracy: 0.8337 - val_loss: 0.4807 - val_accuracy: 0.7983\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.4100 - accuracy: 0.8400 - val_loss: 0.4810 - val_accuracy: 0.7983\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4097 - accuracy: 0.8358 - val_loss: 0.4816 - val_accuracy: 0.7983\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4088 - accuracy: 0.8379 - val_loss: 0.4818 - val_accuracy: 0.8067\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4083 - accuracy: 0.8358 - val_loss: 0.4819 - val_accuracy: 0.8067\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4078 - accuracy: 0.8358 - val_loss: 0.4823 - val_accuracy: 0.8067\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4072 - accuracy: 0.8421 - val_loss: 0.4829 - val_accuracy: 0.8067\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4066 - accuracy: 0.8400 - val_loss: 0.4835 - val_accuracy: 0.8067\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4062 - accuracy: 0.8400 - val_loss: 0.4831 - val_accuracy: 0.8067\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4057 - accuracy: 0.8400 - val_loss: 0.4838 - val_accuracy: 0.8067\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4055 - accuracy: 0.8379 - val_loss: 0.4841 - val_accuracy: 0.8067\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4048 - accuracy: 0.8379 - val_loss: 0.4844 - val_accuracy: 0.8067\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4043 - accuracy: 0.8379 - val_loss: 0.4845 - val_accuracy: 0.8067\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4037 - accuracy: 0.8358 - val_loss: 0.4841 - val_accuracy: 0.8067\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4033 - accuracy: 0.8379 - val_loss: 0.4841 - val_accuracy: 0.8067\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.4032 - accuracy: 0.8379 - val_loss: 0.4847 - val_accuracy: 0.8067\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4027 - accuracy: 0.8358 - val_loss: 0.4845 - val_accuracy: 0.8067\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.4025 - accuracy: 0.8358 - val_loss: 0.4850 - val_accuracy: 0.8067\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.4019 - accuracy: 0.8358 - val_loss: 0.4851 - val_accuracy: 0.8067\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4017 - accuracy: 0.8358 - val_loss: 0.4854 - val_accuracy: 0.8067\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4012 - accuracy: 0.8358 - val_loss: 0.4853 - val_accuracy: 0.8067\n",
      "Epoch 81/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4007 - accuracy: 0.8358 - val_loss: 0.4855 - val_accuracy: 0.8067\n",
      "Epoch 82/100\n",
      "475/475 [==============================] - 0s 85us/sample - loss: 0.4006 - accuracy: 0.8400 - val_loss: 0.4859 - val_accuracy: 0.8067\n",
      "Epoch 83/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.4002 - accuracy: 0.8358 - val_loss: 0.4857 - val_accuracy: 0.8067\n",
      "Epoch 84/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3999 - accuracy: 0.8379 - val_loss: 0.4858 - val_accuracy: 0.8067\n",
      "Epoch 85/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3998 - accuracy: 0.8379 - val_loss: 0.4857 - val_accuracy: 0.8067\n",
      "Epoch 86/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3991 - accuracy: 0.8379 - val_loss: 0.4860 - val_accuracy: 0.8067\n",
      "Epoch 87/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3990 - accuracy: 0.8379 - val_loss: 0.4856 - val_accuracy: 0.8067\n",
      "Epoch 88/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3985 - accuracy: 0.8400 - val_loss: 0.4854 - val_accuracy: 0.8067\n",
      "Epoch 89/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3982 - accuracy: 0.8379 - val_loss: 0.4857 - val_accuracy: 0.8067\n",
      "Epoch 90/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3982 - accuracy: 0.8379 - val_loss: 0.4860 - val_accuracy: 0.8067\n",
      "Epoch 91/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.3975 - accuracy: 0.8379 - val_loss: 0.4859 - val_accuracy: 0.8067\n",
      "Epoch 92/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3973 - accuracy: 0.8400 - val_loss: 0.4857 - val_accuracy: 0.8067\n",
      "Epoch 93/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3971 - accuracy: 0.8421 - val_loss: 0.4860 - val_accuracy: 0.8067\n",
      "Epoch 94/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3966 - accuracy: 0.8421 - val_loss: 0.4860 - val_accuracy: 0.8067\n",
      "Epoch 95/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3964 - accuracy: 0.8400 - val_loss: 0.4862 - val_accuracy: 0.8067\n",
      "Epoch 96/100\n",
      "475/475 [==============================] - 0s 83us/sample - loss: 0.3957 - accuracy: 0.8421 - val_loss: 0.4866 - val_accuracy: 0.8067\n",
      "Epoch 97/100\n",
      "475/475 [==============================] - 0s 80us/sample - loss: 0.3956 - accuracy: 0.8400 - val_loss: 0.4867 - val_accuracy: 0.8067\n",
      "Epoch 98/100\n",
      "475/475 [==============================] - 0s 84us/sample - loss: 0.3954 - accuracy: 0.8400 - val_loss: 0.4866 - val_accuracy: 0.8067\n",
      "Epoch 99/100\n",
      "475/475 [==============================] - 0s 82us/sample - loss: 0.3949 - accuracy: 0.8421 - val_loss: 0.4868 - val_accuracy: 0.8067\n",
      "Epoch 100/100\n",
      "475/475 [==============================] - 0s 81us/sample - loss: 0.3950 - accuracy: 0.8421 - val_loss: 0.4869 - val_accuracy: 0.8067\n",
      "297/297 [==============================] - 0s 48us/sample - loss: 0.4223 - accuracy: 0.8283\n",
      "[CV]  optimizer=RMSprop, n_neurons=6, n_hidden=2, activation=relu, total=   5.0s\n",
      "[CV] optimizer=SGD, n_neurons=77, n_hidden=2, activation=elu .........\n",
      "Train on 475 samples, validate on 119 samples\n",
      "Epoch 1/100\n",
      "475/475 [==============================] - 1s 1ms/sample - loss: 0.7050 - accuracy: 0.5053 - val_loss: 0.6519 - val_accuracy: 0.6639\n",
      "Epoch 2/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.6478 - accuracy: 0.6253 - val_loss: 0.6065 - val_accuracy: 0.7059\n",
      "Epoch 3/100\n",
      "475/475 [==============================] - 0s 86us/sample - loss: 0.6073 - accuracy: 0.7032 - val_loss: 0.5773 - val_accuracy: 0.7143\n",
      "Epoch 4/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5791 - accuracy: 0.7053 - val_loss: 0.5554 - val_accuracy: 0.7143\n",
      "Epoch 5/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5735 - accuracy: 0.7095 - val_loss: 0.5383 - val_accuracy: 0.7227\n",
      "Epoch 6/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5638 - accuracy: 0.7116 - val_loss: 0.5247 - val_accuracy: 0.7311\n",
      "Epoch 7/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5439 - accuracy: 0.7411 - val_loss: 0.5127 - val_accuracy: 0.7479\n",
      "Epoch 8/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5472 - accuracy: 0.7221 - val_loss: 0.5027 - val_accuracy: 0.7899\n",
      "Epoch 9/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5352 - accuracy: 0.7516 - val_loss: 0.4936 - val_accuracy: 0.8067\n",
      "Epoch 10/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.5284 - accuracy: 0.7663 - val_loss: 0.4853 - val_accuracy: 0.8235\n",
      "Epoch 11/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5138 - accuracy: 0.7663 - val_loss: 0.4775 - val_accuracy: 0.8235\n",
      "Epoch 12/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.5106 - accuracy: 0.7832 - val_loss: 0.4706 - val_accuracy: 0.8151\n",
      "Epoch 13/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.5054 - accuracy: 0.7726 - val_loss: 0.4646 - val_accuracy: 0.8235\n",
      "Epoch 14/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4940 - accuracy: 0.7789 - val_loss: 0.4586 - val_accuracy: 0.8235\n",
      "Epoch 15/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4942 - accuracy: 0.8021 - val_loss: 0.4533 - val_accuracy: 0.8235\n",
      "Epoch 16/100\n",
      "475/475 [==============================] - 0s 87us/sample - loss: 0.4909 - accuracy: 0.7747 - val_loss: 0.4483 - val_accuracy: 0.8151\n",
      "Epoch 17/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4686 - accuracy: 0.8000 - val_loss: 0.4433 - val_accuracy: 0.8235\n",
      "Epoch 18/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4747 - accuracy: 0.8000 - val_loss: 0.4384 - val_accuracy: 0.8403\n",
      "Epoch 19/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4711 - accuracy: 0.8084 - val_loss: 0.4347 - val_accuracy: 0.8403\n",
      "Epoch 20/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4844 - accuracy: 0.8000 - val_loss: 0.4311 - val_accuracy: 0.8403\n",
      "Epoch 21/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4712 - accuracy: 0.7832 - val_loss: 0.4278 - val_accuracy: 0.8319\n",
      "Epoch 22/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4837 - accuracy: 0.7874 - val_loss: 0.4246 - val_accuracy: 0.8319\n",
      "Epoch 23/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4682 - accuracy: 0.8084 - val_loss: 0.4212 - val_accuracy: 0.8319\n",
      "Epoch 24/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4686 - accuracy: 0.8000 - val_loss: 0.4189 - val_accuracy: 0.8319\n",
      "Epoch 25/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4525 - accuracy: 0.8253 - val_loss: 0.4167 - val_accuracy: 0.8319\n",
      "Epoch 26/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4550 - accuracy: 0.8084 - val_loss: 0.4143 - val_accuracy: 0.8235\n",
      "Epoch 27/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4452 - accuracy: 0.8274 - val_loss: 0.4117 - val_accuracy: 0.8235\n",
      "Epoch 28/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4418 - accuracy: 0.7979 - val_loss: 0.4090 - val_accuracy: 0.8235\n",
      "Epoch 29/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4452 - accuracy: 0.7916 - val_loss: 0.4065 - val_accuracy: 0.8235\n",
      "Epoch 30/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4462 - accuracy: 0.7958 - val_loss: 0.4041 - val_accuracy: 0.8235\n",
      "Epoch 31/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4462 - accuracy: 0.7979 - val_loss: 0.4021 - val_accuracy: 0.8235\n",
      "Epoch 32/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4507 - accuracy: 0.8147 - val_loss: 0.4005 - val_accuracy: 0.8319\n",
      "Epoch 33/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4416 - accuracy: 0.8105 - val_loss: 0.3994 - val_accuracy: 0.8403\n",
      "Epoch 34/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4374 - accuracy: 0.8126 - val_loss: 0.3984 - val_accuracy: 0.8403\n",
      "Epoch 35/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4287 - accuracy: 0.8126 - val_loss: 0.3962 - val_accuracy: 0.8403\n",
      "Epoch 36/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4323 - accuracy: 0.8147 - val_loss: 0.3946 - val_accuracy: 0.8403\n",
      "Epoch 37/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4382 - accuracy: 0.8168 - val_loss: 0.3935 - val_accuracy: 0.8487\n",
      "Epoch 38/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4393 - accuracy: 0.8211 - val_loss: 0.3927 - val_accuracy: 0.8655\n",
      "Epoch 39/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4203 - accuracy: 0.8232 - val_loss: 0.3912 - val_accuracy: 0.8571\n",
      "Epoch 40/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4482 - accuracy: 0.8021 - val_loss: 0.3904 - val_accuracy: 0.8571\n",
      "Epoch 41/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4396 - accuracy: 0.8126 - val_loss: 0.3897 - val_accuracy: 0.8655\n",
      "Epoch 42/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4327 - accuracy: 0.8126 - val_loss: 0.3884 - val_accuracy: 0.8655\n",
      "Epoch 43/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4292 - accuracy: 0.8211 - val_loss: 0.3875 - val_accuracy: 0.8655\n",
      "Epoch 44/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4442 - accuracy: 0.8063 - val_loss: 0.3864 - val_accuracy: 0.8655\n",
      "Epoch 45/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4277 - accuracy: 0.8211 - val_loss: 0.3852 - val_accuracy: 0.8571\n",
      "Epoch 46/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4388 - accuracy: 0.8147 - val_loss: 0.3840 - val_accuracy: 0.8739\n",
      "Epoch 47/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4190 - accuracy: 0.8274 - val_loss: 0.3826 - val_accuracy: 0.8655\n",
      "Epoch 48/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4328 - accuracy: 0.8042 - val_loss: 0.3822 - val_accuracy: 0.8571\n",
      "Epoch 49/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4344 - accuracy: 0.8105 - val_loss: 0.3816 - val_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4325 - accuracy: 0.8168 - val_loss: 0.3805 - val_accuracy: 0.8571\n",
      "Epoch 51/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4234 - accuracy: 0.8211 - val_loss: 0.3806 - val_accuracy: 0.8655\n",
      "Epoch 52/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4222 - accuracy: 0.8211 - val_loss: 0.3797 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4232 - accuracy: 0.8232 - val_loss: 0.3794 - val_accuracy: 0.8655\n",
      "Epoch 54/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4207 - accuracy: 0.8105 - val_loss: 0.3784 - val_accuracy: 0.8571\n",
      "Epoch 55/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4353 - accuracy: 0.8232 - val_loss: 0.3776 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4126 - accuracy: 0.8189 - val_loss: 0.3767 - val_accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4200 - accuracy: 0.8147 - val_loss: 0.3766 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4337 - accuracy: 0.8063 - val_loss: 0.3769 - val_accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4275 - accuracy: 0.8189 - val_loss: 0.3763 - val_accuracy: 0.8487\n",
      "Epoch 60/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4372 - accuracy: 0.8211 - val_loss: 0.3764 - val_accuracy: 0.8487\n",
      "Epoch 61/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4077 - accuracy: 0.8168 - val_loss: 0.3749 - val_accuracy: 0.8487\n",
      "Epoch 62/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4203 - accuracy: 0.8232 - val_loss: 0.3740 - val_accuracy: 0.8487\n",
      "Epoch 63/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4429 - accuracy: 0.8211 - val_loss: 0.3739 - val_accuracy: 0.8487\n",
      "Epoch 64/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4119 - accuracy: 0.8211 - val_loss: 0.3731 - val_accuracy: 0.8487\n",
      "Epoch 65/100\n",
      "475/475 [==============================] - 0s 94us/sample - loss: 0.4323 - accuracy: 0.8147 - val_loss: 0.3737 - val_accuracy: 0.8487\n",
      "Epoch 66/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4314 - accuracy: 0.8084 - val_loss: 0.3724 - val_accuracy: 0.8487\n",
      "Epoch 67/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4167 - accuracy: 0.8084 - val_loss: 0.3721 - val_accuracy: 0.8487\n",
      "Epoch 68/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4271 - accuracy: 0.8189 - val_loss: 0.3718 - val_accuracy: 0.8487\n",
      "Epoch 69/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4100 - accuracy: 0.8211 - val_loss: 0.3717 - val_accuracy: 0.8487\n",
      "Epoch 70/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4231 - accuracy: 0.8295 - val_loss: 0.3709 - val_accuracy: 0.8487\n",
      "Epoch 71/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4074 - accuracy: 0.8274 - val_loss: 0.3697 - val_accuracy: 0.8487\n",
      "Epoch 72/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4192 - accuracy: 0.8189 - val_loss: 0.3703 - val_accuracy: 0.8487\n",
      "Epoch 73/100\n",
      "475/475 [==============================] - 0s 92us/sample - loss: 0.4136 - accuracy: 0.8337 - val_loss: 0.3705 - val_accuracy: 0.8487\n",
      "Epoch 74/100\n",
      "475/475 [==============================] - 0s 91us/sample - loss: 0.4165 - accuracy: 0.8400 - val_loss: 0.3708 - val_accuracy: 0.8487\n",
      "Epoch 75/100\n",
      "475/475 [==============================] - 0s 90us/sample - loss: 0.4210 - accuracy: 0.8232 - val_loss: 0.3707 - val_accuracy: 0.8487\n",
      "Epoch 76/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4176 - accuracy: 0.8168 - val_loss: 0.3706 - val_accuracy: 0.8487\n",
      "Epoch 77/100\n",
      "475/475 [==============================] - 0s 93us/sample - loss: 0.4191 - accuracy: 0.8126 - val_loss: 0.3702 - val_accuracy: 0.8487\n",
      "Epoch 78/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4340 - accuracy: 0.8189 - val_loss: 0.3696 - val_accuracy: 0.8487\n",
      "Epoch 79/100\n",
      "475/475 [==============================] - 0s 89us/sample - loss: 0.4170 - accuracy: 0.8168 - val_loss: 0.3692 - val_accuracy: 0.8487\n",
      "Epoch 80/100\n",
      "475/475 [==============================] - 0s 88us/sample - loss: 0.4109 - accuracy: 0.8379 - val_loss: 0.3691 - val_accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [1, 2, 3, 4],\n",
    "    \"n_neurons\": [6, 18, 30, 42, 56, 77, 84, 100],\n",
    "    \"activation\": ['relu', 'selu', 'elu'],\n",
    "    \"optimizer\": ['SGD', 'RMSprop', 'Adam'],\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(model3, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'Adam', 'n_neurons': 42, 'n_hidden': 4, 'activation': 'selu'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I condn't find a better model configuration using Randomized Search, each time I run it I got different results:  \n",
    "{'optimizer': 'RMSprop', 'n_neurons': 84, 'n_hidden': 3, 'activation': 'selu'}  \n",
    "{'optimizer': 'Adam', 'n_neurons': 77, 'n_hidden': 4, 'activation': 'elu'}  \n",
    "{'optimizer': 'RMSprop', 'n_neurons': 30, 'n_hidden': 1, 'activation': 'relu'}  \n",
    "{'optimizer': 'RMSprop', 'n_neurons': 6, 'n_hidden': 1, 'activation': 'relu'}  \n",
    "{'optimizer': 'RMSprop', 'n_neurons': 6, 'n_hidden': 4, 'activation': 'selu'}   \n",
    "{'optimizer': 'SGD', 'n_neurons': 100, 'n_hidden': 4, 'activation': 'elu'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 77)                1309      \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 77)                0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 25)                1950      \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 11)                0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 3,837\n",
      "Trainable params: 3,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model4 = build_model(n_hidden=4, n_neurons=77, input_shape=[16], activation = 'elu', optimizer = 'Adam')\n",
    "\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/100\n",
      "712/712 [==============================] - 1s 2ms/sample - loss: 0.6923 - accuracy: 0.5899 - val_loss: 0.5268 - val_accuracy: 0.7765\n",
      "Epoch 2/100\n",
      "712/712 [==============================] - 0s 108us/sample - loss: 0.5425 - accuracy: 0.7640 - val_loss: 0.4510 - val_accuracy: 0.8101\n",
      "Epoch 3/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.5110 - accuracy: 0.7795 - val_loss: 0.4125 - val_accuracy: 0.8436\n",
      "Epoch 4/100\n",
      "712/712 [==============================] - 0s 105us/sample - loss: 0.4960 - accuracy: 0.7837 - val_loss: 0.3835 - val_accuracy: 0.8547\n",
      "Epoch 5/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4682 - accuracy: 0.7879 - val_loss: 0.3673 - val_accuracy: 0.8715\n",
      "Epoch 6/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4633 - accuracy: 0.8034 - val_loss: 0.3602 - val_accuracy: 0.8603\n",
      "Epoch 7/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4663 - accuracy: 0.7992 - val_loss: 0.3521 - val_accuracy: 0.8659\n",
      "Epoch 8/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4521 - accuracy: 0.8132 - val_loss: 0.3441 - val_accuracy: 0.8659\n",
      "Epoch 9/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4573 - accuracy: 0.8104 - val_loss: 0.3390 - val_accuracy: 0.8603\n",
      "Epoch 10/100\n",
      "712/712 [==============================] - 0s 115us/sample - loss: 0.4483 - accuracy: 0.8244 - val_loss: 0.3473 - val_accuracy: 0.8659\n",
      "Epoch 11/100\n",
      "712/712 [==============================] - 0s 99us/sample - loss: 0.4557 - accuracy: 0.8132 - val_loss: 0.3380 - val_accuracy: 0.8771\n",
      "Epoch 12/100\n",
      "712/712 [==============================] - 0s 97us/sample - loss: 0.4478 - accuracy: 0.8090 - val_loss: 0.3422 - val_accuracy: 0.8603\n",
      "Epoch 13/100\n",
      "712/712 [==============================] - 0s 98us/sample - loss: 0.4484 - accuracy: 0.8048 - val_loss: 0.3341 - val_accuracy: 0.8827\n",
      "Epoch 14/100\n",
      "712/712 [==============================] - 0s 101us/sample - loss: 0.4396 - accuracy: 0.8258 - val_loss: 0.3327 - val_accuracy: 0.8771\n",
      "Epoch 15/100\n",
      "712/712 [==============================] - 0s 98us/sample - loss: 0.4342 - accuracy: 0.8230 - val_loss: 0.3340 - val_accuracy: 0.8715\n",
      "Epoch 16/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4386 - accuracy: 0.8048 - val_loss: 0.3351 - val_accuracy: 0.8771\n",
      "Epoch 17/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4224 - accuracy: 0.8146 - val_loss: 0.3278 - val_accuracy: 0.8715\n",
      "Epoch 18/100\n",
      "712/712 [==============================] - 0s 108us/sample - loss: 0.4538 - accuracy: 0.8034 - val_loss: 0.3274 - val_accuracy: 0.8659\n",
      "Epoch 19/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4408 - accuracy: 0.8230 - val_loss: 0.3298 - val_accuracy: 0.8883\n",
      "Epoch 20/100\n",
      "712/712 [==============================] - 0s 110us/sample - loss: 0.4367 - accuracy: 0.8076 - val_loss: 0.3329 - val_accuracy: 0.8827\n",
      "Epoch 21/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4224 - accuracy: 0.8202 - val_loss: 0.3300 - val_accuracy: 0.8827\n",
      "Epoch 22/100\n",
      "712/712 [==============================] - 0s 105us/sample - loss: 0.4289 - accuracy: 0.8343 - val_loss: 0.3260 - val_accuracy: 0.8771\n",
      "Epoch 23/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4383 - accuracy: 0.8132 - val_loss: 0.3261 - val_accuracy: 0.8827\n",
      "Epoch 24/100\n",
      "712/712 [==============================] - 0s 133us/sample - loss: 0.4274 - accuracy: 0.8244 - val_loss: 0.3244 - val_accuracy: 0.8827\n",
      "Epoch 25/100\n",
      "712/712 [==============================] - 0s 111us/sample - loss: 0.4231 - accuracy: 0.8132 - val_loss: 0.3222 - val_accuracy: 0.8883\n",
      "Epoch 26/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4313 - accuracy: 0.8216 - val_loss: 0.3259 - val_accuracy: 0.8827\n",
      "Epoch 27/100\n",
      "712/712 [==============================] - 0s 101us/sample - loss: 0.4439 - accuracy: 0.8188 - val_loss: 0.3256 - val_accuracy: 0.8883\n",
      "Epoch 28/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4360 - accuracy: 0.8146 - val_loss: 0.3283 - val_accuracy: 0.8939\n",
      "Epoch 29/100\n",
      "712/712 [==============================] - 0s 105us/sample - loss: 0.4292 - accuracy: 0.8202 - val_loss: 0.3304 - val_accuracy: 0.8827\n",
      "Epoch 30/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4253 - accuracy: 0.8230 - val_loss: 0.3237 - val_accuracy: 0.8771\n",
      "Epoch 31/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4339 - accuracy: 0.8258 - val_loss: 0.3249 - val_accuracy: 0.8883\n",
      "Epoch 32/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4265 - accuracy: 0.8216 - val_loss: 0.3254 - val_accuracy: 0.8827\n",
      "Epoch 33/100\n",
      "712/712 [==============================] - 0s 113us/sample - loss: 0.4110 - accuracy: 0.8272 - val_loss: 0.3247 - val_accuracy: 0.8994\n",
      "Epoch 34/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4369 - accuracy: 0.8174 - val_loss: 0.3247 - val_accuracy: 0.8883\n",
      "Epoch 35/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4201 - accuracy: 0.8385 - val_loss: 0.3221 - val_accuracy: 0.8771\n",
      "Epoch 36/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4343 - accuracy: 0.8118 - val_loss: 0.3229 - val_accuracy: 0.8939\n",
      "Epoch 37/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4392 - accuracy: 0.8301 - val_loss: 0.3213 - val_accuracy: 0.8994\n",
      "Epoch 38/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4288 - accuracy: 0.8202 - val_loss: 0.3233 - val_accuracy: 0.8939\n",
      "Epoch 39/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4142 - accuracy: 0.8244 - val_loss: 0.3252 - val_accuracy: 0.8827\n",
      "Epoch 40/100\n",
      "712/712 [==============================] - 0s 105us/sample - loss: 0.4337 - accuracy: 0.8230 - val_loss: 0.3251 - val_accuracy: 0.8771\n",
      "Epoch 41/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4178 - accuracy: 0.8301 - val_loss: 0.3224 - val_accuracy: 0.8883\n",
      "Epoch 42/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4252 - accuracy: 0.8258 - val_loss: 0.3274 - val_accuracy: 0.8883\n",
      "Epoch 43/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4329 - accuracy: 0.8160 - val_loss: 0.3192 - val_accuracy: 0.8939\n",
      "Epoch 44/100\n",
      "712/712 [==============================] - 0s 110us/sample - loss: 0.4236 - accuracy: 0.8244 - val_loss: 0.3184 - val_accuracy: 0.8939\n",
      "Epoch 45/100\n",
      "712/712 [==============================] - 0s 109us/sample - loss: 0.4140 - accuracy: 0.8216 - val_loss: 0.3222 - val_accuracy: 0.8827\n",
      "Epoch 46/100\n",
      "712/712 [==============================] - 0s 116us/sample - loss: 0.4215 - accuracy: 0.8174 - val_loss: 0.3229 - val_accuracy: 0.8827\n",
      "Epoch 47/100\n",
      "712/712 [==============================] - 0s 110us/sample - loss: 0.4324 - accuracy: 0.8216 - val_loss: 0.3231 - val_accuracy: 0.8883\n",
      "Epoch 48/100\n",
      "712/712 [==============================] - 0s 111us/sample - loss: 0.4259 - accuracy: 0.8399 - val_loss: 0.3219 - val_accuracy: 0.8994\n",
      "Epoch 49/100\n",
      "712/712 [==============================] - 0s 104us/sample - loss: 0.4284 - accuracy: 0.8202 - val_loss: 0.3223 - val_accuracy: 0.8883\n",
      "Epoch 50/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4131 - accuracy: 0.8188 - val_loss: 0.3217 - val_accuracy: 0.8883\n",
      "Epoch 51/100\n",
      "712/712 [==============================] - 0s 106us/sample - loss: 0.4224 - accuracy: 0.8202 - val_loss: 0.3185 - val_accuracy: 0.8939\n",
      "Epoch 52/100\n",
      "712/712 [==============================] - 0s 103us/sample - loss: 0.4235 - accuracy: 0.8160 - val_loss: 0.3201 - val_accuracy: 0.8994\n",
      "Epoch 53/100\n",
      "712/712 [==============================] - 0s 107us/sample - loss: 0.4269 - accuracy: 0.8216 - val_loss: 0.3226 - val_accuracy: 0.8883\n",
      "Epoch 54/100\n",
      "712/712 [==============================] - 0s 109us/sample - loss: 0.4357 - accuracy: 0.8146 - val_loss: 0.3237 - val_accuracy: 0.8827\n"
     ]
    }
   ],
   "source": [
    "history4 = model4.fit(X_train, y_train, epochs=100,\n",
    "                     validation_split=0.2, callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "pd.DataFrame(history4.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission4 = pd.read_csv(\"../input/titanic/gender_submission.csv\", index_col='PassengerId')\n",
    "submission4['Survived'] = model4.predict(X_test)\n",
    "submission4['Survived'] = submission['Survived'].apply(lambda x: round(x,0)).astype('int')\n",
    "submission4.to_csv('Titanic_model4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it was fun to discover Tensorflow and Keras API, using deep learning for this dataser may be overkill, as this dataset is tiny. If you are looking for best performance maybe Ensembling/Stacking with RandomForest and GradientBoosting works best. But I hope you enjoyed the rundown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
